diff --git a/CHANGELOG.md b/CHANGELOG.md
index ce5d3bff..d6ed7f60 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -76,6 +76,17 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0

 ## [Unreleased]

+### Added
+
+- **[A1] CLI skeleton and doctor with JSON snapshot**
+  - Entry point `spineprep` with subcommands: `run`, `doctor`, `version`
+  - `doctor` command with complete environment diagnostics
+  - JSON output with `--json` flag (file path or `-` for stdout)
+  - Required JSON fields: `python.version`, `spineprep.version`, `platform`, `packages`, `sct.present/version/path`, `pam50.present`, `disk.free_bytes`, `cwd_writeable`, `tmp_writeable`
+  - Non-zero exit code when SCT is missing
+  - Remediation text for missing SCT (Docker/Apptainer or local install)
+  - Disk space and write permission checks
+
 ### Planned

 - Nonlinear registration options
@@ -83,4 +94,3 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0
 - Interactive QC with motion plots
 - Performance profiling and caching
 - Container packaging (GHCR)
-
diff --git a/CITATION.cff b/CITATION.cff
index 0f499d1c..100575c9 100644
--- a/CITATION.cff
+++ b/CITATION.cff
@@ -1,7 +1,7 @@
 cff-version: 1.2.0
 message: "If you use SpinePrep in your research, please cite it as below."
 title: "SpinePrep: Spinal Cord fMRI Preprocessing Pipeline"
-version: 0.1.0
+version: 0.2.0
 date-released: 2025-10-08
 authors:
   - family-names: Sharifi
diff --git a/configs/base.yaml b/configs/base.yaml
index dd3634c6..abb15ada 100644
--- a/configs/base.yaml
+++ b/configs/base.yaml
@@ -1,16 +1,37 @@
-# Minimal defaults (expand later as needed)
+# SpinePrep default configuration
+# CLI args and user YAML files override these defaults
+
 bids_root: null
 output_dir: "./out"
-pipeline:
-  motion:
-    fd_threshold: 0.5
-    dvars_threshold: 1.5
+
+motion:
+  fd:
+    threshold: 0.5  # mm
+  dvars:
+    threshold: 1.5
+  highpass:
+    hz: 0.008  # Hz
+
 confounds:
   acompcor:
     n_components: 6
+  tcompcor:
+    n_components: 0
+  censor:
+    method: "fd"  # One of: fd, dvars, fd+dvars
+
 registration:
-  target: "PAM50"
-  sct:
-    enable: true
+  pam50:
+    spacing: "0.5mm"
+  resample:
+    to: "native"  # One of: native, pam50
+  deformable:
+    enabled: false
+
 qc:
-  enable: true
+  ssim:
+    min: 0.85
+  psnr:
+    min: 20.0
+  report:
+    embed_assets: true
diff --git a/docs/cli.md b/docs/cli.md
new file mode 100644
index 00000000..af0d7f36
--- /dev/null
+++ b/docs/cli.md
@@ -0,0 +1,85 @@
+# Command Line Interface
+
+SpinePrep provides a command-line interface with the following commands:
+
+## Commands
+
+### `spineprep version`
+
+Display the version of SpinePrep.
+
+```bash
+spineprep version
+```
+
+**Output:**
+```
+spineprep 0.2.0
+```
+
+---
+
+### `spineprep doctor`
+
+Check the environment and dependencies. Validates that required tools (SCT, PAM50) are available and writes a diagnostic report.
+
+```bash
+spineprep doctor [OPTIONS]
+```
+
+**Options:**
+
+- `--out PATH` — Output directory for provenance files (default: `./out`)
+- `--pam50 PATH` — Explicit path to PAM50 template directory
+- `--json PATH` — Write additional copy of report to custom JSON path
+- `--strict` — Treat warnings as failures
+
+**Example:**
+```bash
+spineprep doctor --out /data/derivatives/spineprep
+```
+
+**Output:**
+Prints a table of environment diagnostics and writes a timestamped JSON report to `{out}/provenance/doctor-YYYYMMDD_HHMMSS.json`.
+
+---
+
+### `spineprep run`
+
+Run the preprocessing pipeline with configuration validation.
+
+```bash
+spineprep run --bids BIDS_ROOT --out OUTPUT_DIR [OPTIONS]
+```
+
+**Options:**
+
+- `--bids PATH` — Path to BIDS dataset root directory (required)
+- `--out PATH` — Path to output directory for derivatives (required)
+- `--config PATH` — Path to YAML configuration file (optional)
+- `--print-config` — Print the fully resolved configuration and exit
+- `-n`, `--dry-run` — Dry-run mode: export DAG without running pipeline
+
+**Example:**
+```bash
+# Print resolved config
+spineprep run --bids /data/bids --out /data/derivatives/spineprep \
+  --config my_config.yaml --print-config
+
+# Run pipeline
+spineprep run --bids /data/bids --out /data/derivatives/spineprep
+```
+
+**Config Precedence:**
+Configuration values are merged in the following order (later overrides earlier):
+1. Built-in defaults (`configs/base.yaml`)
+2. User YAML file (`--config`)
+3. CLI arguments (`--bids`, `--out`)
+
+---
+
+## Configuration
+
+See [Configuration Guide](user-guide/config.md) for details on YAML structure, precedence rules, and available options.
+
+For schema reference, see [Config Reference](reference/config.md).
diff --git a/docs/reference/config.md b/docs/reference/config.md
index 6d01ba03..c686d98d 100644
--- a/docs/reference/config.md
+++ b/docs/reference/config.md
@@ -2,180 +2,104 @@

 This page documents all configuration options available in SpinePrep.

-**Schema**: SpinePrep configuration
-
 ## Configuration Options

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-### acq
+| `bids_root` | ['string', 'null'] | None | Path to BIDS dataset root directory |
+### confounds

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `acq.echo_spacing_s` | ['number', 'null'] | None | No description available |
-| `acq.pe_dir` | string | None | No description available |
-| `acq.slice_timing` | string | None | No description available |
-| `acq.tr` | number | None | No description available |
-
-### dataset
+### confounds.acompcor

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `dataset.runs_per_subject` | any | None | No description available |
-| `dataset.sessions` | any | None | No description available |
+| `confounds.acompcor.n_components` | integer | None | Number of aCompCor components to extract |

-### options
-
-No description available
-
-| Parameter | Type | Default | Description |
-|-----------|------|---------|-------------|
-### options.acompcor
+### confounds.censor

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `options.acompcor.detrend` | boolean | None | No description available |
-| `options.acompcor.enable` | boolean | None | No description available |
-| `options.acompcor.explained_variance_min` | number | None | No description available |
-| `options.acompcor.highpass_hz` | number | None | No description available |
-| `options.acompcor.n_components_per_tissue` | integer | None | No description available |
-| `options.acompcor.standardize` | boolean | None | No description available |
-| `options.acompcor.tissues` | array of string | None | No description available |
+| `confounds.censor.method` | string | None | Censoring method for high-motion frames |

-### options.censor
+### confounds.tcompcor

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `options.censor.dvars_thresh` | number | None | No description available |
-| `options.censor.enable` | boolean | None | No description available |
-| `options.censor.fd_thresh_mm` | number | None | No description available |
-| `options.censor.min_contig_vols` | integer | None | No description available |
-| `options.censor.pad_vols` | integer | None | No description available |
+| `confounds.tcompcor.n_components` | integer | None | Number of tCompCor components to extract |

-### options.cleanup

-No description available
-
-| Parameter | Type | Default | Description |
-|-----------|------|---------|-------------|
-| `options.cleanup.keep_only_latest_stage` | boolean | None | No description available |
-
-### options.confounds
+### motion

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `options.confounds.dvars_method` | string | None | No description available |
-| `options.confounds.fd_method` | string | None | No description available |
-
-| `options.denoise_mppca` | boolean | None | No description available |
-### options.first_level
+### motion.dvars

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `options.first_level.engine` | string | None | No description available |
-| `options.first_level.feat_variant` | string | None | No description available |
+| `motion.dvars.threshold` | number | None | DVARS threshold for motion detection |

-### options.ingest
+### motion.fd

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `options.ingest.enable` | boolean | None | No description available |
+| `motion.fd.threshold` | number | None | Framewise displacement threshold in mm |

-### options.masks
+### motion.highpass

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `options.masks.binarize_thr` | number | None | No description available |
-| `options.masks.enable` | boolean | None | No description available |
-| `options.masks.source` | string | None | No description available |
+| `motion.highpass.hz` | number | None | Highpass filter cutoff frequency in Hz |

-### options.motion

-No description available
-
-| Parameter | Type | Default | Description |
-|-----------|------|---------|-------------|
-### options.motion.concat
-
-No description available
-
-| Parameter | Type | Default | Description |
-|-----------|------|---------|-------------|
-| `options.motion.concat.mode` | string | None | No description available |
-| `options.motion.concat.require_same` | array of string | None | No description available |
-
-| `options.motion.engine` | string | None | No description available |
-| `options.motion.slice_axis` | string | None | No description available |
-
-### options.sdc
+| `output_dir` | string | None | Path to output directory for derivatives |
+### qc

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `options.sdc.enable` | boolean | None | No description available |
-| `options.sdc.mode` | string | None | No description available |
-
-### options.smoothing
+### qc.psnr

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `options.smoothing.enable` | boolean | None | No description available |
-| `options.smoothing.fwhm_mm` | number | None | No description available |
+| `qc.psnr.min` | number | None | Minimum acceptable PSNR in dB for QC |

-### options.temporal_crop
+### qc.report

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `options.temporal_crop.enable` | boolean | None | No description available |
-| `options.temporal_crop.max_trim_end` | integer | None | No description available |
-| `options.temporal_crop.max_trim_start` | integer | None | No description available |
-| `options.temporal_crop.method` | string | None | No description available |
-| `options.temporal_crop.z_thresh` | number | None | No description available |
-
+| `qc.report.embed_assets` | boolean | None | Embed assets directly in QC HTML reports |

-### paths
+### qc.ssim

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `paths.bids_dir` | string | None | No description available |
-| `paths.deriv_dir` | string | None | No description available |
-| `paths.logs_dir` | string | None | No description available |
-| `paths.root` | string | None | No description available |
-| `paths.work_dir` | string | None | No description available |
-
-| `pipeline_version` | string | None | No description available |
-| `project_name` | string | None | No description available |
-### qc
+| `qc.ssim.min` | number | None | Minimum acceptable SSIM for QC |

-No description available
-
-| Parameter | Type | Default | Description |
-|-----------|------|---------|-------------|
-| `qc.cohort_report` | boolean | None | No description available |
-| `qc.subject_report` | boolean | None | No description available |

 ### registration

@@ -183,65 +107,30 @@ No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `registration.enable` | boolean | None | No description available |
-| `registration.guidance` | string | None | No description available |
-| `registration.levels` | string | None | No description available |
-### registration.rootlets
+### registration.deformable

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `registration.rootlets.enable` | boolean | None | No description available |
-| `registration.rootlets.mode` | string | None | No description available |
+| `registration.deformable.enabled` | boolean | None | Enable deformable (non-linear) registration |

-| `registration.template` | string | None | No description available |
-| `registration.use_gm_wm_masks` | boolean | None | No description available |
-
-### resources
+### registration.pam50

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `resources.default_mem_gb` | integer | None | No description available |
-| `resources.default_threads` | integer | None | No description available |
+| `registration.pam50.spacing` | string | None | PAM50 template spacing (e.g., '0.5mm', '1mm') |

-### runtime
+### registration.resample

 No description available

 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `runtime.container_engine` | ['string', 'null'] | None | No description available |
-
-### study
-
-No description available
+| `registration.resample.to` | string | None | Target space for resampling |

-| Parameter | Type | Default | Description |
-|-----------|------|---------|-------------|
-| `study.name` | string | None | No description available |
-
-### templates
-
-No description available
-
-| Parameter | Type | Default | Description |
-|-----------|------|---------|-------------|
-| `templates.pam50_version` | string | None | No description available |
-
-### tools
-
-No description available
-
-| Parameter | Type | Default | Description |
-|-----------|------|---------|-------------|
-| `tools.ants` | string | None | No description available |
-| `tools.dcm2niix` | string | None | No description available |
-| `tools.fsl` | string | None | No description available |
-| `tools.sct` | string | None | No description available |
-| `tools.sct_min_version` | string | None | No description available |


 ## Example Configuration
diff --git a/mkdocs.yml b/mkdocs.yml
index e532e71c..2f95a9f8 100644
--- a/mkdocs.yml
+++ b/mkdocs.yml
@@ -124,6 +124,7 @@ nav:
     - Quickstart: user-guide/quickstart.md
     - Installation: getting-started.md
   - User Guide:
+    - CLI: cli.md
     - user-guide/usage.md
     - user-guide/doctor.md
     - user-guide/config.md
diff --git a/ops/status/branch_divergence.txt b/ops/status/branch_divergence.txt
new file mode 100644
index 00000000..a4bbdb5b
--- /dev/null
+++ b/ops/status/branch_divergence.txt
@@ -0,0 +1,51 @@
+=== feat/doctor-environment-diagnostics ===
+
+=== feat/spi-001-scaffold-cli-config ===
+
+=== feat/spi-002-snakemake-dag-provenance ===
+58dff246 feat(workflow): add minimal Snakemake DAG with provenance export  [spi-002]
+81712fac feat(registration): implement SCT EPI→PAM50 wrapper with metrics and rule  [spi-xxx]
+1beeb730 feat(confounds): implement FD/DVARS + confounds TSV/JSON (schema v1) and motion plot  [spi-confounds]
+8a30a885 feat(core): scaffold CLI with config loader and doctor command  [spi-001]
+
+=== feat/spi-011a-docs-ci-pages ===
+2c815ac1 ci(docs): harden docs CI and enable GitHub Pages [spi-011a]
+dadbaa4e feat(docs): stand up MkDocs Material site with generators  [spi-011]
+afed40c8 spi-005c: Add aCompCor confounds (cord/WM/CSF masks + PCA)
+
+=== feat/spi-A1-doctor-json-snapshot ===
+b2ed3dd9 feat(cli): add doctor JSON snapshot with complete diagnostics  [spi-A1]
+0c1675fd feat(qc): scaffold per-subject QC HTML with overlays, motion, aCompCor, and env  [spi-a7]
+3d371981 feat(preproc): add cord and CSF masking with QC overlays
+81712fac feat(registration): implement SCT EPI→PAM50 wrapper with metrics and rule  [spi-xxx]
+1beeb730 feat(confounds): implement FD/DVARS + confounds TSV/JSON (schema v1) and motion plot  [spi-confounds]
+
+=== feat/spi-a7-qc-html-scaffold ===
+0c1675fd feat(qc): scaffold per-subject QC HTML with overlays, motion, aCompCor, and env  [spi-a7]
+3d371981 feat(preproc): add cord and CSF masking with QC overlays
+81712fac feat(registration): implement SCT EPI→PAM50 wrapper with metrics and rule  [spi-xxx]
+1beeb730 feat(confounds): implement FD/DVARS + confounds TSV/JSON (schema v1) and motion plot  [spi-confounds]
+8a30a885 feat(core): scaffold CLI with config loader and doctor command  [spi-001]
+
+=== feat/spi-cli-scaffold ===
+b858cb63 feat(doctor): implement environment diagnostics
+5b5401b6 feat(doctor): implement environment diagnostics
+
+=== feat/spi-confounds-acompcor-tcompcor-censor ===
+81712fac feat(registration): implement SCT EPI→PAM50 wrapper with metrics and rule  [spi-xxx]
+1beeb730 feat(confounds): implement FD/DVARS + confounds TSV/JSON (schema v1) and motion plot  [spi-confounds]
+8a30a885 feat(core): scaffold CLI with config loader and doctor command  [spi-001]
+
+=== feat/spi-confounds-fd-dvars-tsv-json-v1 ===
+1beeb730 feat(confounds): implement FD/DVARS + confounds TSV/JSON (schema v1) and motion plot  [spi-confounds]
+8a30a885 feat(core): scaffold CLI with config loader and doctor command  [spi-001]
+
+=== feat/spi-masking-cord-csf-qc ===
+81712fac feat(registration): implement SCT EPI→PAM50 wrapper with metrics and rule  [spi-xxx]
+1beeb730 feat(confounds): implement FD/DVARS + confounds TSV/JSON (schema v1) and motion plot  [spi-confounds]
+8a30a885 feat(core): scaffold CLI with config loader and doctor command  [spi-001]
+
+=== feat/spi-xxx-registration-pam50-wrapper ===
+81712fac feat(registration): implement SCT EPI→PAM50 wrapper with metrics and rule  [spi-xxx]
+1beeb730 feat(confounds): implement FD/DVARS + confounds TSV/JSON (schema v1) and motion plot  [spi-confounds]
+8a30a885 feat(core): scaffold CLI with config loader and doctor command  [spi-001]
diff --git a/ops/status/branch_file_changes.txt b/ops/status/branch_file_changes.txt
new file mode 100644
index 00000000..0116b79f
--- /dev/null
+++ b/ops/status/branch_file_changes.txt
@@ -0,0 +1,109 @@
+=== feat/spi-A1-doctor-json-snapshot ===
+M	CHANGELOG.md
+M	CITATION.cff
+M	configs/base.yaml
+A	docs/cli.md
+M	docs/reference/config.md
+M	mkdocs.yml
+M	pytest.ini
+M	schemas/config.schema.json
+A	spineprep/_sct.py
+M	spineprep/_version.py
+M	spineprep/cli.py
+A	spineprep/confounds/__init__.py
+A	spineprep/confounds/__pycache__/__init__.cpython-312.pyc
+A	spineprep/confounds/__pycache__/io.cpython-312.pyc
+A	spineprep/confounds/__pycache__/motion.cpython-312.pyc
+A	spineprep/confounds/__pycache__/plot.cpython-312.pyc
+A	spineprep/confounds/io.py
+A	spineprep/confounds/motion.py
+A	spineprep/confounds/plot.py
+A	spineprep/confounds/schema_v1.json
+
+=== feat/spi-a7-qc-html-scaffold ===
+M	CITATION.cff
+M	configs/base.yaml
+A	docs/cli.md
+M	docs/reference/config.md
+M	mkdocs.yml
+M	pytest.ini
+M	schemas/config.schema.json
+A	spineprep/_sct.py
+M	spineprep/_version.py
+M	spineprep/cli.py
+A	spineprep/confounds/__init__.py
+A	spineprep/confounds/__pycache__/__init__.cpython-312.pyc
+A	spineprep/confounds/__pycache__/io.cpython-312.pyc
+A	spineprep/confounds/__pycache__/motion.cpython-312.pyc
+A	spineprep/confounds/__pycache__/plot.cpython-312.pyc
+A	spineprep/confounds/io.py
+A	spineprep/confounds/motion.py
+A	spineprep/confounds/plot.py
+A	spineprep/confounds/schema_v1.json
+A	spineprep/preproc/__init__.py
+
+=== feat/spi-002-snakemake-dag-provenance ===
+M	CITATION.cff
+M	configs/base.yaml
+A	docs/cli.md
+M	docs/reference/config.md
+M	mkdocs.yml
+M	pytest.ini
+M	schemas/config.schema.json
+M	spineprep/_version.py
+M	spineprep/cli.py
+A	spineprep/confounds/__init__.py
+A	spineprep/confounds/__pycache__/__init__.cpython-312.pyc
+A	spineprep/confounds/__pycache__/censor.cpython-312.pyc
+A	spineprep/confounds/__pycache__/compcor.cpython-312.pyc
+A	spineprep/confounds/__pycache__/io.cpython-312.pyc
+A	spineprep/confounds/__pycache__/motion.cpython-312.pyc
+A	spineprep/confounds/__pycache__/plot.cpython-312.pyc
+A	spineprep/confounds/censor.py
+A	spineprep/confounds/compcor.py
+A	spineprep/confounds/io.py
+A	spineprep/confounds/motion.py
+
+=== feat/spi-confounds-acompcor-tcompcor-censor ===
+M	CITATION.cff
+M	configs/base.yaml
+A	docs/cli.md
+M	docs/reference/config.md
+M	mkdocs.yml
+M	pytest.ini
+M	schemas/config.schema.json
+M	spineprep/_version.py
+M	spineprep/cli.py
+A	spineprep/confounds/__init__.py
+A	spineprep/confounds/__pycache__/__init__.cpython-312.pyc
+A	spineprep/confounds/__pycache__/io.cpython-312.pyc
+A	spineprep/confounds/__pycache__/motion.cpython-312.pyc
+A	spineprep/confounds/__pycache__/plot.cpython-312.pyc
+A	spineprep/confounds/io.py
+A	spineprep/confounds/motion.py
+A	spineprep/confounds/plot.py
+A	spineprep/confounds/schema_v1.json
+A	spineprep/registration/__init__.py
+A	spineprep/registration/header.py
+
+=== feat/spi-confounds-fd-dvars-tsv-json-v1 ===
+M	CITATION.cff
+M	configs/base.yaml
+A	docs/cli.md
+M	docs/reference/config.md
+M	mkdocs.yml
+M	pytest.ini
+M	schemas/config.schema.json
+M	spineprep/_version.py
+M	spineprep/cli.py
+A	spineprep/confounds/__init__.py
+A	spineprep/confounds/__pycache__/__init__.cpython-312.pyc
+A	spineprep/confounds/__pycache__/io.cpython-312.pyc
+A	spineprep/confounds/__pycache__/motion.cpython-312.pyc
+A	spineprep/confounds/__pycache__/plot.cpython-312.pyc
+A	spineprep/confounds/io.py
+A	spineprep/confounds/motion.py
+A	spineprep/confounds/plot.py
+A	spineprep/confounds/schema_v1.json
+A	tests/confounds/__init__.py
+A	tests/confounds/__pycache__/__init__.cpython-312.pyc
diff --git a/ops/status/branches.txt b/ops/status/branches.txt
new file mode 100644
index 00000000..c3ad3d5a
--- /dev/null
+++ b/ops/status/branches.txt
@@ -0,0 +1,12 @@
+feat/doctor-environment-diagnostics|07bb6e2f|2025-10-08 16:06:51 +0200
+feat/spi-001-scaffold-cli-config|e2955226|2025-10-08 22:21:23 +0200
+feat/spi-002-snakemake-dag-provenance|58dff246|2025-10-09 14:09:56 +0200
+feat/spi-011a-docs-ci-pages|2c815ac1|2025-10-08 11:10:04 +0200
+feat/spi-A1-doctor-json-snapshot|b2ed3dd9|2025-10-09 18:01:13 +0200
+feat/spi-a7-qc-html-scaffold|0c1675fd|2025-10-09 17:58:12 +0200
+feat/spi-cli-scaffold|b858cb63|2025-10-08 17:36:16 +0200
+feat/spi-confounds-acompcor-tcompcor-censor|81712fac|2025-10-09 14:01:35 +0200
+feat/spi-confounds-fd-dvars-tsv-json-v1|1beeb730|2025-10-09 14:00:17 +0200
+feat/spi-masking-cord-csf-qc|81712fac|2025-10-09 14:01:35 +0200
+feat/spi-xxx-registration-pam50-wrapper|81712fac|2025-10-09 14:01:35 +0200
+main|e2955226|2025-10-08 22:21:23 +0200
diff --git a/ops/status/check_ignore_sample.txt b/ops/status/check_ignore_sample.txt
new file mode 100644
index 00000000..e69de29b
diff --git a/ops/status/codespell.txt b/ops/status/codespell.txt
new file mode 100644
index 00000000..7d385056
--- /dev/null
+++ b/ops/status/codespell.txt
@@ -0,0 +1,1202 @@
+WARNING: Cannot decode file using encoding "utf-8": ./private/03_literature/litrature.pdf
+WARNING: Trying next encoding "iso-8859-1"
+WARNING: Cannot decode file using encoding "utf-8": ./private/03_literature/scopus_ai/3.pdf
+WARNING: Trying next encoding "iso-8859-1"
+WARNING: Cannot decode file using encoding "utf-8": ./private/03_literature/scopus_ai/4.pdf
+WARNING: Trying next encoding "iso-8859-1"
+WARNING: Cannot decode file using encoding "utf-8": ./private/03_literature/scopus_ai/8.pdf
+WARNING: Trying next encoding "iso-8859-1"
+./site/assets/javascripts/lunr/wordcut.js:122: ans ==> and
+./site/assets/javascripts/lunr/wordcut.js:136: ans ==> and
+./site/assets/javascripts/lunr/wordcut.js:145: ans ==> and
+./site/assets/javascripts/lunr/wordcut.js:3334: abd ==> and, bad
+./site/assets/javascripts/lunr/wordcut.js:4377: enviroments ==> environments
+./site/assets/javascripts/lunr/wordcut.js:4386: maddness ==> madness
+./site/assets/javascripts/lunr/wordcut.js:4393: hopfully ==> hopefully
+./site/assets/javascripts/lunr/wordcut.js:4402: enviroments ==> environments
+./site/assets/javascripts/lunr/wordcut.js:4411: maddness ==> madness
+./site/assets/javascripts/lunr/wordcut.js:4418: hopfully ==> hopefully
+./site/assets/javascripts/lunr/wordcut.js:4484: predictible ==> predictable
+./site/assets/javascripts/lunr/wordcut.js:5620: egal ==> equal
+./site/assets/javascripts/lunr/wordcut.js:5620: egal ==> equal
+./site/assets/javascripts/lunr/wordcut.js:5642: egal ==> equal
+./site/assets/javascripts/lunr/wordcut.js:5964: offest ==> offset
+./site/assets/stylesheets/main.2a3383ac.min.css:1: nd ==> and, 2nd
+./private/03_literature/library.bib:22: Ather ==> Other
+./private/03_literature/library.bib:1316: Oder ==> Order, Older, Coder, Odder, Odor, Over, Doer
+./private/03_literature/library.bib:2183: Hendler ==> Handler
+./private/03_literature/library.bib:2252: Yau ==> You, Yaw
+./private/03_literature/library.bib:2570: Claus ==> Clause
+./private/03_literature/library.bib:2750: Sherif ==> Sheriff
+./private/03_literature/library.bib:3505: Teh ==> The
+./private/03_literature/library.bib:4116: onl ==> only
+./private/03_literature/library.bib:4353: Bui ==> Buoy, Buy
+./private/03_literature/library.bib:4715: Leary ==> Leery
+./private/03_literature/library.bib:5531: Toom ==> Tomb
+./private/03_literature/library.bib:5901: Alle ==> All, Alley
+./private/03_literature/library.bib:5928: Nam ==> Name
+./private/03_literature/library.bib:6128: RIN ==> RING, RINK, RIND, RAIN, REIN, RUIN, GRIN
+./private/03_literature/library.bib:8011: Gage ==> Gauge
+./private/03_literature/library.bib:8432: Claus ==> Clause
+./private/03_literature/library.bib:8836: Alle ==> All, Alley
+./private/03_literature/litrature.pdf:527: YUr ==> your
+./private/03_literature/litrature.pdf:573: OLl ==> all, ole, old, olly, oil
+./private/03_literature/litrature.pdf:855: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/litrature.pdf:861: EdN ==> end
+./private/03_literature/litrature.pdf:1238: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/litrature.pdf:1311: eGe ==> edge
+./private/03_literature/litrature.pdf:2092: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/litrature.pdf:2699: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/litrature.pdf:3048: nd ==> and, 2nd
+./private/03_literature/litrature.pdf:3095: CaF ==> calf
+./private/03_literature/litrature.pdf:3106: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/litrature.pdf:3188: yuO ==> you
+./private/03_literature/litrature.pdf:3371: ths ==> the, this
+./private/03_literature/litrature.pdf:3495: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/litrature.pdf:3748: MOT ==> NOT
+./private/03_literature/litrature.pdf:3918: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/litrature.pdf:4249: TeY ==> they
+./private/03_literature/litrature.pdf:4293: toi ==> to, toy
+./private/03_literature/litrature.pdf:4479: ND ==> AND, 2ND
+./private/03_literature/litrature.pdf:4742: Nd ==> And, 2nd
+./private/03_literature/litrature.pdf:4919: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/litrature.pdf:5044: nD ==> and, 2nd
+./private/03_literature/litrature.pdf:5045: EGE ==> EDGE
+./private/03_literature/litrature.pdf:5055: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/litrature.pdf:5103: oNw ==> own
+./private/03_literature/litrature.pdf:5469: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/litrature.pdf:5502: HDa ==> had
+./private/03_literature/litrature.pdf:5515: oLY ==> only
+./private/03_literature/litrature.pdf:5889: nD ==> and, 2nd
+./private/03_literature/litrature.pdf:5923: aBd ==> and, bad
+./private/03_literature/litrature.pdf:5973: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/litrature.pdf:6154: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/litrature.pdf:6224: Yhe ==> The
+./private/03_literature/litrature.pdf:6289: OnW ==> own
+./private/03_literature/litrature.pdf:6634: nD ==> and, 2nd
+./private/03_literature/litrature.pdf:7588: tHq ==> the
+./private/03_literature/litrature.pdf:8092: sIZ ==> size, six
+./private/03_literature/litrature.pdf:8851: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/litrature.pdf:9009: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/litrature.pdf:9315: nD ==> and, 2nd
+./private/03_literature/litrature.pdf:9448: Nd ==> And, 2nd
+./private/03_literature/litrature.pdf:9522: NMAE ==> NAME
+./private/03_literature/litrature.pdf:9558: nd ==> and, 2nd
+./private/03_literature/litrature.pdf:9646: nd ==> and, 2nd
+./private/03_literature/litrature.pdf:9858: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/litrature.pdf:10992: oll ==> all, ole, old, olly, oil
+./private/03_literature/litrature.pdf:11170: yse ==> yes, use, nyse
+./private/03_literature/litrature.pdf:11256: Nd ==> And, 2nd
+./private/03_literature/litrature.pdf:11266: ND ==> AND, 2ND
+./private/03_literature/litrature.pdf:11417: ND ==> AND, 2ND
+./private/03_literature/litrature.pdf:11645: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/litrature.pdf:11988: myE ==> may, my
+./private/03_literature/litrature.pdf:12151: WEE ==> WE
+./private/03_literature/litrature.pdf:12624: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/litrature.pdf:13355: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/litrature.pdf:13454: nD ==> and, 2nd
+./private/03_literature/litrature.pdf:13589: onY ==> only, on, one
+./private/03_literature/litrature.pdf:13632: nd ==> and, 2nd
+./private/03_literature/litrature.pdf:14277: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/litrature.pdf:14756: nD ==> and, 2nd
+./private/03_literature/litrature.pdf:15392: 2rD ==> 2nd
+./private/03_literature/litrature.pdf:15563: vEW ==> view, vow, vex
+./private/03_literature/litrature.pdf:15663: nd ==> and, 2nd
+./private/03_literature/scopus_ai/3.pdf:163: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/scopus_ai/3.pdf:7252: myE ==> may, my
+./private/03_literature/scopus_ai/3.pdf:7540: WEE ==> WE
+./private/03_literature/scopus_ai/4.pdf:140: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/scopus_ai/4.pdf:490: nd ==> and, 2nd
+./private/03_literature/scopus_ai/4.pdf:537: CaF ==> calf
+./private/03_literature/scopus_ai/4.pdf:549: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/scopus_ai/4.pdf:631: yuO ==> you
+./private/03_literature/scopus_ai/4.pdf:826: ths ==> the, this
+./private/03_literature/scopus_ai/4.pdf:950: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/scopus_ai/8.pdf:249: nD ==> and, 2nd
+./private/03_literature/scopus_ai/8.pdf:283: aBd ==> and, bad
+./private/03_literature/scopus_ai/8.pdf:333: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/scopus_ai/8.pdf:517: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/scopus_ai/8.pdf:588: Yhe ==> The
+./private/03_literature/scopus_ai/8.pdf:653: OnW ==> own
+./private/03_literature/scopus_ai/8.pdf:8179: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/scopus_ai/scopus (1).bib:191: onl ==> only
+./private/03_literature/scopus_ai/scopus (1).bib:235: Gage ==> Gauge
+./private/03_literature/scopus_ai/scopus (1).bib:949: Teh ==> The
+./private/03_literature/scopus_ai/scopus (10).bib:79: Claus ==> Clause
+./private/03_literature/scopus_ai/scopus (10).bib:288: Ather ==> Other
+./private/03_literature/scopus_ai/scopus (10).bib:636: Hendler ==> Handler
+./private/03_literature/scopus_ai/scopus (10).bib:666: Leary ==> Leery
+WARNING: Cannot decode file using encoding "utf-8": ./private/03_literature/papers/imag-3-123.pdf
+WARNING: Trying next encoding "iso-8859-1"
+./private/03_literature/scopus_ai/scopus (10).bib:905: Bui ==> Buoy, Buy
+./private/03_literature/scopus_ai/scopus (10).bib:1056: onl ==> only
+./private/03_literature/scopus_ai/scopus (10).bib:1493: Alle ==> All, Alley
+./private/03_literature/scopus_ai/scopus (11).bib:79: Claus ==> Clause
+./private/03_literature/scopus_ai/scopus (11).bib:288: Ather ==> Other
+./private/03_literature/scopus_ai/scopus (11).bib:636: Hendler ==> Handler
+./private/03_literature/scopus_ai/scopus (11).bib:666: Leary ==> Leery
+./private/03_literature/scopus_ai/scopus (11).bib:905: Bui ==> Buoy, Buy
+./private/03_literature/scopus_ai/scopus (11).bib:1056: onl ==> only
+./private/03_literature/scopus_ai/scopus (11).bib:1493: Alle ==> All, Alley
+./private/03_literature/scopus_ai/scopus (2).bib:157: Yau ==> You, Yaw
+./private/03_literature/scopus_ai/scopus (2).bib:204: Toom ==> Tomb
+./private/03_literature/scopus_ai/scopus (3).bib:126: Oder ==> Order, Older, Coder, Odder, Odor, Over, Doer
+./private/03_literature/scopus_ai/scopus (3).bib:601: Yau ==> You, Yaw
+./private/03_literature/scopus_ai/scopus (5).bib:81: Claus ==> Clause
+./private/03_literature/scopus_ai/scopus (5).bib:169: Nam ==> Name
+./private/03_literature/scopus_ai/scopus (8).bib:124: Nam ==> Name
+./private/03_literature/scopus_ai/scopus (8).bib:278: Claus ==> Clause
+./private/03_literature/scopus_ai/scopus (9).bib:473: Claus ==> Clause
+./private/03_literature/scopus_ai/scopus (9).bib:489: Sherif ==> Sheriff
+./private/03_literature/scopus_ai/scopus (9).bib:778: RIN ==> RING, RINK, RIND, RAIN, REIN, RUIN, GRIN
+./private/03_literature/scopus_ai/scopus.bib:124: Alle ==> All, Alley
+./private/03_literature/papers/imag-3-123.pdf:239: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:403: nWE ==> new
+./private/03_literature/papers/imag-3-123.pdf:616: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:1071: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:1140: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:1445: weW ==> we
+./private/03_literature/papers/imag-3-123.pdf:1572: wll ==> will
+./private/03_literature/papers/imag-3-123.pdf:1935: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:2059: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:2076: oLY ==> only
+./private/03_literature/papers/imag-3-123.pdf:2087: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:2141: aDDD ==> add
+./private/03_literature/papers/imag-3-123.pdf:2368: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:2661: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:3283: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:3309: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:3517: Daa ==> Data
+./private/03_literature/papers/imag-3-123.pdf:3587: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:3751: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:3848: coo ==> coup
+./private/03_literature/papers/imag-3-123.pdf:3905: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:4452: gES ==> goes, guess
+./private/03_literature/papers/imag-3-123.pdf:4969: gir ==> git
+./private/03_literature/papers/imag-3-123.pdf:5020: MKE ==> MAKE
+./private/03_literature/papers/imag-3-123.pdf:5065: abl ==> able
+./private/03_literature/papers/imag-3-123.pdf:5195: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:5301: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:5479: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:5491: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:5733: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:5860: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:5864: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:6005: NaM ==> name
+./private/03_literature/papers/imag-3-123.pdf:6061: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:6153: nOO ==> no
+./private/03_literature/papers/imag-3-123.pdf:6435: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:6880: wEE ==> we
+./private/03_literature/papers/imag-3-123.pdf:6912: Hsa ==> Has
+./private/03_literature/papers/imag-3-123.pdf:6999: rIN ==> ring, rink, rind, rain, rein, ruin, grin
+./private/03_literature/papers/imag-3-123.pdf:7333: ALLL ==> ALL, ALLY
+./private/03_literature/papers/imag-3-123.pdf:7568: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:7665: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:7868: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:8353: NOo ==> no
+./private/03_literature/papers/imag-3-123.pdf:8372: ADDD ==> ADD
+./private/03_literature/papers/imag-3-123.pdf:8400: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:8900: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:9021: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:9181: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:9295: wYA ==> way
+./private/03_literature/papers/imag-3-123.pdf:9427: Wll ==> Will
+./private/03_literature/papers/imag-3-123.pdf:9574: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:9719: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:9775: daa ==> data
+./private/03_literature/papers/imag-3-123.pdf:9782: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:9935: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:10095: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:10140: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:10140: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:10212: DIsGN ==> design
+./private/03_literature/papers/imag-3-123.pdf:10512: daa ==> data
+./private/03_literature/papers/imag-3-123.pdf:10535: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:10557: noo ==> no
+./private/03_literature/papers/imag-3-123.pdf:10896: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:10937: oyU ==> you
+./private/03_literature/papers/imag-3-123.pdf:10943: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:11014: coo ==> coup
+./private/03_literature/papers/imag-3-123.pdf:11230: noo ==> no
+./private/03_literature/papers/imag-3-123.pdf:11336: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:11459: Haa ==> Has
+./private/03_literature/papers/imag-3-123.pdf:11760: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:11953: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:11972: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:12230: haX ==> hex
+./private/03_literature/papers/imag-3-123.pdf:12298: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:12533: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:13078: wIT ==> with
+./private/03_literature/papers/imag-3-123.pdf:13848: HAA ==> HAS
+./private/03_literature/papers/imag-3-123.pdf:13883: Joo ==> You
+./private/03_literature/papers/imag-3-123.pdf:13884: daa ==> data
+./private/03_literature/papers/imag-3-123.pdf:13895: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:14100: tHt ==> the, that
+./private/03_literature/papers/imag-3-123.pdf:14227: tBE ==> the
+./private/03_literature/papers/imag-3-123.pdf:14231: pTRSS ==> press
+./private/03_literature/papers/imag-3-123.pdf:14335: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:14378: JOO ==> YOU
+./private/03_literature/papers/imag-3-123.pdf:14509: Oll ==> All, Ole, Old, Olly, Oil
+./private/03_literature/papers/imag-3-123.pdf:14634: noo ==> no
+./private/03_literature/papers/imag-3-123.pdf:14674: afe ==> safe
+./private/03_literature/papers/imag-3-123.pdf:14693: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:14736: Coo ==> Coup
+./private/03_literature/papers/imag-3-123.pdf:14809: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:15344: VIE ==> VIA
+./private/03_literature/papers/imag-3-123.pdf:15438: AlO ==> also
+./private/03_literature/papers/imag-3-123.pdf:15447: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:15515: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:15528: MeY ==> may
+./private/03_literature/papers/imag-3-123.pdf:15594: sEh ==> she
+./private/03_literature/papers/imag-3-123.pdf:15704: SEh ==> she
+./private/03_literature/papers/imag-3-123.pdf:15766: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:15863: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:16069: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:16872: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:17094: wHn ==> when
+./private/03_literature/papers/imag-3-123.pdf:17167: haX ==> hex
+./private/03_literature/papers/imag-3-123.pdf:17382: OtU ==> out
+./private/03_literature/papers/imag-3-123.pdf:18180: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:18218: JOO ==> YOU
+./private/03_literature/papers/imag-3-123.pdf:18241: WEe ==> we
+./private/03_literature/papers/imag-3-123.pdf:18331: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:18660: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:19058: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:19178: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:19529: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:20489: Wih ==> With
+./private/03_literature/papers/imag-3-123.pdf:20836: fPt ==> ftp
+./private/03_literature/papers/imag-3-123.pdf:20855: TBe ==> the
+./private/03_literature/papers/imag-3-123.pdf:21028: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:21255: fom ==> from, form
+./private/03_literature/papers/imag-3-123.pdf:21278: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:21354: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:21415: hSi ==> his
+./private/03_literature/papers/imag-3-123.pdf:21572: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:21594: iSt ==> is, it, its, it's, sit, list
+./private/03_literature/papers/imag-3-123.pdf:21621: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:21935: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:22027: NWE ==> NEW
+./private/03_literature/papers/imag-3-123.pdf:22146: IHs ==> his
+./private/03_literature/papers/imag-3-123.pdf:22228: aNe ==> and
+./private/03_literature/papers/imag-3-123.pdf:22794: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:22857: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:23065: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:23176: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:23622: Aci ==> Acpi
+./private/03_literature/papers/imag-3-123.pdf:23751: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:23795: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:23889: Wee ==> We
+./private/03_literature/papers/imag-3-123.pdf:23889: vaI ==> via, vie
+./private/03_literature/papers/imag-3-123.pdf:23906: AnE ==> and
+./private/03_literature/papers/imag-3-123.pdf:24111: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:24349: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:24454: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:24495: Tye ==> Type, Tie
+./private/03_literature/papers/imag-3-123.pdf:25472: hDA ==> had
+./private/03_literature/papers/imag-3-123.pdf:25979: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:26002: iif ==> if
+./private/03_literature/papers/imag-3-123.pdf:26833: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:27199: TNe ==> the
+./private/03_literature/papers/imag-3-123.pdf:28353: tNE ==> the
+./private/03_literature/papers/imag-3-123.pdf:28622: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:28901: LeW ==> lieu, hew, sew, dew
+./private/03_literature/papers/imag-3-123.pdf:28929: vEW ==> view, vow, vex
+./private/03_literature/papers/imag-3-123.pdf:28934: Ned ==> Need, End, Nod
+./private/03_literature/papers/imag-3-123.pdf:29048: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:29320: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:29705: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:29899: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:30175: AfoR ==> for
+./private/03_literature/papers/imag-3-123.pdf:30371: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:31080: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:31139: tHt ==> the, that
+./private/03_literature/papers/imag-3-123.pdf:31184: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:31350: whN ==> when
+./private/03_literature/papers/imag-3-123.pdf:31472: 2rd ==> 2nd
+./private/03_literature/papers/imag-3-123.pdf:31542: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:31715: Fom ==> From, Form
+./private/03_literature/papers/imag-3-123.pdf:31879: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:31952: IiF ==> if
+./private/03_literature/papers/imag-3-123.pdf:31959: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:32408: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:32649: WTH ==> WITH
+./private/03_literature/papers/imag-3-123.pdf:32675: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:32680: Tge ==> The
+./private/03_literature/papers/imag-3-123.pdf:32689: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:32886: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:33037: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:33043: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:33095: noo ==> no
+./private/03_literature/papers/imag-3-123.pdf:33402: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:33504: Nwo ==> Now
+./private/03_literature/papers/imag-3-123.pdf:33527: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:33607: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:33937: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:33949: onW ==> own
+./private/03_literature/papers/imag-3-123.pdf:34060: bVE ==> be
+./private/03_literature/papers/imag-3-123.pdf:34125: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:34301: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:34327: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:34447: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:34508: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:34613: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:34790: Fot ==> For, Fit, Dot, Rot, Cot, Got, Tot, Fog
+./private/03_literature/papers/imag-3-123.pdf:34913: 1ND ==> 1ST
+./private/03_literature/papers/imag-3-123.pdf:35162: 2RD ==> 2ND
+./private/03_literature/papers/imag-3-123.pdf:35342: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:35361: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:35457: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:35479: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:35523: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:35738: FPr ==> for, far, fps
+./private/03_literature/papers/imag-3-123.pdf:35863: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:36379: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:36654: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:36733: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:36764: PTD ==> PDF
+./private/03_literature/papers/imag-3-123.pdf:36961: hEl ==> help, hell, heal
+./private/03_literature/papers/imag-3-123.pdf:37087: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:37158: Noo ==> No
+./private/03_literature/papers/imag-3-123.pdf:37384: iNH ==> in
+./private/03_literature/papers/imag-3-123.pdf:37415: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:37562: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:37566: ois ==> is
+./private/03_literature/papers/imag-3-123.pdf:37599: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:37704: wTh ==> with
+./private/03_literature/papers/imag-3-123.pdf:37750: CoO ==> coup
+./private/03_literature/papers/imag-3-123.pdf:38214: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:38297: weE ==> we
+./private/03_literature/papers/imag-3-123.pdf:38624: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:38635: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:38674: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:39313: nax ==> max, nad
+./private/03_literature/papers/imag-3-123.pdf:39767: noo ==> no
+./private/03_literature/papers/imag-3-123.pdf:39846: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:40109: Coo ==> Coup
+./private/03_literature/papers/imag-3-123.pdf:40195: noo ==> no
+./private/03_literature/papers/imag-3-123.pdf:40247: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:40376: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:40430: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:40512: COO ==> COUP
+./private/03_literature/papers/imag-3-123.pdf:40661: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:40738: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:40995: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:41101: DuM ==> dumb
+./private/03_literature/papers/imag-3-123.pdf:41178: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:41252: cppp ==> cpp
+./private/03_literature/papers/imag-3-123.pdf:41257: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:41260: cPPP ==> cpp
+./private/03_literature/papers/imag-3-123.pdf:41348: HSI ==> HIS
+./private/03_literature/papers/imag-3-123.pdf:41527: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:41569: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:41633: yhe ==> the
+./private/03_literature/papers/imag-3-123.pdf:41637: Alll ==> All, Ally
+./private/03_literature/papers/imag-3-123.pdf:41959: oLL ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:42004: HAA ==> HAS
+./private/03_literature/papers/imag-3-123.pdf:42078: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:42474: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:42610: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:42701: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:42768: nOO ==> no
+./private/03_literature/papers/imag-3-123.pdf:42942: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:42950: Noo ==> No
+./private/03_literature/papers/imag-3-123.pdf:42961: COO ==> COUP
+./private/03_literature/papers/imag-3-123.pdf:43156: Joo ==> You
+./private/03_literature/papers/imag-3-123.pdf:43186: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:43387: Coo ==> Coup
+./private/03_literature/papers/imag-3-123.pdf:43442: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:43591: coo ==> coup
+./private/03_literature/papers/imag-3-123.pdf:43692: JOO ==> YOU
+./private/03_literature/papers/imag-3-123.pdf:43796: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:43799: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:43799: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:43803: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:43808: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:43829: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:43925: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:44137: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:44275: HEL ==> HELP, HELL, HEAL
+./private/03_literature/papers/imag-3-123.pdf:44277: wlll ==> will
+./private/03_literature/papers/imag-3-123.pdf:44399: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:44682: WEL ==> WELL
+./private/03_literature/papers/imag-3-123.pdf:44779: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:44858: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:45135: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:45417: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:45545: EGE ==> EDGE
+./private/03_literature/papers/imag-3-123.pdf:45665: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:45915: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:46006: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:46150: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:46410: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:46477: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:46561: JOO ==> YOU
+./private/03_literature/papers/imag-3-123.pdf:46588: Joo ==> You
+./private/03_literature/papers/imag-3-123.pdf:46651: ALLL ==> ALL, ALLY
+./private/03_literature/papers/imag-3-123.pdf:47210: TIcH ==> thick, tick, titch, stitch
+./private/03_literature/papers/imag-3-123.pdf:47257: tYE ==> type, tie
+./private/03_literature/papers/imag-3-123.pdf:47712: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:47872: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:47932: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:48007: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:48062: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:48082: LOOS ==> LOOSE, LOSE
+./private/03_literature/papers/imag-3-123.pdf:48408: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:48731: Aadd ==> Add
+./private/03_literature/papers/imag-3-123.pdf:49336: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:49836: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:49941: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:49972: CAf ==> calf
+./private/03_literature/papers/imag-3-123.pdf:50010: oNy ==> only, on, one
+./private/03_literature/papers/imag-3-123.pdf:50278: tHs ==> the, this
+./private/03_literature/papers/imag-3-123.pdf:50944: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:50963: laf ==> laugh, leaf, loaf, lad, lag, lac, kaf, kaph
+./private/03_literature/papers/imag-3-123.pdf:50999: laf ==> laugh, leaf, loaf, lad, lag, lac, kaf, kaph
+./private/03_literature/papers/imag-3-123.pdf:51476: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:51614: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:51782: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:52095: fOm ==> from, form
+./private/03_literature/papers/imag-3-123.pdf:52259: dum ==> dumb
+./private/03_literature/papers/imag-3-123.pdf:52341: aFE ==> safe
+./private/03_literature/papers/imag-3-123.pdf:52594: noo ==> no
+./private/03_literature/papers/imag-3-123.pdf:53293: tOi ==> to, toy
+./private/03_literature/papers/imag-3-123.pdf:53336: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:53351: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:53482: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:53620: thS ==> the, this
+./private/03_literature/papers/imag-3-123.pdf:53646: COO ==> COUP
+./private/03_literature/papers/imag-3-123.pdf:53779: COO ==> COUP
+./private/03_literature/papers/imag-3-123.pdf:53876: iNh ==> in
+./private/03_literature/papers/imag-3-123.pdf:53943: noo ==> no
+./private/03_literature/papers/imag-3-123.pdf:54217: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:54326: coO ==> coup
+./private/03_literature/papers/imag-3-123.pdf:54459: hdA ==> had
+./private/03_literature/papers/imag-3-123.pdf:54487: aCi ==> acpi
+./private/03_literature/papers/imag-3-123.pdf:54520: HVe ==> have
+./private/03_literature/papers/imag-3-123.pdf:54929: hAx ==> hex
+./private/03_literature/papers/imag-3-123.pdf:54995: fOF ==> for
+./private/03_literature/papers/imag-3-123.pdf:55015: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:55101: aCII ==> ascii
+./private/03_literature/papers/imag-3-123.pdf:55189: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:55453: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:55533: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:55784: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:56174: Alll ==> All, Ally
+./private/03_literature/papers/imag-3-123.pdf:56326: DAA ==> DATA
+./private/03_literature/papers/imag-3-123.pdf:56505: TrU ==> through, true
+./private/03_literature/papers/imag-3-123.pdf:56782: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:56882: lIk ==> like, lick, link
+./private/03_literature/papers/imag-3-123.pdf:56886: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:57113: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:57144: ABd ==> and, bad
+./private/03_literature/papers/imag-3-123.pdf:57547: cppp ==> cpp
+./private/03_literature/papers/imag-3-123.pdf:57802: LOOS ==> LOOSE, LOSE
+./private/03_literature/papers/imag-3-123.pdf:57905: Coo ==> Coup
+./private/03_literature/papers/imag-3-123.pdf:58466: UesD ==> used
+./private/03_literature/papers/imag-3-123.pdf:58480: UesD ==> used
+./private/03_literature/papers/imag-3-123.pdf:58544: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:58768: wew ==> we
+./private/03_literature/papers/imag-3-123.pdf:58850: ACi ==> acpi
+./private/03_literature/papers/imag-3-123.pdf:59208: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:59358: TOOO ==> TODO, TOO, TOOL, TOOK
+./private/03_literature/papers/imag-3-123.pdf:59377: TOOO ==> TODO, TOO, TOOL, TOOK
+./private/03_literature/papers/imag-3-123.pdf:59441: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:59483: SAV ==> SAVE
+./private/03_literature/papers/imag-3-123.pdf:59599: wLLL ==> will
+./private/03_literature/papers/imag-3-123.pdf:60027: naM ==> name
+./private/03_literature/papers/imag-3-123.pdf:60104: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:60144: Coo ==> Coup
+./private/03_literature/papers/imag-3-123.pdf:60251: OnL ==> only
+./private/03_literature/papers/imag-3-123.pdf:60285: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:60354: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:60382: fOf ==> for
+./private/03_literature/papers/imag-3-123.pdf:60408: sHS ==> ssh, nhs
+./private/03_literature/papers/imag-3-123.pdf:60522: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:60775: LIk ==> like, lick, link
+./private/03_literature/papers/imag-3-123.pdf:60960: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:61003: Wlll ==> Will
+./private/03_literature/papers/imag-3-123.pdf:61028: Wee ==> We
+./private/03_literature/papers/imag-3-123.pdf:61065: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:61087: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:61277: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:61301: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:61313: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:61455: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:61724: wll ==> will
+./private/03_literature/papers/imag-3-123.pdf:61851: Coo ==> Coup
+./private/03_literature/papers/imag-3-123.pdf:62359: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:62598: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:62601: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:62612: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:62634: aaS ==> ass, as
+./private/03_literature/papers/imag-3-123.pdf:62683: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:62748: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:62872: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:63198: ihs ==> his
+./private/03_literature/papers/imag-3-123.pdf:63224: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:63355: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:63436: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:63592: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:64047: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:64273: AUoT ==> auto
+./private/03_literature/papers/imag-3-123.pdf:64422: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:64457: wee ==> we
+./private/03_literature/papers/imag-3-123.pdf:64479: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:64634: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:64691: ALLL ==> ALL, ALLY
+./private/03_literature/papers/imag-3-123.pdf:64694: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:64746: Yau ==> You, Yaw
+./private/03_literature/papers/imag-3-123.pdf:64957: lik ==> like, lick, link
+./private/03_literature/papers/imag-3-123.pdf:65322: TiCH ==> thick, tick, titch, stitch
+./private/03_literature/papers/imag-3-123.pdf:65343: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:65447: eDE ==> edge
+./private/03_literature/papers/imag-3-123.pdf:65674: COO ==> COUP
+./private/03_literature/papers/imag-3-123.pdf:65780: TyE ==> type, tie
+./private/03_literature/papers/imag-3-123.pdf:65804: soM ==> some
+./private/03_literature/papers/imag-3-123.pdf:65824: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:65890: TOi ==> to, toy
+./private/03_literature/papers/imag-3-123.pdf:65998: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:66103: ned ==> need, end, nod
+./private/03_literature/papers/imag-3-123.pdf:66600: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:66950: CPPP ==> CPP
+./private/03_literature/papers/imag-3-123.pdf:67141: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:67347: HSa ==> has
+./private/03_literature/papers/imag-3-123.pdf:67384: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:67624: Oll ==> All, Ole, Old, Olly, Oil
+./private/03_literature/papers/imag-3-123.pdf:67783: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:67807: cppp ==> cpp
+./private/03_literature/papers/imag-3-123.pdf:67827: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:67981: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:67983: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:68075: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:68119: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:68267: SEh ==> she
+./private/03_literature/papers/imag-3-123.pdf:68425: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:68700: eDE ==> edge
+./private/03_literature/papers/imag-3-123.pdf:68860: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:69778: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:69823: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:69891: FIlLL ==> fill
+./private/03_literature/papers/imag-3-123.pdf:70100: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:70186: slq ==> sql
+./private/03_literature/papers/imag-3-123.pdf:70338: bu ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:70584: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:70614: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:70775: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:71033: NOO ==> NO
+./private/03_literature/papers/imag-3-123.pdf:71052: MKE ==> MAKE
+./private/03_literature/papers/imag-3-123.pdf:71127: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:71343: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:71597: noo ==> no
+./private/03_literature/papers/imag-3-123.pdf:71638: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:71674: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:71734: daa ==> data
+./private/03_literature/papers/imag-3-123.pdf:71830: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:71834: udO ==> undo, sudo, judo, ado, udon, ufo
+./private/03_literature/papers/imag-3-123.pdf:71897: noo ==> no
+./private/03_literature/papers/imag-3-123.pdf:72316: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:72920: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:72952: alll ==> all, ally
+./private/03_literature/papers/imag-3-123.pdf:72971: joo ==> you
+./private/03_literature/papers/imag-3-123.pdf:73141: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:73184: wll ==> will
+./private/03_literature/papers/imag-3-123.pdf:73287: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:73351: LSIt ==> list, slit, sit
+./private/03_literature/papers/imag-3-123.pdf:73501: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:73619: TBe ==> the
+./private/03_literature/papers/imag-3-123.pdf:74359: wll ==> will
+./private/03_literature/papers/imag-3-123.pdf:74882: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:74892: Ser ==> Set
+./private/03_literature/papers/imag-3-123.pdf:74945: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:75024: LIK ==> LIKE, LICK, LINK
+./private/03_literature/papers/imag-3-123.pdf:75251: nOO ==> no
+./private/03_literature/papers/imag-3-123.pdf:75590: AnC ==> and
+./private/03_literature/papers/imag-3-123.pdf:75662: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:75729: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:75906: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:75948: ND ==> AND, 2ND
+./private/03_literature/papers/imag-3-123.pdf:76278: WLl ==> will
+./private/03_literature/papers/imag-3-123.pdf:76313: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./private/03_literature/papers/imag-3-123.pdf:76370: tOI ==> to, toy
+./private/03_literature/papers/imag-3-123.pdf:76402: dAA ==> data
+./private/03_literature/papers/imag-3-123.pdf:76425: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:76625: ded ==> dead
+./private/03_literature/papers/imag-3-123.pdf:76936: uNX ==> unix
+./private/03_literature/papers/imag-3-123.pdf:77535: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:77603: yaU ==> you, yaw
+./private/03_literature/papers/imag-3-123.pdf:77803: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:77810: kaKe ==> cake, take
+./private/03_literature/papers/imag-3-123.pdf:78131: TOi ==> to, toy
+./private/03_literature/papers/imag-3-123.pdf:78139: nD ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:78204: jus ==> just
+./private/03_literature/papers/imag-3-123.pdf:78210: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:78549: Daa ==> Data
+./private/03_literature/papers/imag-3-123.pdf:78615: TRU ==> THROUGH, TRUE
+./private/03_literature/papers/imag-3-123.pdf:78621: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:78756: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:78807: Nd ==> And, 2nd
+./private/03_literature/papers/imag-3-123.pdf:79363: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./private/03_literature/papers/imag-3-123.pdf:79417: OlT ==> old
+./private/03_literature/papers/imag-3-123.pdf:79570: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:79625: Hve ==> Have
+./private/03_literature/papers/imag-3-123.pdf:79899: oll ==> all, ole, old, olly, oil
+./private/03_literature/papers/imag-3-123.pdf:79936: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./private/03_literature/papers/imag-3-123.pdf:80769: nd ==> and, 2nd
+./private/03_literature/papers/imag-3-123.pdf:81633: bU ==> by, be, but, bug, bun, bud, buy, bum
+./private/03_literature/papers/imag-3-123.pdf:81879: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:9: ans ==> and
+./ops/status/codespell.txt:10: ans ==> and
+./ops/status/codespell.txt:11: ans ==> and
+./ops/status/codespell.txt:12: abd ==> and, bad
+./ops/status/codespell.txt:13: enviroments ==> environments
+./ops/status/codespell.txt:14: maddness ==> madness
+./ops/status/codespell.txt:15: hopfully ==> hopefully
+./ops/status/codespell.txt:16: enviroments ==> environments
+./ops/status/codespell.txt:17: maddness ==> madness
+./ops/status/codespell.txt:18: hopfully ==> hopefully
+./ops/status/codespell.txt:19: predictible ==> predictable
+./ops/status/codespell.txt:20: egal ==> equal
+./ops/status/codespell.txt:21: egal ==> equal
+./ops/status/codespell.txt:22: egal ==> equal
+./ops/status/codespell.txt:23: offest ==> offset
+./ops/status/codespell.txt:24: nd ==> and, 2nd
+./ops/status/codespell.txt:25: Ather ==> Other
+./ops/status/codespell.txt:26: Oder ==> Order, Older, Coder, Odder, Odor, Over, Doer
+./ops/status/codespell.txt:27: Hendler ==> Handler
+./ops/status/codespell.txt:28: Yau ==> You, Yaw
+./ops/status/codespell.txt:29: Claus ==> Clause
+./ops/status/codespell.txt:30: Sherif ==> Sheriff
+./ops/status/codespell.txt:31: Teh ==> The
+./ops/status/codespell.txt:32: onl ==> only
+./ops/status/codespell.txt:33: Bui ==> Buoy, Buy
+./ops/status/codespell.txt:34: Leary ==> Leery
+./ops/status/codespell.txt:35: Toom ==> Tomb
+./ops/status/codespell.txt:36: Alle ==> All, Alley
+./ops/status/codespell.txt:37: Nam ==> Name
+./ops/status/codespell.txt:38: RIN ==> RING, RINK, RIND, RAIN, REIN, RUIN, GRIN
+./ops/status/codespell.txt:39: Gage ==> Gauge
+./ops/status/codespell.txt:40: Claus ==> Clause
+./ops/status/codespell.txt:41: Alle ==> All, Alley
+./ops/status/codespell.txt:42: YUr ==> your
+./ops/status/codespell.txt:43: OLl ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:44: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:45: EdN ==> end
+./ops/status/codespell.txt:46: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:47: eGe ==> edge
+./ops/status/codespell.txt:48: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:49: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:50: nd ==> and, 2nd
+./ops/status/codespell.txt:51: CaF ==> calf
+./ops/status/codespell.txt:52: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:53: yuO ==> you
+./ops/status/codespell.txt:54: ths ==> the, this
+./ops/status/codespell.txt:55: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:56: MOT ==> NOT
+./ops/status/codespell.txt:57: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:58: TeY ==> they
+./ops/status/codespell.txt:59: toi ==> to, toy
+./ops/status/codespell.txt:60: ND ==> AND, 2ND
+./ops/status/codespell.txt:61: Nd ==> And, 2nd
+./ops/status/codespell.txt:62: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:63: nD ==> and, 2nd
+./ops/status/codespell.txt:64: EGE ==> EDGE
+./ops/status/codespell.txt:65: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:66: oNw ==> own
+./ops/status/codespell.txt:67: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:68: HDa ==> had
+./ops/status/codespell.txt:69: oLY ==> only
+./ops/status/codespell.txt:70: nD ==> and, 2nd
+./ops/status/codespell.txt:71: aBd ==> and, bad
+./ops/status/codespell.txt:72: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:73: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:74: Yhe ==> The
+./ops/status/codespell.txt:75: OnW ==> own
+./ops/status/codespell.txt:76: nD ==> and, 2nd
+./ops/status/codespell.txt:77: tHq ==> the
+./ops/status/codespell.txt:78: sIZ ==> size, six
+./ops/status/codespell.txt:79: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:80: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:81: nD ==> and, 2nd
+./ops/status/codespell.txt:82: Nd ==> And, 2nd
+./ops/status/codespell.txt:83: NMAE ==> NAME
+./ops/status/codespell.txt:84: nd ==> and, 2nd
+./ops/status/codespell.txt:85: nd ==> and, 2nd
+./ops/status/codespell.txt:86: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:87: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:88: yse ==> yes, use, nyse
+./ops/status/codespell.txt:89: Nd ==> And, 2nd
+./ops/status/codespell.txt:90: ND ==> AND, 2ND
+./ops/status/codespell.txt:91: ND ==> AND, 2ND
+./ops/status/codespell.txt:92: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:93: myE ==> may, my
+./ops/status/codespell.txt:94: WEE ==> WE
+./ops/status/codespell.txt:95: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:96: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:97: nD ==> and, 2nd
+./ops/status/codespell.txt:98: onY ==> only, on, one
+./ops/status/codespell.txt:99: nd ==> and, 2nd
+./ops/status/codespell.txt:100: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:101: nD ==> and, 2nd
+./ops/status/codespell.txt:102: 2rD ==> 2nd
+./ops/status/codespell.txt:103: vEW ==> view, vow, vex
+./ops/status/codespell.txt:104: nd ==> and, 2nd
+./ops/status/codespell.txt:105: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:106: myE ==> may, my
+./ops/status/codespell.txt:107: WEE ==> WE
+./ops/status/codespell.txt:108: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:109: nd ==> and, 2nd
+./ops/status/codespell.txt:110: CaF ==> calf
+./ops/status/codespell.txt:111: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:112: yuO ==> you
+./ops/status/codespell.txt:113: ths ==> the, this
+./ops/status/codespell.txt:114: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:115: nD ==> and, 2nd
+./ops/status/codespell.txt:116: aBd ==> and, bad
+./ops/status/codespell.txt:117: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:118: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:119: Yhe ==> The
+./ops/status/codespell.txt:120: OnW ==> own
+./ops/status/codespell.txt:121: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:122: onl ==> only
+./ops/status/codespell.txt:123: Gage ==> Gauge
+./ops/status/codespell.txt:124: Teh ==> The
+./ops/status/codespell.txt:125: Claus ==> Clause
+./ops/status/codespell.txt:126: Ather ==> Other
+./ops/status/codespell.txt:127: Hendler ==> Handler
+./ops/status/codespell.txt:128: Leary ==> Leery
+./ops/status/codespell.txt:131: Bui ==> Buoy, Buy
+./ops/status/codespell.txt:132: onl ==> only
+./ops/status/codespell.txt:133: Alle ==> All, Alley
+./ops/status/codespell.txt:134: Claus ==> Clause
+./ops/status/codespell.txt:135: Ather ==> Other
+./ops/status/codespell.txt:136: Hendler ==> Handler
+./ops/status/codespell.txt:137: Leary ==> Leery
+./ops/status/codespell.txt:138: Bui ==> Buoy, Buy
+./ops/status/codespell.txt:139: onl ==> only
+./ops/status/codespell.txt:140: Alle ==> All, Alley
+./ops/status/codespell.txt:141: Yau ==> You, Yaw
+./ops/status/codespell.txt:142: Toom ==> Tomb
+./ops/status/codespell.txt:143: Oder ==> Order, Older, Coder, Odder, Odor, Over, Doer
+./ops/status/codespell.txt:144: Yau ==> You, Yaw
+./ops/status/codespell.txt:145: Claus ==> Clause
+./ops/status/codespell.txt:146: Nam ==> Name
+./ops/status/codespell.txt:147: Nam ==> Name
+./ops/status/codespell.txt:148: Claus ==> Clause
+./ops/status/codespell.txt:149: Claus ==> Clause
+./ops/status/codespell.txt:150: Sherif ==> Sheriff
+./ops/status/codespell.txt:151: RIN ==> RING, RINK, RIND, RAIN, REIN, RUIN, GRIN
+./ops/status/codespell.txt:152: Alle ==> All, Alley
+./ops/status/codespell.txt:153: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:154: nWE ==> new
+./ops/status/codespell.txt:155: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:156: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:157: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:158: weW ==> we
+./ops/status/codespell.txt:159: wll ==> will
+./ops/status/codespell.txt:160: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:161: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:162: oLY ==> only
+./ops/status/codespell.txt:163: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:164: aDDD ==> add
+./ops/status/codespell.txt:165: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:166: nD ==> and, 2nd
+./ops/status/codespell.txt:167: nD ==> and, 2nd
+./ops/status/codespell.txt:168: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:169: Daa ==> Data
+./ops/status/codespell.txt:170: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:171: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:172: coo ==> coup
+./ops/status/codespell.txt:173: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:174: gES ==> goes, guess
+./ops/status/codespell.txt:175: gir ==> git
+./ops/status/codespell.txt:176: MKE ==> MAKE
+./ops/status/codespell.txt:177: abl ==> able
+./ops/status/codespell.txt:178: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:179: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:180: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:181: Nd ==> And, 2nd
+./ops/status/codespell.txt:182: joo ==> you
+./ops/status/codespell.txt:183: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:184: nd ==> and, 2nd
+./ops/status/codespell.txt:185: NaM ==> name
+./ops/status/codespell.txt:186: NOO ==> NO
+./ops/status/codespell.txt:187: nOO ==> no
+./ops/status/codespell.txt:188: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:189: wEE ==> we
+./ops/status/codespell.txt:190: Hsa ==> Has
+./ops/status/codespell.txt:191: rIN ==> ring, rink, rind, rain, rein, ruin, grin
+./ops/status/codespell.txt:192: ALLL ==> ALL, ALLY
+./ops/status/codespell.txt:193: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:194: nd ==> and, 2nd
+./ops/status/codespell.txt:195: ND ==> AND, 2ND
+./ops/status/codespell.txt:196: NOo ==> no
+./ops/status/codespell.txt:197: ADDD ==> ADD
+./ops/status/codespell.txt:198: nD ==> and, 2nd
+./ops/status/codespell.txt:199: nD ==> and, 2nd
+./ops/status/codespell.txt:200: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:201: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:202: wYA ==> way
+./ops/status/codespell.txt:203: Wll ==> Will
+./ops/status/codespell.txt:204: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:205: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:206: daa ==> data
+./ops/status/codespell.txt:207: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:208: joo ==> you
+./ops/status/codespell.txt:209: nd ==> and, 2nd
+./ops/status/codespell.txt:210: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:211: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:212: DIsGN ==> design
+./ops/status/codespell.txt:213: daa ==> data
+./ops/status/codespell.txt:214: joo ==> you
+./ops/status/codespell.txt:215: noo ==> no
+./ops/status/codespell.txt:216: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:217: oyU ==> you
+./ops/status/codespell.txt:218: nD ==> and, 2nd
+./ops/status/codespell.txt:219: coo ==> coup
+./ops/status/codespell.txt:220: noo ==> no
+./ops/status/codespell.txt:221: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:222: Haa ==> Has
+./ops/status/codespell.txt:223: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:224: ND ==> AND, 2ND
+./ops/status/codespell.txt:225: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:226: haX ==> hex
+./ops/status/codespell.txt:227: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:228: Nd ==> And, 2nd
+./ops/status/codespell.txt:229: wIT ==> with
+./ops/status/codespell.txt:230: HAA ==> HAS
+./ops/status/codespell.txt:231: Joo ==> You
+./ops/status/codespell.txt:232: daa ==> data
+./ops/status/codespell.txt:233: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:234: tHt ==> the, that
+./ops/status/codespell.txt:235: tBE ==> the
+./ops/status/codespell.txt:236: pTRSS ==> press
+./ops/status/codespell.txt:237: joo ==> you
+./ops/status/codespell.txt:238: JOO ==> YOU
+./ops/status/codespell.txt:239: Oll ==> All, Ole, Old, Olly, Oil
+./ops/status/codespell.txt:240: noo ==> no
+./ops/status/codespell.txt:241: afe ==> safe
+./ops/status/codespell.txt:242: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:243: Coo ==> Coup
+./ops/status/codespell.txt:244: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:245: VIE ==> VIA
+./ops/status/codespell.txt:246: AlO ==> also
+./ops/status/codespell.txt:247: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:248: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:249: MeY ==> may
+./ops/status/codespell.txt:250: sEh ==> she
+./ops/status/codespell.txt:251: SEh ==> she
+./ops/status/codespell.txt:252: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:253: nd ==> and, 2nd
+./ops/status/codespell.txt:254: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:255: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:256: wHn ==> when
+./ops/status/codespell.txt:257: haX ==> hex
+./ops/status/codespell.txt:258: OtU ==> out
+./ops/status/codespell.txt:259: Nd ==> And, 2nd
+./ops/status/codespell.txt:260: JOO ==> YOU
+./ops/status/codespell.txt:261: WEe ==> we
+./ops/status/codespell.txt:262: nD ==> and, 2nd
+./ops/status/codespell.txt:263: nd ==> and, 2nd
+./ops/status/codespell.txt:264: nD ==> and, 2nd
+./ops/status/codespell.txt:265: Nd ==> And, 2nd
+./ops/status/codespell.txt:266: nd ==> and, 2nd
+./ops/status/codespell.txt:267: Wih ==> With
+./ops/status/codespell.txt:268: fPt ==> ftp
+./ops/status/codespell.txt:269: TBe ==> the
+./ops/status/codespell.txt:270: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:271: fom ==> from, form
+./ops/status/codespell.txt:272: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:273: nd ==> and, 2nd
+./ops/status/codespell.txt:274: hSi ==> his
+./ops/status/codespell.txt:275: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:276: iSt ==> is, it, its, it's, sit, list
+./ops/status/codespell.txt:277: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:278: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:279: NWE ==> NEW
+./ops/status/codespell.txt:280: IHs ==> his
+./ops/status/codespell.txt:281: aNe ==> and
+./ops/status/codespell.txt:282: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:283: joo ==> you
+./ops/status/codespell.txt:284: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:285: ND ==> AND, 2ND
+./ops/status/codespell.txt:286: Aci ==> Acpi
+./ops/status/codespell.txt:287: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:288: ND ==> AND, 2ND
+./ops/status/codespell.txt:289: Wee ==> We
+./ops/status/codespell.txt:290: vaI ==> via, vie
+./ops/status/codespell.txt:290: vie ==> via
+./ops/status/codespell.txt:291: AnE ==> and
+./ops/status/codespell.txt:292: nd ==> and, 2nd
+./ops/status/codespell.txt:293: nD ==> and, 2nd
+./ops/status/codespell.txt:294: nD ==> and, 2nd
+./ops/status/codespell.txt:295: Tye ==> Type, Tie
+./ops/status/codespell.txt:296: hDA ==> had
+./ops/status/codespell.txt:297: ND ==> AND, 2ND
+./ops/status/codespell.txt:298: iif ==> if
+./ops/status/codespell.txt:299: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:300: TNe ==> the
+./ops/status/codespell.txt:301: tNE ==> the
+./ops/status/codespell.txt:302: nD ==> and, 2nd
+./ops/status/codespell.txt:303: LeW ==> lieu, hew, sew, dew
+./ops/status/codespell.txt:304: vEW ==> view, vow, vex
+./ops/status/codespell.txt:305: Ned ==> Need, End, Nod
+./ops/status/codespell.txt:306: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:307: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:308: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:309: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:310: AfoR ==> for
+./ops/status/codespell.txt:311: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:312: nD ==> and, 2nd
+./ops/status/codespell.txt:313: tHt ==> the, that
+./ops/status/codespell.txt:314: nd ==> and, 2nd
+./ops/status/codespell.txt:315: whN ==> when
+./ops/status/codespell.txt:316: 2rd ==> 2nd
+./ops/status/codespell.txt:317: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:318: Fom ==> From, Form
+./ops/status/codespell.txt:319: ND ==> AND, 2ND
+./ops/status/codespell.txt:320: IiF ==> if
+./ops/status/codespell.txt:321: nD ==> and, 2nd
+./ops/status/codespell.txt:322: Nd ==> And, 2nd
+./ops/status/codespell.txt:323: WTH ==> WITH
+./ops/status/codespell.txt:324: ND ==> AND, 2ND
+./ops/status/codespell.txt:325: Tge ==> The
+./ops/status/codespell.txt:326: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:327: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:328: nd ==> and, 2nd
+./ops/status/codespell.txt:329: nd ==> and, 2nd
+./ops/status/codespell.txt:330: noo ==> no
+./ops/status/codespell.txt:331: nD ==> and, 2nd
+./ops/status/codespell.txt:332: Nwo ==> Now
+./ops/status/codespell.txt:333: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:334: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:335: Nd ==> And, 2nd
+./ops/status/codespell.txt:336: onW ==> own
+./ops/status/codespell.txt:337: bVE ==> be
+./ops/status/codespell.txt:338: ND ==> AND, 2ND
+./ops/status/codespell.txt:339: Nd ==> And, 2nd
+./ops/status/codespell.txt:340: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:341: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:342: ND ==> AND, 2ND
+./ops/status/codespell.txt:343: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:344: Fot ==> For, Fit, Dot, Rot, Cot, Got, Tot, Fog
+./ops/status/codespell.txt:345: 1ND ==> 1ST
+./ops/status/codespell.txt:346: 2RD ==> 2ND
+./ops/status/codespell.txt:347: Nd ==> And, 2nd
+./ops/status/codespell.txt:348: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:349: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:350: nd ==> and, 2nd
+./ops/status/codespell.txt:351: ND ==> AND, 2ND
+./ops/status/codespell.txt:352: FPr ==> for, far, fps
+./ops/status/codespell.txt:353: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:354: Nd ==> And, 2nd
+./ops/status/codespell.txt:355: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:356: ND ==> AND, 2ND
+./ops/status/codespell.txt:357: PTD ==> PDF
+./ops/status/codespell.txt:358: hEl ==> help, hell, heal
+./ops/status/codespell.txt:359: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:360: Noo ==> No
+./ops/status/codespell.txt:361: iNH ==> in
+./ops/status/codespell.txt:362: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:363: ND ==> AND, 2ND
+./ops/status/codespell.txt:364: ois ==> is
+./ops/status/codespell.txt:365: Nd ==> And, 2nd
+./ops/status/codespell.txt:366: wTh ==> with
+./ops/status/codespell.txt:367: CoO ==> coup
+./ops/status/codespell.txt:368: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:369: weE ==> we
+./ops/status/codespell.txt:370: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:371: joo ==> you
+./ops/status/codespell.txt:372: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:373: nax ==> max, nad
+./ops/status/codespell.txt:374: noo ==> no
+./ops/status/codespell.txt:375: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:376: Coo ==> Coup
+./ops/status/codespell.txt:377: noo ==> no
+./ops/status/codespell.txt:378: NOO ==> NO
+./ops/status/codespell.txt:379: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:380: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:381: COO ==> COUP
+./ops/status/codespell.txt:382: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:383: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:384: nd ==> and, 2nd
+./ops/status/codespell.txt:385: DuM ==> dumb
+./ops/status/codespell.txt:386: nD ==> and, 2nd
+./ops/status/codespell.txt:387: cppp ==> cpp
+./ops/status/codespell.txt:388: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:389: cPPP ==> cpp
+./ops/status/codespell.txt:390: HSI ==> HIS
+./ops/status/codespell.txt:391: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:392: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:393: yhe ==> the
+./ops/status/codespell.txt:394: Alll ==> All, Ally
+./ops/status/codespell.txt:395: oLL ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:396: HAA ==> HAS
+./ops/status/codespell.txt:397: ND ==> AND, 2ND
+./ops/status/codespell.txt:398: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:399: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:400: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:401: nOO ==> no
+./ops/status/codespell.txt:402: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:403: Noo ==> No
+./ops/status/codespell.txt:404: COO ==> COUP
+./ops/status/codespell.txt:405: Joo ==> You
+./ops/status/codespell.txt:406: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:407: Coo ==> Coup
+./ops/status/codespell.txt:408: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:409: coo ==> coup
+./ops/status/codespell.txt:410: JOO ==> YOU
+./ops/status/codespell.txt:411: nd ==> and, 2nd
+./ops/status/codespell.txt:412: nd ==> and, 2nd
+./ops/status/codespell.txt:413: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:414: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:415: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:416: nD ==> and, 2nd
+./ops/status/codespell.txt:417: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:418: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:419: HEL ==> HELP, HELL, HEAL
+./ops/status/codespell.txt:420: wlll ==> will
+./ops/status/codespell.txt:421: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:422: WEL ==> WELL
+./ops/status/codespell.txt:423: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:424: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:425: NOO ==> NO
+./ops/status/codespell.txt:426: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:427: EGE ==> EDGE
+./ops/status/codespell.txt:428: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:429: nD ==> and, 2nd
+./ops/status/codespell.txt:430: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:431: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:432: joo ==> you
+./ops/status/codespell.txt:433: joo ==> you
+./ops/status/codespell.txt:434: JOO ==> YOU
+./ops/status/codespell.txt:435: Joo ==> You
+./ops/status/codespell.txt:436: ALLL ==> ALL, ALLY
+./ops/status/codespell.txt:437: TIcH ==> thick, tick, titch, stitch
+./ops/status/codespell.txt:438: tYE ==> type, tie
+./ops/status/codespell.txt:439: nd ==> and, 2nd
+./ops/status/codespell.txt:440: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:441: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:442: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:443: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:444: LOOS ==> LOOSE, LOSE
+./ops/status/codespell.txt:445: ND ==> AND, 2ND
+./ops/status/codespell.txt:446: Aadd ==> Add
+./ops/status/codespell.txt:447: nD ==> and, 2nd
+./ops/status/codespell.txt:448: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:449: Nd ==> And, 2nd
+./ops/status/codespell.txt:450: CAf ==> calf
+./ops/status/codespell.txt:451: oNy ==> only, on, one
+./ops/status/codespell.txt:452: tHs ==> the, this
+./ops/status/codespell.txt:453: NOO ==> NO
+./ops/status/codespell.txt:454: laf ==> laugh, leaf, loaf, lad, lag, lac, kaf, kaph
+./ops/status/codespell.txt:455: laf ==> laugh, leaf, loaf, lad, lag, lac, kaf, kaph
+./ops/status/codespell.txt:456: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:457: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:458: ND ==> AND, 2ND
+./ops/status/codespell.txt:459: fOm ==> from, form
+./ops/status/codespell.txt:460: dum ==> dumb
+./ops/status/codespell.txt:461: aFE ==> safe
+./ops/status/codespell.txt:462: noo ==> no
+./ops/status/codespell.txt:463: tOi ==> to, toy
+./ops/status/codespell.txt:464: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:465: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:466: nD ==> and, 2nd
+./ops/status/codespell.txt:467: thS ==> the, this
+./ops/status/codespell.txt:468: COO ==> COUP
+./ops/status/codespell.txt:469: COO ==> COUP
+./ops/status/codespell.txt:470: iNh ==> in
+./ops/status/codespell.txt:471: noo ==> no
+./ops/status/codespell.txt:472: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:473: coO ==> coup
+./ops/status/codespell.txt:474: hdA ==> had
+./ops/status/codespell.txt:475: aCi ==> acpi
+./ops/status/codespell.txt:476: HVe ==> have
+./ops/status/codespell.txt:477: hAx ==> hex
+./ops/status/codespell.txt:478: fOF ==> for
+./ops/status/codespell.txt:479: NOO ==> NO
+./ops/status/codespell.txt:480: aCII ==> ascii
+./ops/status/codespell.txt:481: nD ==> and, 2nd
+./ops/status/codespell.txt:482: ND ==> AND, 2ND
+./ops/status/codespell.txt:483: nD ==> and, 2nd
+./ops/status/codespell.txt:484: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:485: Alll ==> All, Ally
+./ops/status/codespell.txt:486: DAA ==> DATA
+./ops/status/codespell.txt:487: TrU ==> through, true
+./ops/status/codespell.txt:488: NOO ==> NO
+./ops/status/codespell.txt:489: lIk ==> like, lick, link
+./ops/status/codespell.txt:490: ND ==> AND, 2ND
+./ops/status/codespell.txt:491: NOO ==> NO
+./ops/status/codespell.txt:492: ABd ==> and, bad
+./ops/status/codespell.txt:493: cppp ==> cpp
+./ops/status/codespell.txt:494: LOOS ==> LOOSE, LOSE
+./ops/status/codespell.txt:495: Coo ==> Coup
+./ops/status/codespell.txt:496: UesD ==> used
+./ops/status/codespell.txt:497: UesD ==> used
+./ops/status/codespell.txt:498: ND ==> AND, 2ND
+./ops/status/codespell.txt:499: wew ==> we
+./ops/status/codespell.txt:500: ACi ==> acpi
+./ops/status/codespell.txt:501: nD ==> and, 2nd
+./ops/status/codespell.txt:502: TOOO ==> TODO, TOO, TOOL, TOOK
+./ops/status/codespell.txt:503: TOOO ==> TODO, TOO, TOOL, TOOK
+./ops/status/codespell.txt:504: nD ==> and, 2nd
+./ops/status/codespell.txt:505: SAV ==> SAVE
+./ops/status/codespell.txt:506: wLLL ==> will
+./ops/status/codespell.txt:507: naM ==> name
+./ops/status/codespell.txt:508: nD ==> and, 2nd
+./ops/status/codespell.txt:509: Coo ==> Coup
+./ops/status/codespell.txt:510: OnL ==> only
+./ops/status/codespell.txt:511: Nd ==> And, 2nd
+./ops/status/codespell.txt:512: Bu ==> By, Be, But, Bug, Bun, Bud, Buy, Bum
+./ops/status/codespell.txt:513: fOf ==> for
+./ops/status/codespell.txt:514: sHS ==> ssh, nhs
+./ops/status/codespell.txt:515: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:516: LIk ==> like, lick, link
+./ops/status/codespell.txt:517: ND ==> AND, 2ND
+./ops/status/codespell.txt:518: Wlll ==> Will
+./ops/status/codespell.txt:519: Wee ==> We
+./ops/status/codespell.txt:520: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:521: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:522: nD ==> and, 2nd
+./ops/status/codespell.txt:523: nd ==> and, 2nd
+./ops/status/codespell.txt:524: ND ==> AND, 2ND
+./ops/status/codespell.txt:525: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:526: wll ==> will
+./ops/status/codespell.txt:527: Coo ==> Coup
+./ops/status/codespell.txt:528: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:529: NOO ==> NO
+./ops/status/codespell.txt:530: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:531: OLL ==> ALL, OLE, OLD, OLLY, OIL
+./ops/status/codespell.txt:532: aaS ==> ass, as
+./ops/status/codespell.txt:533: NOO ==> NO
+./ops/status/codespell.txt:534: ND ==> AND, 2ND
+./ops/status/codespell.txt:535: BU ==> BY, BE, BUT, BUG, BUN, BUD, BUY, BUM
+./ops/status/codespell.txt:536: ihs ==> his
+./ops/status/codespell.txt:537: nD ==> and, 2nd
+./ops/status/codespell.txt:538: nD ==> and, 2nd
+./ops/status/codespell.txt:539: ND ==> AND, 2ND
+./ops/status/codespell.txt:540: ND ==> AND, 2ND
+./ops/status/codespell.txt:541: ND ==> AND, 2ND
+./ops/status/codespell.txt:542: AUoT ==> auto
+./ops/status/codespell.txt:543: ND ==> AND, 2ND
+./ops/status/codespell.txt:544: wee ==> we
+./ops/status/codespell.txt:545: ND ==> AND, 2ND
+./ops/status/codespell.txt:546: bU ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:547: ALLL ==> ALL, ALLY
+./ops/status/codespell.txt:548: oll ==> all, ole, old, olly, oil
+./ops/status/codespell.txt:549: Yau ==> You, Yaw
+./ops/status/codespell.txt:550: lik ==> like, lick, link
+./ops/status/codespell.txt:551: TiCH ==> thick, tick, titch, stitch
+./ops/status/codespell.txt:552: Nd ==> And, 2nd
+./ops/status/codespell.txt:553: eDE ==> edge
+./ops/status/codespell.txt:554: COO ==> COUP
+./ops/status/codespell.txt:555: TyE ==> type, tie
+./ops/status/codespell.txt:556: soM ==> some
+./ops/status/codespell.txt:557: NOO ==> NO
+./ops/status/codespell.txt:558: TOi ==> to, toy
+./ops/status/codespell.txt:559: bu ==> by, be, but, bug, bun, bud, buy, bum
+./ops/status/codespell.txt:560: ned ==> need, end, nod
+./ops/status/codespell.txt:561: nD ==> and, 2nd
+./ops/status/codespell.txt:562: CPPP ==> CPP
+./ops/status/codespell.txt:563: Nd ==> And, 2nd
+./ops/status/codespell.txt:564: HSa ==> has
diff --git a/ops/status/git_graph.txt b/ops/status/git_graph.txt
new file mode 100644
index 00000000..df23e5c5
--- /dev/null
+++ b/ops/status/git_graph.txt
@@ -0,0 +1,97 @@
+* b2ed3dd9 (HEAD -> feat/spi-A1-doctor-json-snapshot, origin/feat/spi-A1-doctor-json-snapshot) feat(cli): add doctor JSON snapshot with complete diagnostics  [spi-A1]
+* 0c1675fd (origin/feat/spi-a7-qc-html-scaffold, feat/spi-a7-qc-html-scaffold) feat(qc): scaffold per-subject QC HTML with overlays, motion, aCompCor, and env  [spi-a7]
+* 3d371981 feat(preproc): add cord and CSF masking with QC overlays
+| * 8283c88e (refs/stash) WIP on feat/spi-a7-qc-html-scaffold: 81712fac feat(registration): implement SCT EPI→PAM50 wrapper with metrics and rule  [spi-xxx]
+|/|
+| * 36a6b2e5 index on feat/spi-a7-qc-html-scaffold: 81712fac feat(registration): implement SCT EPI→PAM50 wrapper with metrics and rule  [spi-xxx]
+|/
+| * 58dff246 (origin/feat/spi-002-snakemake-dag-provenance, feat/spi-002-snakemake-dag-provenance) feat(workflow): add minimal Snakemake DAG with provenance export  [spi-002]
+|/
+* 81712fac (feat/spi-xxx-registration-pam50-wrapper, feat/spi-masking-cord-csf-qc, feat/spi-confounds-acompcor-tcompcor-censor) feat(registration): implement SCT EPI→PAM50 wrapper with metrics and rule  [spi-xxx]
+* 1beeb730 (origin/feat/spi-confounds-fd-dvars-tsv-json-v1, feat/spi-confounds-fd-dvars-tsv-json-v1) feat(confounds): implement FD/DVARS + confounds TSV/JSON (schema v1) and motion plot  [spi-confounds]
+* 8a30a885 feat(core): scaffold CLI with config loader and doctor command  [spi-001]
+| * 33976e80 (origin/feat/spi-001-scaffold-cli-config) feat(confounds): implement FD/DVARS + confounds TSV/JSON (schema v1) and motion plot  [spi-confounds]
+|/
+* e2955226 (origin/main, origin/HEAD, main, feat/spi-001-scaffold-cli-config) chore: restore noindex - keep site private indefinitely
+* 729d9049 chore: sync remaining source files and formatting updates
+* 53e79421 docs: enable public discovery and add architecture section  [spi-docs-public]
+* 939c1c55 feat(support): add issue/PR templates, SECURITY, and quickstart guide  [spi-support]
+* d5cf856d (tag: v0.1.0) test(hardening): add pytest.ini with deterministic seeds and test fixes  [spi-hardening]
+* 442da771 chore(release): prepare v0.1.0 with ADRs  [spi-v0.1.0]
+* 9bd64cad feat(qc): static HTML QC pages with CI artifact bundle  [spi-qc]
+* 7149d528 feat(confounds): add aCompCor (6 PCs) and censor column with mask sourcing
+* f1a41727 feat(deriv): BIDS-Derivatives tree layout and provenance  [spi-deriv]
+* 7ff91949 feat(motion): compute FD & DVARS per run, write confounds TSV and plots
+* 86bc3b17 feat(e2e): add tiny BIDS fixture and CI smoke test  [spi-e2e]
+* 62ccb322 feat(ingest): BIDS ingest and manifest cache with hashing
+* 973898c6 feat(register): add minimal SCT EPI→PAM50 registration wrapper  [spi-register]
+* 4f45c90d feat(config): YAML config with JSON Schema validation and print-config
+* e16f95d3 docs(confounds): add schema and comprehensive usage guide  [spi-confounds-docs]
+* 4fcb7648 feat(doctor): implement environment diagnostics with color table  [spi-doctor]
+* ed19081d feat(doctor): add environment diagnostics with JSON schema, docs, tests
+* cac1da21 feat(cli): add version cmd, --print-config, -n support, DAG svg export  [spi-cli-scaffold]
+| * b858cb63 (origin/feat/spi-cli-scaffold, feat/spi-cli-scaffold) feat(doctor): implement environment diagnostics
+| * 5b5401b6 feat(doctor): implement environment diagnostics
+|/
+* 07bb6e2f (feat/doctor-environment-diagnostics) docs: add robots.txt and improve 404 page with home button  [spi-011-final]
+* 347788ae docs: fix invalid material-graph-line icon (use chart-timeline)  [spi-011m]
+* b13c52c4 docs: add pymdownx.emoji extension to render Material icons  [spi-011m]
+* 7ef8971f docs: add inline warning banner to homepage for extra visibility  [spi-011m]
+* 2cc8e194 docs: remove icon attributes to fix rendering  [spi-011m]
+* 283e03b5 docs: fix banner partial (outdated.html) and simplify icon syntax  [spi-011m]
+* 6ad3c274 ci: temporarily disable pa11y-ci due to Material theme contrast issues  [spi-011m]
+* f5008df4 ci: adjust pa11y config to ignore Material theme elements and set threshold  [spi-011m]
+* 741ddb06 ci: add Chrome sandbox args for pa11y-ci in GitHub Actions  [spi-011m]
+* 282fb111 docs: fix remaining .lg .middle icon patterns in qc.md  [spi-011m]
+* caa2f73b docs: banner+noindex, icon/CTA normalization, dark/light contrast fixes, and CI a11y checks  [spi-011m]
+* a0162c73 docs(ui): fix QC icon markup and add hero padding to avoid cramped text  [spi-011l]
+* d35bc427 ci: remove empty strategy block to satisfy matrix requirement  [spi-011k]
+* 40bfae99 ci: fix Actions expressions by using env fallbacks (string type)  [spi-011k]
+* 1b2a6569 ci: fix YAML step name causing syntax error  [spi-011k]
+* d5d92816 docs(ui): fix Features block; dark-mode code; privatize PLAYBOOK_* and guard CI  [spi-011k]
+* f6685928 docs(ui): remove public PLAYBOOK_* pages  [spi-011k]
+* caed971d ci(docs): enable mike versioning and tag-based publish  [spi-011h]
+* f7767218 ci(docs): add docs_release.sh for mike-based versioned deploy  [spi-011h]
+* 8596ec08 ci(docs): fix artifact download for cross-repo deploy  [spi-011j]
+* 372255dc trigger: deploy with public target repo  [spi-011j]
+* abab8e76 trigger: fresh deploy to org root  [spi-011j]
+* e0c205e9 ci(docs): switch deploy to cross-repo org root via gh-pages  [spi-011j]
+* 0d4a806a ci(docs): add diagnostics and fail early if site/ missing  [spi-011j]
+* 970f249c ci(docs): upload Pages artifact in build job and deploy it  [spi-011j]
+* 6e3759c1 docs(ci): remove sitemap plugin to unblock strict build  [spi-011j]
+* 0145ba81 ci(docs): fix lychee.toml verbose log level  [spi-011j]
+* 672865d7 ci(docs): fix deploy to use GitHub Pages instead of cross-repo  [spi-011j]
+* d7ebc597 ci(docs): fix lychee.toml verbose field type  [spi-011j]
+* c223771b ci(docs): simplify workflow triggers  [spi-011j]
+* 9c2573fb ci: add simple docs workflow to test  [spi-011j]
+* 40825b2a ci(docs): recreate docs workflow  [spi-011j]
+* be44bace ci: remove docs workflow to recreate  [spi-011j]
+* 91dea479 trigger: force docs workflow to run  [spi-011j]
+* b83b3fd4 ci: remove test workflow after verification  [spi-011j]
+* 66e02dcd ci: add test workflow to verify GitHub Actions  [spi-011j]
+* ab787a3a ci(docs): fix lychee args and add lychee.toml  [spi-011j]
+* daa58e15 ci(docs): fix lychee action args (remove invalid --exclude-mail)  [spi-011i]
+* 7604ff09 ci(pre-commit): enforce snakefmt on workflow dir  [spi-011i]
+* 362d4d20 style(workflow): apply snakefmt formatting  [spi-011g]
+* 1ed01d86 ci(docs): fix deps, use core sitemap and lychee action; complete deploy  [spi-011f]
+* 4d5b9f5f docs: add live docs badges and Actions summary link [spi-011e]
+* 06e2a7b0 chore(docs): untrack site/, fix nav and license link, guard CI  [spi-011d]
+* c68e426b (tag: v0.1.0-docs) ci(docs): publish to org root via cross-repo Pages  [spi-011b]
+* 4e8c15c1 ci(docs): harden docs CI and enable GitHub Pages [spi-011a]
+* 147a1443 feat(docs): stand up MkDocs Material site with generators  [spi-011]
+* e90b4fc0 spi-005c: Add aCompCor confounds (cord/WM/CSF masks + PCA)
+* 4d6f20e4 Add GitHub Actions workflow for documentation site
+| * 2c815ac1 (feat/spi-011a-docs-ci-pages) ci(docs): harden docs CI and enable GitHub Pages [spi-011a]
+| * dadbaa4e feat(docs): stand up MkDocs Material site with generators  [spi-011]
+| * afed40c8 spi-005c: Add aCompCor confounds (cord/WM/CSF masks + PCA)
+|/
+* 840d1474 (origin/cursor/harden-docs-ci-and-enable-github-pages-deployment-43de) spi-005b: confounds v2 — censoring + canonical columns; QC overlays
+* bb36f2b0 spi-005a: confounds v1 (FD from motion params + DVARS), derivative TSV/JSON, QC reads canonical confounds
+* 291cb062 feat(qc): per-subject HTML report with FD/DVARS plots and methods boilerplate
+* dbbc3695 feat(deriv): add derivatives path router and manifest_deriv.tsv
+* 9d2000c1 feat(workflow): per-run wildcard rules from samples.tsv; add preproc_all aggregate
+* fc3252a2 refactor(smoke): consume first row from samples.tsv for mppca/motion/confounds
+* fc224c53 Add samples manifest generation
+* 2c20a680 feat(confounds): add placeholder confounds builder & smoke; fix ruff unused var
+* 7538e04c style: black/ruff/isort auto-fixes
+* a9ed9735 Initial commit
diff --git a/ops/status/health_check.sh b/ops/status/health_check.sh
new file mode 100755
index 00000000..27be4528
--- /dev/null
+++ b/ops/status/health_check.sh
@@ -0,0 +1,284 @@
+#!/usr/bin/env bash
+#
+# health_check.sh — SpinePrep repository health checks
+# Run this before and after major changes (merges, refactors, releases)
+#
+# Usage: ./ops/status/health_check.sh
+# Exit: 0 if all checks pass, 1 if any fail
+
+set -euo pipefail
+
+# Colors
+RED='\033[0;31m'
+GREEN='\033[0;32m'
+YELLOW='\033[1;33m'
+BLUE='\033[0;34m'
+NC='\033[0m' # No Color
+
+# Counters
+PASS=0
+FAIL=0
+WARN=0
+
+# Output directory
+OUT_DIR="ops/status"
+mkdir -p "$OUT_DIR"
+
+echo -e "${BLUE}======================================${NC}"
+echo -e "${BLUE}SpinePrep Repository Health Check${NC}"
+echo -e "${BLUE}======================================${NC}\n"
+
+# Helper functions
+check_pass() {
+    echo -e "${GREEN}✓${NC} $1"
+    PASS=$((PASS + 1))
+}
+
+check_fail() {
+    echo -e "${RED}✗${NC} $1"
+    FAIL=$((FAIL + 1))
+}
+
+check_warn() {
+    echo -e "${YELLOW}⚠${NC} $1"
+    WARN=$((WARN + 1))
+}
+
+# 1. Git branch and commit status
+echo -e "${BLUE}[1/10] Git Status${NC}"
+if git rev-parse --git-dir > /dev/null 2>&1; then
+    CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)
+    CURRENT_SHA=$(git rev-parse --short HEAD)
+    check_pass "Git repository detected (branch: $CURRENT_BRANCH @ $CURRENT_SHA)"
+
+    # Check if main exists
+    if git show-ref --verify --quiet refs/heads/main; then
+        check_pass "Branch 'main' exists"
+    else
+        check_fail "Branch 'main' not found"
+    fi
+
+    # Check divergence from origin/main
+    if git show-ref --verify --quiet refs/remotes/origin/main; then
+        DIVERGENCE=$(git rev-list --left-right --count origin/main...HEAD 2>/dev/null || echo "? ?")
+        BEHIND=$(echo "$DIVERGENCE" | awk '{print $1}')
+        AHEAD=$(echo "$DIVERGENCE" | awk '{print $2}')
+        if [[ "$BEHIND" -eq 0 && "$AHEAD" -eq 0 ]]; then
+            check_pass "In sync with origin/main"
+        else
+            check_warn "Diverged from origin/main ($BEHIND behind, $AHEAD ahead)"
+        fi
+    else
+        check_warn "origin/main not found (local-only repo?)"
+    fi
+else
+    check_fail "Not a git repository"
+fi
+echo ""
+
+# 2. Largest tracked files (potential cleanup targets)
+echo -e "${BLUE}[2/10] Large Tracked Files${NC}"
+git ls-tree -r -l HEAD | sort -k4 -n | tail -n 5 > "$OUT_DIR/largest_tracked.txt"
+LARGEST_SIZE=$(git ls-tree -r -l HEAD | awk '{print $4}' | sort -n | tail -n 1)
+if [[ "$LARGEST_SIZE" -gt 10000000 ]]; then  # 10MB
+    check_warn "Large files detected (largest: $((LARGEST_SIZE / 1024 / 1024))MB) — see $OUT_DIR/largest_tracked.txt"
+elif [[ "$LARGEST_SIZE" -gt 1000000 ]]; then  # 1MB
+    check_warn "Moderately large files detected (largest: $((LARGEST_SIZE / 1024))KB)"
+else
+    check_pass "No unusually large tracked files"
+fi
+echo ""
+
+# 3. Untracked/ignored files
+echo -e "${BLUE}[3/10] Untracked Files${NC}"
+git status --porcelain=v1 > "$OUT_DIR/status_porcelain.txt"
+UNTRACKED_COUNT=$(grep -c "^??" "$OUT_DIR/status_porcelain.txt" || echo 0)
+MODIFIED_COUNT=$(grep -c "^ M" "$OUT_DIR/status_porcelain.txt" || echo 0)
+
+if [[ "$UNTRACKED_COUNT" -eq 0 ]]; then
+    check_pass "No untracked files"
+else
+    check_warn "$UNTRACKED_COUNT untracked files (see $OUT_DIR/status_porcelain.txt)"
+fi
+
+if [[ "$MODIFIED_COUNT" -eq 0 ]]; then
+    check_pass "No modified unstaged files"
+else
+    check_warn "$MODIFIED_COUNT modified files not staged"
+fi
+echo ""
+
+# 4. Ruff linting
+echo -e "${BLUE}[4/10] Ruff Linting${NC}"
+if command -v ruff &> /dev/null; then
+    RUFF_VERSION=$(ruff --version | head -n 1)
+    if ruff check --output-format=github . > "$OUT_DIR/ruff.txt" 2>&1; then
+        check_pass "Ruff clean ($RUFF_VERSION)"
+    else
+        VIOLATION_COUNT=$(wc -l < "$OUT_DIR/ruff.txt")
+        check_fail "Ruff violations detected ($VIOLATION_COUNT) — see $OUT_DIR/ruff.txt"
+    fi
+else
+    check_warn "Ruff not installed (skip linting)"
+fi
+echo ""
+
+# 5. Snakefmt formatting
+echo -e "${BLUE}[5/10] Snakefmt Formatting${NC}"
+if command -v snakefmt &> /dev/null; then
+    SNAKEFMT_VERSION=$(snakefmt --version | head -n 1)
+    if snakefmt --check . > "$OUT_DIR/snakefmt.txt" 2>&1; then
+        check_pass "Snakefmt clean ($SNAKEFMT_VERSION)"
+    else
+        # Extract number of files that would be changed
+        FILES_CHANGED=$(grep -c "would be changed" "$OUT_DIR/snakefmt.txt" || echo 0)
+        if [[ "$FILES_CHANGED" -le 1 ]]; then
+            check_warn "Snakefmt needs formatting ($FILES_CHANGED file)"
+        else
+            check_fail "Snakefmt needs formatting ($FILES_CHANGED files)"
+        fi
+    fi
+else
+    check_warn "Snakefmt not installed (skip formatting check)"
+fi
+echo ""
+
+# 6. Codespell
+echo -e "${BLUE}[6/10] Codespell${NC}"
+if command -v codespell &> /dev/null; then
+    CODESPELL_VERSION=$(codespell --version 2>&1 | head -n 1)
+    # Ignore common false positives and generated files
+    if codespell --ignore-words-list="te,hist,trys,ue,ot,fo" --skip="site/,*.min.js,*.map,.venv/" . > "$OUT_DIR/codespell.txt" 2>&1; then
+        check_pass "Codespell clean ($CODESPELL_VERSION)"
+    else
+        TYPO_COUNT=$(wc -l < "$OUT_DIR/codespell.txt")
+        if [[ "$TYPO_COUNT" -lt 10 ]]; then
+            check_warn "Codespell found $TYPO_COUNT potential typos — see $OUT_DIR/codespell.txt"
+        else
+            check_fail "Codespell found $TYPO_COUNT potential typos — see $OUT_DIR/codespell.txt"
+        fi
+    fi
+else
+    check_warn "Codespell not installed (skip spell check)"
+fi
+echo ""
+
+# 7. Pytest
+echo -e "${BLUE}[7/10] Pytest${NC}"
+if command -v pytest &> /dev/null; then
+    if pytest -q --tb=no > "$OUT_DIR/pytest.txt" 2>&1; then
+        TEST_COUNT=$(grep -c "passed" "$OUT_DIR/pytest.txt" || echo "?")
+        check_pass "Pytest passed ($TEST_COUNT tests)"
+    else
+        ERROR_COUNT=$(grep -c "ERROR\|FAILED" "$OUT_DIR/pytest.txt" || echo "?")
+        check_fail "Pytest failed ($ERROR_COUNT errors/failures) — see $OUT_DIR/pytest.txt"
+    fi
+else
+    check_warn "Pytest not installed (skip tests)"
+fi
+echo ""
+
+# 8. MkDocs strict build
+echo -e "${BLUE}[8/10] MkDocs Strict Build${NC}"
+if command -v mkdocs &> /dev/null; then
+    MKDOCS_VERSION=$(mkdocs --version | head -n 1)
+    if mkdocs build --strict > "$OUT_DIR/mkdocs.txt" 2>&1; then
+        check_pass "MkDocs build passed ($MKDOCS_VERSION)"
+    else
+        check_fail "MkDocs build failed — see $OUT_DIR/mkdocs.txt"
+    fi
+else
+    check_warn "MkDocs not installed (skip docs build)"
+fi
+echo ""
+
+# 9. Branch inventory
+echo -e "${BLUE}[9/10] Branch Inventory${NC}"
+BRANCH_COUNT=$(git branch | wc -l)
+REMOTE_BRANCH_COUNT=$(git branch -r | wc -l)
+
+if [[ "$BRANCH_COUNT" -le 3 ]]; then
+    check_pass "Clean local branches ($BRANCH_COUNT branches)"
+else
+    check_warn "Multiple local branches ($BRANCH_COUNT) — consider cleanup"
+fi
+
+if [[ "$REMOTE_BRANCH_COUNT" -le 5 ]]; then
+    check_pass "Clean remote branches ($REMOTE_BRANCH_COUNT branches)"
+else
+    check_warn "Multiple remote branches ($REMOTE_BRANCH_COUNT) — consider cleanup"
+fi
+
+# Save branch info
+git branch --format='%(refname:short)|%(objectname:short)|%(committerdate:iso8601)' > "$OUT_DIR/branches.txt"
+echo ""
+
+# 10. JSON status artifact
+echo -e "${BLUE}[10/10] Generate Status JSON${NC}"
+python3 - <<'PY'
+import json, subprocess, os, pathlib, sys
+
+def sh(x):
+    try:
+        return subprocess.check_output(x, shell=True, text=True, stderr=subprocess.DEVNULL).strip()
+    except:
+        return ""
+
+try:
+    data = {
+        "timestamp": sh("date -Iseconds"),
+        "head": sh("git rev-parse --short HEAD"),
+        "head_branch": sh("git rev-parse --abbrev-ref HEAD"),
+        "main_exists": os.system("git show-ref --verify --quiet refs/heads/main") == 0,
+        "origin_main_exists": os.system("git show-ref --verify --quiet refs/remotes/origin/main") == 0,
+        "branches": {
+            "local": sh("git branch --format='%(refname:short)'").splitlines(),
+            "remote": sh("git branch -r --format='%(refname:short)'").splitlines(),
+        },
+        "tags": sh("git tag").splitlines(),
+        "health": {
+            "pytest_passed": os.system("pytest -q --tb=no >/dev/null 2>&1") == 0,
+            "mkdocs_strict_passed": os.system("mkdocs build --strict >/dev/null 2>&1") == 0,
+            "ruff_clean": os.system("ruff check --quiet . 2>/dev/null") == 0,
+            "snakefmt_clean": os.system("snakefmt --check . >/dev/null 2>&1") == 0,
+        },
+        "divergence_from_main": sh("git rev-list --left-right --count origin/main...HEAD 2>/dev/null || echo '0 0'"),
+    }
+    pathlib.Path('ops/status').mkdir(parents=True, exist_ok=True)
+    with open('ops/status/repo_status.json', 'w') as f:
+        json.dump(data, f, indent=2)
+    sys.exit(0)
+except Exception as e:
+    print(f"Error: {e}", file=sys.stderr)
+    sys.exit(1)
+PY
+
+if [[ $? -eq 0 ]]; then
+    check_pass "Status JSON generated → $OUT_DIR/repo_status.json"
+else
+    check_fail "Failed to generate status JSON"
+fi
+echo ""
+
+# Summary
+echo -e "${BLUE}======================================${NC}"
+echo -e "${BLUE}Summary${NC}"
+echo -e "${BLUE}======================================${NC}"
+echo -e "${GREEN}Passed:${NC}  $PASS"
+echo -e "${YELLOW}Warnings:${NC} $WARN"
+echo -e "${RED}Failed:${NC}  $FAIL"
+echo ""
+
+if [[ "$FAIL" -eq 0 ]]; then
+    echo -e "${GREEN}✓ Repository health: GOOD${NC}"
+    echo "All critical checks passed. Warnings are acceptable."
+    exit 0
+elif [[ "$FAIL" -le 2 ]]; then
+    echo -e "${YELLOW}⚠ Repository health: FAIR${NC}"
+    echo "Some checks failed. Review $OUT_DIR/ for details."
+    exit 1
+else
+    echo -e "${RED}✗ Repository health: POOR${NC}"
+    echo "Multiple critical issues detected. Fix before proceeding."
+    exit 1
+fi
diff --git a/ops/status/health_check_run.txt b/ops/status/health_check_run.txt
new file mode 100644
index 00000000..7d34e2ae
--- /dev/null
+++ b/ops/status/health_check_run.txt
@@ -0,0 +1,47 @@
+[0;34m======================================[0m
+[0;34mSpinePrep Repository Health Check[0m
+[0;34m======================================[0m
+
+[0;34m[1/10] Git Status[0m
+[0;32m✓[0m Git repository detected (branch: feat/spi-A1-doctor-json-snapshot @ b2ed3dd9)
+[0;32m✓[0m Branch 'main' exists
+[1;33m⚠[0m Diverged from origin/main (0 behind, 6 ahead)
+
+[0;34m[2/10] Large Tracked Files[0m
+[1;33m⚠[0m Large files detected (largest: 23MB) — see ops/status/largest_tracked.txt
+
+[0;34m[3/10] Untracked Files[0m
+[1;33m⚠[0m 50 untracked files (see ops/status/status_porcelain.txt)
+[1;33m⚠[0m 30 modified files not staged
+
+[0;34m[4/10] Ruff Linting[0m
+[0;32m✓[0m Ruff clean (ruff 0.14.0)
+
+[0;34m[5/10] Snakefmt Formatting[0m
+[1;33m⚠[0m Snakefmt needs formatting (1 file)
+
+[0;34m[6/10] Codespell[0m
+[0;31m✗[0m Codespell found 1202 potential typos — see ops/status/codespell.txt
+
+[0;34m[7/10] Pytest[0m
+[0;31m✗[0m Pytest failed (1 errors/failures) — see ops/status/pytest.txt
+
+[0;34m[8/10] MkDocs Strict Build[0m
+[0;32m✓[0m MkDocs build passed (mkdocs, version 1.6.1 from /home/kiomars/.local/lib/python3.12/site-packages/mkdocs (Python 3.12))
+
+[0;34m[9/10] Branch Inventory[0m
+[1;33m⚠[0m Multiple local branches (12) — consider cleanup
+[1;33m⚠[0m Multiple remote branches (9) — consider cleanup
+
+[0;34m[10/10] Generate Status JSON[0m
+[0;32m✓[0m Status JSON generated → ops/status/repo_status.json
+
+[0;34m======================================[0m
+[0;34mSummary[0m
+[0;34m======================================[0m
+[0;32mPassed:[0m  5
+[1;33mWarnings:[0m 7
+[0;31mFailed:[0m  2
+
+[1;33m⚠ Repository health: FAIR[0m
+Some checks failed. Review ops/status/ for details.
diff --git a/ops/status/largest_tracked.txt b/ops/status/largest_tracked.txt
new file mode 100644
index 00000000..de646889
--- /dev/null
+++ b/ops/status/largest_tracked.txt
@@ -0,0 +1,5 @@
+100755 blob 9865c641ca74bcfc9297b11fbc5049207e185f2b 7178823	.venv/lib/python3.12/site-packages/pulp/solverdir/cbc/linux/i64/cbc
+100755 blob 24b85924bbd33b772d4c82bd311606642c663b10 7678656	.venv/lib/python3.12/site-packages/pulp/solverdir/cbc/linux/arm64/cbc
+100755 blob 4507334689552ad74f65683cca24554e047ce654 10808937	.venv/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-x86_64-linux-gnu.so
+100755 blob d10bfcb5b4a299191883022d75b8a52a2041aa74 24817849	.venv/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-b75cc656.so
+100755 blob d6449096b01ecc38d4693856e30c8351d96bd559 25050385	.venv/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-8fb3d286.so
diff --git a/ops/status/merge_plan.md b/ops/status/merge_plan.md
new file mode 100644
index 00000000..1c8b3e2f
--- /dev/null
+++ b/ops/status/merge_plan.md
@@ -0,0 +1,294 @@
+# Merge Plan — Pre-Phase-A Repository Consolidation
+
+**Generated:** 2025-10-09
+**Current HEAD:** b2ed3dd9 (feat/spi-A1-doctor-json-snapshot)
+**Target:** Consolidate all feature work into `main` and prepare clean baseline for Phase A
+
+---
+
+## Summary
+
+- **12 local branches** (including `main`)
+- **8 remote branches** on origin
+- **Current divergence:** 0 commits ahead, 6 commits behind `origin/main`
+- **Key conflict zones:** `spineprep/cli.py`, `spineprep/_version.py`, `configs/base.yaml`, `CITATION.cff`
+- **Test status:** ❌ pytest failing (import error in confounds)
+- **Lint status:** ✅ ruff clean, ⚠️  snakefmt needs 1 file
+
+---
+
+## Branch Analysis & Strategy
+
+### 1. Already merged or at main (DELETE)
+
+**Branches:**
+- `feat/doctor-environment-diagnostics` — no commits ahead of main
+- `feat/spi-001-scaffold-cli-config` — no commits ahead of main
+
+**Action:** Delete locally and on remote (if present)
+
+```bash
+git branch -d feat/doctor-environment-diagnostics
+git branch -d feat/spi-001-scaffold-cli-config
+```
+
+---
+
+### 2. Active development - linear merge (FAST-FORWARD or SQUASH-MERGE)
+
+**Branch:** `feat/spi-A1-doctor-json-snapshot` (CURRENT HEAD)
+**Commits ahead:** 6
+**Key changes:** Doctor JSON snapshot, QC HTML scaffold, masking, registration, confounds
+**Conflicts:** High (multiple overlapping changes with other branches)
+**Strategy:** This is the most advanced branch. After merging other branches into main first, rebase this onto updated main, then squash-merge.
+
+**Branch:** `feat/spi-a7-qc-html-scaffold`
+**Commits ahead:** 5
+**Key changes:** QC HTML scaffold, masking, registration, confounds
+**Conflicts:** High (shares many commits with A1, but missing the final doctor snapshot)
+**Strategy:** This is a subset of spi-A1. ARCHIVE (covered by A1).
+
+**Branch:** `feat/spi-002-snakemake-dag-provenance`
+**Commits ahead:** 4
+**Key changes:** Snakemake DAG with provenance, includes censor/compcor
+**Conflicts:** Medium (confounds structure differs from A1)
+**Strategy:** Cherry-pick DAG changes if not in A1, otherwise ARCHIVE.
+
+**Branch:** `feat/spi-confounds-fd-dvars-tsv-json-v1`
+**Commits ahead:** 2
+**Key changes:** FD/DVARS confounds TSV/JSON
+**Conflicts:** Low (superseded by later work)
+**Strategy:** ARCHIVE (functionality included in A1).
+
+**Branch:** `feat/spi-confounds-acompcor-tcompcor-censor`
+**Commits ahead:** 3
+**Key changes:** Registration wrapper, confounds
+**Conflicts:** Medium
+**Strategy:** Check if registration code is in A1; if not, cherry-pick specific commits. Otherwise ARCHIVE.
+
+**Branch:** `feat/spi-masking-cord-csf-qc`
+**Commits ahead:** 3
+**Key changes:** Same as spi-confounds-acompcor (same SHA)
+**Conflicts:** Medium
+**Strategy:** ARCHIVE (duplicate/covered by A1).
+
+**Branch:** `feat/spi-xxx-registration-pam50-wrapper`
+**Commits ahead:** 3
+**Key changes:** Same as above (same SHA)
+**Conflicts:** Medium
+**Strategy:** ARCHIVE (duplicate/covered by A1).
+
+---
+
+### 3. Old experimental branches (ARCHIVE)
+
+**Branch:** `feat/spi-cli-scaffold`
+**Commits ahead:** 2
+**Key changes:** Doctor diagnostics (older version)
+**Strategy:** ARCHIVE (superseded by A1).
+
+**Branch:** `feat/spi-011a-docs-ci-pages`
+**Commits ahead:** 3
+**Key changes:** Docs CI and GitHub Pages setup
+**Conflicts:** Low (isolated to CI/docs)
+**Strategy:** Keep docs changes if valuable; likely ARCHIVE since docs workflow exists.
+
+---
+
+## Step-by-Step Merge Sequence
+
+### Phase 1: Pre-flight checks
+
+```bash
+# 1. Tag current main for rollback
+git checkout main
+git tag pre-phaseA-20251009
+git push origin pre-phaseA-20251009
+
+# 2. Ensure main is synced
+git fetch origin
+git pull origin main --ff-only
+
+# 3. Create backup branches for any uncertain merges
+git branch backup/spi-A1 feat/spi-A1-doctor-json-snapshot
+git branch backup/spi-002 feat/spi-002-snakemake-dag-provenance
+```
+
+### Phase 2: Delete already-merged branches
+
+```bash
+git branch -d feat/doctor-environment-diagnostics
+git branch -d feat/spi-001-scaffold-cli-config
+```
+
+### Phase 3: Main merge - feat/spi-A1-doctor-json-snapshot
+
+This branch contains the most complete work. Strategy: rebase then squash-merge.
+
+```bash
+# Checkout and rebase onto latest main
+git checkout feat/spi-A1-doctor-json-snapshot
+git rebase origin/main
+
+# If conflicts, resolve them, then:
+# git rebase --continue
+
+# Once clean, merge to main
+git checkout main
+git merge --squash feat/spi-A1-doctor-json-snapshot
+git commit -m "feat: consolidate Phase-A baseline (doctor, QC, confounds, registration, masking)  [pre-A1]
+
+Squashed commits:
+- feat(cli): add doctor JSON snapshot with complete diagnostics  [spi-A1]
+- feat(qc): scaffold per-subject QC HTML with overlays, motion, aCompCor, and env  [spi-a7]
+- feat(preproc): add cord and CSF masking with QC overlays
+- feat(registration): implement SCT EPI→PAM50 wrapper with metrics and rule  [spi-xxx]
+- feat(confounds): implement FD/DVARS + confounds TSV/JSON (schema v1) and motion plot  [spi-confounds]
+- feat(core): scaffold CLI with config loader and doctor command  [spi-001]
+
+Co-authored-by: Developer <dev@example.com>
+"
+```
+
+### Phase 4: Check for unique commits in other branches
+
+```bash
+# Check if spi-002 has unique DAG/provenance code not in A1
+git log feat/spi-A1-doctor-json-snapshot..feat/spi-002-snakemake-dag-provenance
+
+# If unique commits exist, cherry-pick:
+git checkout main
+git cherry-pick <commit-sha>
+
+# Otherwise, just delete the branch
+```
+
+### Phase 5: Archive remaining branches
+
+```bash
+# Archive branches that are subsets or superseded
+git branch -D feat/spi-a7-qc-html-scaffold
+git branch -D feat/spi-confounds-fd-dvars-tsv-json-v1
+git branch -D feat/spi-confounds-acompcor-tcompcor-censor
+git branch -D feat/spi-masking-cord-csf-qc
+git branch -D feat/spi-xxx-registration-pam50-wrapper
+git branch -D feat/spi-cli-scaffold
+git branch -D feat/spi-011a-docs-ci-pages
+git branch -D feat/spi-002-snakemake-dag-provenance  # if no unique commits
+
+# Remove from remote (only if they exist and you're sure)
+git push origin --delete feat/spi-a7-qc-html-scaffold 2>/dev/null || true
+git push origin --delete feat/spi-cli-scaffold 2>/dev/null || true
+git push origin --delete feat/spi-confounds-fd-dvars-tsv-json-v1 2>/dev/null || true
+git push origin --delete feat/spi-A1-doctor-json-snapshot 2>/dev/null || true
+git push origin --delete feat/spi-002-snakemake-dag-provenance 2>/dev/null || true
+```
+
+### Phase 6: Push consolidated main
+
+```bash
+git push origin main
+
+# Update remote tags
+git push origin pre-phaseA-20251009
+```
+
+### Phase 7: Verify and clean up
+
+```bash
+# List remaining branches
+git branch -a
+
+# Should see:
+# - main (local and remote)
+# - backup/* branches (local only, for safety)
+
+# Clean up local tracking of deleted remote branches
+git fetch --all --prune
+
+# Verify health
+./ops/status/health_check.sh
+```
+
+---
+
+## Conflict Resolution Strategy
+
+### High-probability conflicts
+
+1. **`spineprep/cli.py`**
+   - Multiple branches modify CLI structure
+   - Resolution: Accept latest (A1) version, verify all commands present
+
+2. **`spineprep/_version.py`**
+   - Version bumps in multiple branches
+   - Resolution: Use chronologically latest version string
+
+3. **`configs/base.yaml`**
+   - Config schema changes across branches
+   - Resolution: Merge all new fields, validate with schema
+
+4. **`CITATION.cff`**
+   - Date/version updates in multiple branches
+   - Resolution: Use latest date, consolidate authors
+
+### Mitigation steps
+
+- For each conflict file:
+  1. Accept theirs (A1 version)
+  2. Manually verify no missing functionality from other branches
+  3. Run tests after each resolution
+  4. If tests fail, inspect what was lost and manually add back
+
+---
+
+## Rollback Procedure
+
+If the merge process goes wrong:
+
+```bash
+# Reset main to pre-merge state
+git checkout main
+git reset --hard pre-phaseA-20251009
+
+# Restore branches from backups if needed
+git checkout -b feat/spi-A1-doctor-json-snapshot backup/spi-A1
+git checkout -b feat/spi-002-snakemake-dag-provenance backup/spi-002
+
+# Force push main (ONLY if you're certain)
+# git push origin main --force  # DANGER: only if solo developer
+```
+
+---
+
+## Post-Merge Checklist
+
+- [ ] `git branch -a` shows only `main` and `origin/main` (plus backups)
+- [ ] `ruff check .` passes
+- [ ] `snakefmt --check .` passes (or 1 acceptable file)
+- [ ] `pytest -q` passes (or known failures documented)
+- [ ] `mkdocs build --strict` passes
+- [ ] All desired features from branches present in main
+- [ ] Tag `pre-phaseA-20251009` exists and is pushed
+- [ ] CI passing on main
+- [ ] README updated to reflect consolidated state
+
+---
+
+## Estimated Impact
+
+- **Files changed:** ~50-70 files (including new modules, tests, configs)
+- **LOC touched:** ~3000-4000 lines (estimate)
+- **Test coverage impact:** Some tests may need fixes (confounds import errors)
+- **Breaking changes:** None (internal refactor only)
+- **Migration needed:** No (pre-release consolidation)
+
+---
+
+## Notes
+
+- The `feat/spi-A1-doctor-json-snapshot` branch is the most comprehensive and should be the primary merge source.
+- Several branches share identical commits (e.g., 81712fac appears in 3 branches), indicating parallel development on same features.
+- The `.venv` directory is being tracked (should be in .gitignore) — will be addressed in tree hygiene ticket.
+- `site/` directory (built docs) is tracked — should be in .gitignore or explicitly kept out of repo.
+- Many `__pycache__` files are staged — need .gitignore update.
diff --git a/ops/status/mkdocs.txt b/ops/status/mkdocs.txt
new file mode 100644
index 00000000..12636301
--- /dev/null
+++ b/ops/status/mkdocs.txt
@@ -0,0 +1,6 @@
+INFO    -  Cleaning site directory
+INFO    -  Building documentation to directory: /mnt/ssd1/SpinePrep/site
+INFO    -  The following pages exist in the docs directory, but are not included in the "nav" configuration:
+  - workflow.md
+INFO    -  Doc file 'index.md' contains an absolute link '/stable/', it was left as is.
+INFO    -  Documentation built in 0.44 seconds
diff --git a/ops/status/pytest.txt b/ops/status/pytest.txt
new file mode 100644
index 00000000..e4689000
--- /dev/null
+++ b/ops/status/pytest.txt
@@ -0,0 +1,5 @@
+
+=========================== short test summary info ============================
+ERROR tests/test_confounds.py
+!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
+1 error in 0.40s
diff --git a/ops/status/repo_risks.md b/ops/status/repo_risks.md
new file mode 100644
index 00000000..312f1d04
--- /dev/null
+++ b/ops/status/repo_risks.md
@@ -0,0 +1,315 @@
+# Repository Risks & Mitigations — Pre-Phase-A
+
+**Generated:** 2025-10-09
+**Assessment scope:** Branch consolidation, tree hygiene, test health
+**Risk horizon:** Ticket 1 (merge execution) through Phase A gate
+
+---
+
+## 🔴 Critical Risks
+
+### R1: Test Suite Broken (Import Errors)
+
+**Impact:** Cannot validate merges, regression risk
+**Likelihood:** CERTAIN (already failing)
+**Evidence:**
+```
+ERROR tests/test_confounds.py
+ImportError: cannot import name 'compute_censor' from 'spineprep.confounds'
+```
+
+**Root cause:**
+- `tests/test_confounds.py` expects `compute_censor` function
+- `spineprep/confounds/__init__.py` may not export it, or it's in a submodule
+- Different branches have different confounds structures
+
+**Mitigation:**
+1. After merge, immediately inspect `spineprep/confounds/__init__.py` exports
+2. Check if `compute_censor` moved to `spineprep/confounds/censor.py`
+3. Update test imports to match new module structure
+4. Add to merge checklist: "Fix confounds test imports"
+5. Run `pytest tests/test_confounds.py -v` after merge to verify
+
+**Fallback:**
+- Skip broken tests temporarily, create hotfix ticket
+- Document known test failures in merge commit
+
+---
+
+### R2: Merge Conflicts in Core CLI (High Complexity)
+
+**Impact:** CLI may be broken or have duplicate logic
+**Likelihood:** HIGH (4+ branches modify `spineprep/cli.py`)
+**Conflict zones:**
+- `spineprep/cli.py` (M in 5+ branches)
+- `spineprep/_version.py` (M in 5+ branches)
+- `configs/base.yaml` (M in 5+ branches)
+
+**Evidence:**
+- feat/spi-A1-doctor-json-snapshot: adds doctor JSON command
+- feat/spi-002-snakemake-dag-provenance: may add DAG commands
+- feat/spi-cli-scaffold: older doctor implementation
+
+**Mitigation:**
+1. Choose **feat/spi-A1-doctor-json-snapshot** as canonical (most recent, most complete)
+2. Before merge, manually diff other branches' cli.py to identify unique commands
+3. Extract unique functionality from other branches before archiving
+4. After merge, run `spineprep --help` to ensure all expected commands present
+5. Test each CLI command manually: `spineprep doctor`, `spineprep version`, etc.
+
+**Fallback:**
+- Keep backup branches (`backup/spi-*`) for 30 days to extract lost functionality
+
+---
+
+### R3: Large Tracked Files (.venv, __pycache__, site/)
+
+**Impact:** Bloated repo size, slow clones, accidental secrets exposure
+**Likelihood:** CERTAIN (already tracked)
+**Evidence:**
+- `.venv/lib/` contains 25MB+ of Python packages (numpy, scipy, pandas)
+- `site/` contains generated MkDocs output (~5MB+)
+- `__pycache__/` directories throughout
+
+**Current state:**
+- Largest tracked file: 25MB `libscipy_openblas64_`
+- Total .venv size: ~100MB+
+- These should NEVER be in git
+
+**Mitigation (urgent, do in Ticket 1):**
+1. **Before any merge**, add to `.gitignore`:
+   ```
+   .venv/
+   __pycache__/
+   *.pyc
+   site/
+   .snakemake/
+   .coverage
+   *.egg-info/
+   ```
+2. Remove from git history (careful!):
+   ```bash
+   git rm -r --cached .venv/
+   git rm -r --cached site/
+   git rm -r --cached **/__pycache__/
+   git rm -r --cached **/*.egg-info/
+   git commit -m "chore: remove build artifacts and venv from tracking"
+   ```
+3. Verify removal: `git ls-tree -r -l HEAD | grep venv` should be empty
+4. Force push to rewrite history (ONLY if no other collaborators):
+   ```bash
+   # DO NOT RUN without CEO approval
+   # git push origin main --force
+   ```
+
+**Fallback:**
+- If force-push not allowed, accept bloated history and add `.gitignore` going forward
+- Use `git-filter-repo` for deep clean (destructive, needs coordination)
+
+---
+
+## 🟡 Medium Risks
+
+### R4: Confounds Module Structure Inconsistency
+
+**Impact:** API instability, test fragility
+**Likelihood:** MEDIUM
+**Evidence:**
+- Branch `spi-002` has `confounds/censor.py` and `confounds/compcor.py`
+- Branch `spi-A1` has `confounds/io.py`, `confounds/motion.py`, `confounds/plot.py`
+- Different branches may have different `__init__.py` exports
+
+**Mitigation:**
+1. After merge, audit `spineprep/confounds/__init__.py`
+2. Ensure all submodules are properly exported
+3. Run full test suite: `pytest tests/confounds/ -v`
+4. Document public API in `spineprep/confounds/README.md`
+
+---
+
+### R5: Version String Conflicts
+
+**Impact:** Incorrect version displayed, changelog confusion
+**Likelihood:** MEDIUM (5 branches modify `_version.py`)
+**Evidence:**
+- Multiple branches bump version independently
+- `CITATION.cff` also has version strings and dates
+
+**Mitigation:**
+1. After merge, manually set version to `0.2.0-dev` or similar
+2. Sync version across: `_version.py`, `CITATION.cff`, `pyproject.toml`
+3. Update `CHANGELOG.md` with consolidated entry for all merged work
+4. Verify: `spineprep version` shows expected string
+
+---
+
+### R6: Duplicate Branches (Same SHA)
+
+**Impact:** Confusion, wasted merge effort
+**Likelihood:** CERTAIN (already exists)
+**Evidence:**
+- `feat/spi-confounds-acompcor-tcompcor-censor` @ 81712fac
+- `feat/spi-masking-cord-csf-qc` @ 81712fac
+- `feat/spi-xxx-registration-pam50-wrapper` @ 81712fac
+
+**Mitigation:**
+1. Identify duplicates: already done (see merge_plan.md)
+2. Keep one, archive others immediately
+3. No merge needed, just delete redundant branches
+
+---
+
+### R7: Untracked Generated Files (doctor.json, out/, test_work*)
+
+**Impact:** Repo clutter, potential sensitive data leak
+**Likelihood:** LOW (only local, not pushed)
+**Evidence:**
+```
+?? doctor.json
+?? out/
+?? test_work/
+?? test_work2/ ... test_work8/
+```
+
+**Mitigation:**
+1. Add to `.gitignore`:
+   ```
+   doctor.json
+   out/
+   test_work*/
+   *.ok
+   ```
+2. Delete locally:
+   ```bash
+   rm -rf test_work* out/ doctor.json *.ok
+   ```
+3. Verify not staged: `git status --porcelain` should show no untracked files
+
+---
+
+## 🟢 Low Risks
+
+### R8: Snakefmt Formatting (1 file)
+
+**Impact:** CI may fail on formatting checks
+**Likelihood:** LOW
+**Evidence:** `snakefmt --check .` reports 1 file needs formatting
+
+**Mitigation:**
+1. Run `snakefmt .` to auto-fix
+2. Commit: `style: apply snakefmt`
+3. Add pre-commit hook (optional, in Ticket 2)
+
+---
+
+### R9: Codespell False Positives (site/ minified JS)
+
+**Impact:** Noisy CI logs, mask real typos
+**Likelihood:** LOW (cosmetic)
+**Evidence:** 50+ false positives in `site/assets/javascripts/bundle.*.min.js`
+
+**Mitigation:**
+1. Add to `.codespellrc` or `pyproject.toml`:
+   ```toml
+   [tool.codespell]
+   skip = 'site/,*.min.js,*.map'
+   ignore-words-list = 'te,hist,trys,ue,ot'
+   ```
+2. Or, remove `site/` from repo (already recommended in R3)
+
+---
+
+### R10: Missing DAG/Provenance Code in A1
+
+**Impact:** Loss of Snakemake DAG export functionality
+**Likelihood:** MEDIUM
+**Evidence:** `feat/spi-002-snakemake-dag-provenance` has unique DAG work
+
+**Mitigation:**
+1. Before archiving spi-002, check if its DAG code is in A1
+2. If not, cherry-pick commit `58dff246` to main
+3. Verify DAG works: `snakemake -n -p --dag | dot -Tsvg > test.svg`
+
+---
+
+## Risk Scoring Matrix
+
+| Risk | Impact | Likelihood | Severity | Ticket |
+|------|--------|-----------|----------|--------|
+| R1 - Test suite broken | Critical | Certain | 🔴 10/10 | Ticket 1 |
+| R2 - CLI merge conflicts | High | High | 🔴 8/10 | Ticket 1 |
+| R3 - Tracked .venv/site | Medium | Certain | 🔴 7/10 | Ticket 1 |
+| R4 - Confounds API drift | Medium | Medium | 🟡 5/10 | Ticket 1 |
+| R5 - Version conflicts | Medium | Medium | 🟡 4/10 | Ticket 1 |
+| R6 - Duplicate branches | Low | Certain | 🟢 2/10 | Ticket 1 |
+| R7 - Untracked clutter | Low | Low | 🟢 2/10 | Ticket 1 |
+| R8 - Snakefmt | Low | Low | 🟢 1/10 | Ticket 2 |
+| R9 - Codespell noise | Low | Low | 🟢 1/10 | Ticket 2 |
+| R10 - Missing DAG code | Medium | Medium | 🟡 5/10 | Ticket 1 |
+
+---
+
+## Recommended Action Sequence (for Ticket 1)
+
+1. **Immediate (before any merge):**
+   - Update `.gitignore` (R3, R7)
+   - Remove `.venv`, `site/`, `__pycache__` from tracking (R3)
+   - Tag `pre-phaseA-20251009` for rollback
+
+2. **During merge:**
+   - Use feat/spi-A1-doctor-json-snapshot as base (R2)
+   - Cherry-pick DAG commits from spi-002 if missing (R10)
+   - Resolve conflicts by accepting A1 version, manually verify (R2)
+
+3. **After merge:**
+   - Fix confounds test imports (R1)
+   - Sync version strings (R5)
+   - Run full test suite and fix failures (R1, R4)
+   - Run `snakefmt .` (R8)
+
+4. **Verification:**
+   - `pytest -q` passes (or document known failures)
+   - `ruff check .` clean
+   - `snakefmt --check .` clean
+   - `mkdocs build --strict` clean
+   - `spineprep --help` shows all commands
+   - `spineprep doctor` runs without error
+
+---
+
+## Rollback Triggers
+
+Abort merge and rollback if:
+- ✅ More than 5 test files have import errors after merge
+- ✅ CLI commands entirely missing (not just broken)
+- ✅ Merge conflicts take >2 hours to resolve
+- ✅ Main branch becomes unbuildable (import errors in core modules)
+
+Use: `git reset --hard pre-phaseA-20251009`
+
+---
+
+## Long-term Debt (defer to Ticket 2 or later)
+
+- Add pre-commit hooks for ruff, snakefmt, codespell
+- Set up branch protection on `main` (require PR, CI pass)
+- Add `mypy` type checking to CI
+- Clean git history with `git-filter-repo` (if CEO approves force-push)
+- Document branching strategy in CONTRIBUTING.md
+
+---
+
+## Confidence Assessment
+
+**Overall confidence in merge success:** 🟡 MEDIUM (60%)
+
+**Blockers:**
+- Test suite must be fixed post-merge (not a blocker to merge, but to validation)
+- Confounds module structure must be reconciled
+
+**Enablers:**
+- Good commit history and messages
+- Clear feature separation across branches
+- Rollback tag in place
+
+**Recommendation:** Proceed with Ticket 1, but allocate 4-6 hours for merge + fixes.
diff --git a/ops/status/repo_status.json b/ops/status/repo_status.json
new file mode 100644
index 00000000..659c89f1
--- /dev/null
+++ b/ops/status/repo_status.json
@@ -0,0 +1,45 @@
+{
+  "timestamp": "2025-10-09T18:30:23+02:00",
+  "head": "b2ed3dd9",
+  "head_branch": "feat/spi-A1-doctor-json-snapshot",
+  "main_exists": true,
+  "origin_main_exists": true,
+  "branches": {
+    "local": [
+      "feat/doctor-environment-diagnostics",
+      "feat/spi-001-scaffold-cli-config",
+      "feat/spi-002-snakemake-dag-provenance",
+      "feat/spi-011a-docs-ci-pages",
+      "feat/spi-A1-doctor-json-snapshot",
+      "feat/spi-a7-qc-html-scaffold",
+      "feat/spi-cli-scaffold",
+      "feat/spi-confounds-acompcor-tcompcor-censor",
+      "feat/spi-confounds-fd-dvars-tsv-json-v1",
+      "feat/spi-masking-cord-csf-qc",
+      "feat/spi-xxx-registration-pam50-wrapper",
+      "main"
+    ],
+    "remote": [
+      "origin",
+      "origin/cursor/harden-docs-ci-and-enable-github-pages-deployment-43de",
+      "origin/feat/spi-001-scaffold-cli-config",
+      "origin/feat/spi-002-snakemake-dag-provenance",
+      "origin/feat/spi-A1-doctor-json-snapshot",
+      "origin/feat/spi-a7-qc-html-scaffold",
+      "origin/feat/spi-cli-scaffold",
+      "origin/feat/spi-confounds-fd-dvars-tsv-json-v1",
+      "origin/main"
+    ]
+  },
+  "tags": [
+    "v0.1.0",
+    "v0.1.0-docs"
+  ],
+  "health": {
+    "pytest_passed": false,
+    "mkdocs_strict_passed": true,
+    "ruff_clean": true,
+    "snakefmt_clean": false
+  },
+  "divergence_from_main": "0\t6"
+}
diff --git a/ops/status/ruff.txt b/ops/status/ruff.txt
new file mode 100644
index 00000000..e69de29b
diff --git a/ops/status/snakefmt.txt b/ops/status/snakefmt.txt
new file mode 100644
index 00000000..817765e2
--- /dev/null
+++ b/ops/status/snakefmt.txt
@@ -0,0 +1,2 @@
+[INFO] 1 file(s) would be changed 😬
+[INFO] 4 file(s) would be left unchanged 🎉
diff --git a/ops/status/status_porcelain.txt b/ops/status/status_porcelain.txt
new file mode 100644
index 00000000..22fb1f05
--- /dev/null
+++ b/ops/status/status_porcelain.txt
@@ -0,0 +1,80 @@
+ M spineprep.egg-info/PKG-INFO
+ M spineprep.egg-info/SOURCES.txt
+ M spineprep.egg-info/requires.txt
+ M spineprep.egg-info/top_level.txt
+ M spineprep/__pycache__/__init__.cpython-312.pyc
+ M spineprep/__pycache__/cli.cpython-312.pyc
+ M spineprep/__pycache__/config.cpython-312.pyc
+ M spineprep/confounds/__pycache__/__init__.cpython-312.pyc
+ M spineprep/confounds/__pycache__/io.cpython-312.pyc
+ M spineprep/confounds/__pycache__/motion.cpython-312.pyc
+ M spineprep/confounds/__pycache__/plot.cpython-312.pyc
+ M spineprep/preproc/__pycache__/__init__.cpython-312.pyc
+ M spineprep/preproc/__pycache__/masking.cpython-312.pyc
+ M spineprep/qc/report.py
+ M tests/confounds/__pycache__/__init__.cpython-312.pyc
+ M tests/confounds/__pycache__/test_motion.cpython-312-pytest-8.4.2.pyc
+ M tests/integration/__pycache__/test_confounds_acompcor_rule.cpython-312-pytest-8.4.2.pyc
+ M tests/integration/__pycache__/test_motion_engines.cpython-312-pytest-8.4.2.pyc
+ M tests/test_doctor_json.py
+ M tests/test_qc_report.py
+ M tests/unit/__pycache__/test_acompcor_math.cpython-312-pytest-8.4.2.pyc
+ M tests/unit/__pycache__/test_censor_contiguity.cpython-312-pytest-8.4.2.pyc
+ M tests/unit/__pycache__/test_confounds_columns_acompcor.cpython-312-pytest-8.4.2.pyc
+ M tests/unit/__pycache__/test_crop_detector.cpython-312-pytest-8.4.2.pyc
+ M tests/unit/__pycache__/test_docs_generators.cpython-312-pytest-8.4.2.pyc
+ M tests/unit/__pycache__/test_mask_paths.cpython-312-pytest-8.4.2.pyc
+ M workflow/lib/__pycache__/confounds.cpython-312.pyc
+ M workflow/lib/__pycache__/deriv.cpython-312.pyc
+ M workflow/lib/__pycache__/qc.cpython-312.pyc
+ M workflow/lib/__pycache__/registration.cpython-312.pyc
+?? .coverage
+?? .snakemake/log/2025-10-08T192411.244361.snakemake.log
+?? .snakemake/log/2025-10-09T135956.825827.snakemake.log
+?? .snakemake/log/2025-10-09T140016.592494.snakemake.log
+?? .snakemake/log/2025-10-09T140800.179998.snakemake.log
+?? .snakemake/log/2025-10-09T140801.439704.snakemake.log
+?? .snakemake/log/2025-10-09T140803.499669.snakemake.log
+?? .snakemake/log/2025-10-09T140804.645053.snakemake.log
+?? .snakemake/log/2025-10-09T140828.068964.snakemake.log
+?? .snakemake/log/2025-10-09T140838.914664.snakemake.log
+?? .snakemake/log/2025-10-09T140840.957756.snakemake.log
+?? .snakemake/log/2025-10-09T140906.024071.snakemake.log
+?? .snakemake/log/2025-10-09T175435.464133.snakemake.log
+?? .snakemake/metadata/L3RtcC90ZXN0X291dDMvZGVyaXZhdGl2ZXMvc3BpbmVwcmVwLy5zcGluZXByZXBfc2NhZmZvbGQ=
+?? .snakemake/metadata/L3RtcC90ZXN0X291dDMvcHJvdmVuYW5jZS9kYWcuc3Zn
+?? .snakemake/metadata/L3RtcC90bXBmYWllNWQ0cy9vdXQvZGVyaXZhdGl2ZXMvc3BpbmVwcmVwLy5zcGluZXByZXBfc2NhZmZvbGQ=
+?? .snakemake/metadata/L3RtcC90bXBmYWllNWQ0cy9vdXQvcHJvdmVuYW5jZS9kYWcuc3Zn
+?? docs/workflow.md
+?? doctor.json
+?? ops/
+?? out/
+?? spineprep/__pycache__/_sct.cpython-312.pyc
+?? spineprep/__pycache__/_version.cpython-312.pyc
+?? spineprep/__pycache__/confounds.cpython-312.pyc
+?? spineprep/__pycache__/derivatives.cpython-312.pyc
+?? spineprep/__pycache__/doctor.cpython-312.pyc
+?? spineprep/__pycache__/ingest.cpython-312.pyc
+?? spineprep/__pycache__/motion.cpython-312.pyc
+?? spineprep/__pycache__/provenance.cpython-312.pyc
+?? spineprep/__pycache__/qc.cpython-312.pyc
+?? spineprep/confounds/__pycache__/censor.cpython-312.pyc
+?? spineprep/confounds/__pycache__/compcor.cpython-312.pyc
+?? spineprep/confounds/censor.py
+?? spineprep/confounds/compcor.py
+?? spineprep/registration/__pycache__/
+?? test_acceptance_masking.py
+?? test_doctor.json
+?? tests/__pycache__/
+?? tests/confounds/__pycache__/test_censor.cpython-312-pytest-8.4.2.pyc
+?? tests/confounds/__pycache__/test_compcor.cpython-312-pytest-8.4.2.pyc
+?? tests/confounds/test_censor.py
+?? tests/confounds/test_compcor.py
+?? tests/test_dag_render.py
+?? tests/unit/__pycache__/test_doctor.cpython-312-pytest-8.4.2.pyc
+?? tests/unit/__pycache__/test_header_checks.cpython-312-pytest-8.4.2.pyc
+?? tests/unit/__pycache__/test_registration_metrics.cpython-312-pytest-8.4.2.pyc
+?? tests/unit/__pycache__/test_sct_wrapper.cpython-312-pytest-8.4.2.pyc
+?? tests/unit/__pycache__/test_targets.cpython-312-pytest-8.4.2.pyc
+?? workflow/Snakefile.backup
+?? workflow/lib/__pycache__/targets.cpython-312.pyc
diff --git a/pytest.ini b/pytest.ini
index 98a2cec0..628f2609 100644
--- a/pytest.ini
+++ b/pytest.ini
@@ -21,9 +21,6 @@ filterwarnings =
     ignore::DeprecationWarning:jsonschema
     ignore::RuntimeWarning:skimage

-# Random seed for reproducibility
-randomly-seed = 42
-
 # Coverage
 addopts =
     --strict-markers
@@ -37,4 +34,3 @@ markers =
     integration: marks integration tests (may require external tools)
     e2e: marks end-to-end tests (full pipeline)
     unit: marks unit tests (fast, isolated)
-
diff --git a/schemas/config.schema.json b/schemas/config.schema.json
index 71f96668..02d3f9d8 100644
--- a/schemas/config.schema.json
+++ b/schemas/config.schema.json
@@ -1,41 +1,169 @@
 {
-  "$schema":"https://json-schema.org/draft/2020-12/schema",
-  "type":"object",
-  "required":["bids_root","output_dir","pipeline","confounds","registration","qc"],
-  "properties":{
-    "bids_root":{"type":["string","null"]},
-    "output_dir":{"type":"string"},
-    "pipeline":{
-      "type":"object",
-      "properties":{
-        "motion":{
-          "type":"object",
-          "properties":{
-            "fd_threshold":{"type":"number"},
-            "dvars_threshold":{"type":"number"}
-          },
-          "required":["fd_threshold","dvars_threshold"]
+  "$schema": "https://json-schema.org/draft/2020-12/schema",
+  "type": "object",
+  "required": ["bids_root", "output_dir", "motion", "confounds", "registration", "qc"],
+  "properties": {
+    "bids_root": {
+      "type": ["string", "null"],
+      "description": "Path to BIDS dataset root directory"
+    },
+    "output_dir": {
+      "type": "string",
+      "description": "Path to output directory for derivatives"
+    },
+    "motion": {
+      "type": "object",
+      "required": ["fd", "dvars", "highpass"],
+      "properties": {
+        "fd": {
+          "type": "object",
+          "required": ["threshold"],
+          "properties": {
+            "threshold": {
+              "type": "number",
+              "minimum": 0,
+              "description": "Framewise displacement threshold in mm"
+            }
+          }
+        },
+        "dvars": {
+          "type": "object",
+          "required": ["threshold"],
+          "properties": {
+            "threshold": {
+              "type": "number",
+              "minimum": 0,
+              "description": "DVARS threshold for motion detection"
+            }
+          }
+        },
+        "highpass": {
+          "type": "object",
+          "required": ["hz"],
+          "properties": {
+            "hz": {
+              "type": "number",
+              "minimum": 0,
+              "description": "Highpass filter cutoff frequency in Hz"
+            }
+          }
         }
       }
     },
-    "confounds":{
-      "type":"object",
-      "properties":{
-        "acompcor":{
-          "type":"object",
-          "properties":{"n_components":{"type":"integer","minimum":1}},
-          "required":["n_components"]
+    "confounds": {
+      "type": "object",
+      "required": ["acompcor", "tcompcor", "censor"],
+      "properties": {
+        "acompcor": {
+          "type": "object",
+          "required": ["n_components"],
+          "properties": {
+            "n_components": {
+              "type": "integer",
+              "minimum": 0,
+              "description": "Number of aCompCor components to extract"
+            }
+          }
+        },
+        "tcompcor": {
+          "type": "object",
+          "required": ["n_components"],
+          "properties": {
+            "n_components": {
+              "type": "integer",
+              "minimum": 0,
+              "description": "Number of tCompCor components to extract"
+            }
+          }
+        },
+        "censor": {
+          "type": "object",
+          "required": ["method"],
+          "properties": {
+            "method": {
+              "type": "string",
+              "enum": ["fd", "dvars", "fd+dvars"],
+              "description": "Censoring method for high-motion frames"
+            }
+          }
         }
       }
     },
-    "registration":{
-      "type":"object",
-      "properties":{
-        "target":{"type":"string","enum":["PAM50"]},
-        "sct":{"type":"object","properties":{"enable":{"type":"boolean"}},"required":["enable"]}
-      },
-      "required":["target","sct"]
+    "registration": {
+      "type": "object",
+      "required": ["pam50", "resample", "deformable"],
+      "properties": {
+        "pam50": {
+          "type": "object",
+          "required": ["spacing"],
+          "properties": {
+            "spacing": {
+              "type": "string",
+              "description": "PAM50 template spacing (e.g., '0.5mm', '1mm')"
+            }
+          }
+        },
+        "resample": {
+          "type": "object",
+          "required": ["to"],
+          "properties": {
+            "to": {
+              "type": "string",
+              "enum": ["native", "pam50"],
+              "description": "Target space for resampling"
+            }
+          }
+        },
+        "deformable": {
+          "type": "object",
+          "required": ["enabled"],
+          "properties": {
+            "enabled": {
+              "type": "boolean",
+              "description": "Enable deformable (non-linear) registration"
+            }
+          }
+        }
+      }
     },
-    "qc":{"type":"object","properties":{"enable":{"type":"boolean"}},"required":["enable"]}
+    "qc": {
+      "type": "object",
+      "required": ["ssim", "psnr", "report"],
+      "properties": {
+        "ssim": {
+          "type": "object",
+          "required": ["min"],
+          "properties": {
+            "min": {
+              "type": "number",
+              "minimum": 0,
+              "maximum": 1,
+              "description": "Minimum acceptable SSIM for QC"
+            }
+          }
+        },
+        "psnr": {
+          "type": "object",
+          "required": ["min"],
+          "properties": {
+            "min": {
+              "type": "number",
+              "minimum": 0,
+              "description": "Minimum acceptable PSNR in dB for QC"
+            }
+          }
+        },
+        "report": {
+          "type": "object",
+          "required": ["embed_assets"],
+          "properties": {
+            "embed_assets": {
+              "type": "boolean",
+              "description": "Embed assets directly in QC HTML reports"
+            }
+          }
+        }
+      }
+    }
   }
 }
diff --git a/spineprep/_sct.py b/spineprep/_sct.py
new file mode 100644
index 00000000..bf8aa3ce
--- /dev/null
+++ b/spineprep/_sct.py
@@ -0,0 +1,165 @@
+"""Generic SCT (Spinal Cord Toolbox) command runner.
+
+Thin wrapper for executing SCT commands with provenance tracking.
+"""
+
+from __future__ import annotations
+
+import subprocess
+from pathlib import Path
+from typing import Any
+
+
+def get_sct_version() -> str:
+    """
+    Get installed SCT version.
+
+    Returns:
+        SCT version string (e.g., "6.0.0") or "unknown"
+    """
+    try:
+        result = subprocess.run(
+            ["sct_version"],
+            capture_output=True,
+            text=True,
+            timeout=10,
+            check=False,
+        )
+        if result.returncode == 0:
+            # Parse first line of version output
+            return result.stdout.strip().split("\n")[0]
+    except Exception:
+        pass
+    return "unknown"
+
+
+def run_sct_deepseg(
+    t2w_path: str,
+    out_mask: str,
+    contrast: str = "t2",
+) -> dict[str, Any]:
+    """
+    Run sct_deepseg_sc for spinal cord segmentation.
+
+    Args:
+        t2w_path: Path to input T2w image
+        out_mask: Path to output mask
+        contrast: Contrast type (default: "t2")
+
+    Returns:
+        Dictionary with:
+        - success: bool
+        - return_code: int
+        - stdout: str
+        - stderr: str
+        - sct_version: str
+        - cmd: str (full command executed)
+    """
+    # Get SCT version for provenance
+    sct_version = get_sct_version()
+
+    # Build command
+    cmd = [
+        "sct_deepseg_sc",
+        "-i",
+        t2w_path,
+        "-c",
+        contrast,
+        "-o",
+        out_mask,
+    ]
+
+    # Ensure output directory exists
+    Path(out_mask).parent.mkdir(parents=True, exist_ok=True)
+
+    # Run segmentation
+    try:
+        result = subprocess.run(
+            cmd,
+            capture_output=True,
+            text=True,
+            check=False,
+            timeout=600,  # 10 minute timeout
+        )
+
+        return {
+            "success": result.returncode == 0,
+            "return_code": result.returncode,
+            "stdout": result.stdout,
+            "stderr": result.stderr,
+            "sct_version": sct_version,
+            "cmd": " ".join(cmd),
+        }
+
+    except subprocess.TimeoutExpired:
+        return {
+            "success": False,
+            "return_code": -1,
+            "stdout": "",
+            "stderr": "sct_deepseg_sc timed out after 10 minutes",
+            "sct_version": sct_version,
+            "cmd": " ".join(cmd),
+        }
+    except Exception as e:
+        return {
+            "success": False,
+            "return_code": -1,
+            "stdout": "",
+            "stderr": str(e),
+            "sct_version": sct_version,
+            "cmd": " ".join(cmd),
+        }
+
+
+def run_sct_command(
+    cmd_args: list[str],
+    timeout: int = 600,
+) -> dict[str, Any]:
+    """
+    Generic SCT command runner with provenance.
+
+    Args:
+        cmd_args: Command arguments (e.g., ["sct_propseg", "-i", "..."])
+        timeout: Timeout in seconds (default: 600)
+
+    Returns:
+        Dictionary with success, return_code, stdout, stderr, sct_version, cmd
+    """
+    sct_version = get_sct_version()
+
+    try:
+        result = subprocess.run(
+            cmd_args,
+            capture_output=True,
+            text=True,
+            check=False,
+            timeout=timeout,
+        )
+
+        return {
+            "success": result.returncode == 0,
+            "return_code": result.returncode,
+            "stdout": result.stdout,
+            "stderr": result.stderr,
+            "sct_version": sct_version,
+            "cmd": " ".join(cmd_args),
+        }
+
+    except subprocess.TimeoutExpired:
+        return {
+            "success": False,
+            "return_code": -1,
+            "stdout": "",
+            "stderr": f"Command timed out after {timeout} seconds",
+            "sct_version": sct_version,
+            "cmd": " ".join(cmd_args),
+        }
+    except Exception as e:
+        return {
+            "success": False,
+            "return_code": -1,
+            "stdout": "",
+            "stderr": str(e),
+            "sct_version": sct_version,
+            "cmd": " ".join(cmd_args),
+        }
diff --git a/spineprep/_version.py b/spineprep/_version.py
index 07c5de9e..25312d0a 100644
--- a/spineprep/_version.py
+++ b/spineprep/_version.py
@@ -1,2 +1,2 @@
 __all__ = ["__version__"]
-__version__ = "0.1.0"
+__version__ = "0.2.0"
diff --git a/spineprep/cli.py b/spineprep/cli.py
index 41372da6..919a8fb0 100644
--- a/spineprep/cli.py
+++ b/spineprep/cli.py
@@ -6,10 +6,7 @@ from pathlib import Path
 from jsonschema import ValidationError

 from .config import print_config, resolve_config
-from .confounds import process as confounds_process
 from .doctor import cmd_doctor
-from .ingest import ingest
-from .motion import process_from_manifest


 def build_parser() -> argparse.ArgumentParser:
@@ -19,7 +16,12 @@ def build_parser() -> argparse.ArgumentParser:
     d = sub.add_parser("doctor", help="Check environment and dependencies")
     d.add_argument("--out", type=Path, default=Path("./out"))
     d.add_argument("--pam50", type=str, default=None)
-    d.add_argument("--json", type=Path, default=None)
+    d.add_argument(
+        "--json",
+        type=str,
+        default=None,
+        help="Write JSON report to file or '-' for stdout",
+    )
     d.add_argument("--strict", action="store_true")

     r = sub.add_parser("run", help="Resolve config and run pipeline (stub)")
@@ -41,7 +43,11 @@ def main(argv=None) -> int:
     p = build_parser()
     a = p.parse_args(argv)
     if a.cmd == "doctor":
-        return cmd_doctor(a.out, a.pam50, a.json, a.strict)
+        # Handle --json argument: convert to Path unless it's "-"
+        json_arg = None
+        if a.json:
+            json_arg = Path(a.json) if a.json != "-" else a.json
+        return cmd_doctor(a.out, a.pam50, json_arg, a.strict)
     if a.cmd == "run":
         try:
             cfg = resolve_config(a.bids_root, a.output_dir, a.config)
@@ -117,28 +123,9 @@ def main(argv=None) -> int:
             print("[dry-run] Pipeline check complete")
             return 0

-        # Run ingest (non-dry-run)
-        out_dir = Path(cfg["output_dir"])
-        stats = ingest(Path(cfg["bids_root"]), out_dir, hash_large=False)
-        print(
-            f"[ingest] {stats['total']} files indexed ({stats['func']} func, {stats['anat']} anat, {stats['other']} other) → {stats['manifest']}"
-        )
-
-        # Run motion processing
-        manifest_path = out_dir / "manifest.csv"
-        motion_stats = process_from_manifest(manifest_path, out_dir)
-        print(
-            f"[motion] {motion_stats['processed']} runs processed → confounds TSVs and plots"
-        )
-
-        # Run confounds processing
-        confounds_stats = confounds_process(manifest_path, out_dir, cfg)
-        print(
-            f"[confounds] {confounds_stats['runs']} runs updated, {confounds_stats['skipped']} skipped, "
-            f"{confounds_stats['censored']} frames censored total"
-        )
-
+        # Pipeline execution not yet implemented
         print("[run] config resolved ✓")
+        print("[run] Pipeline execution deferred to Snakemake DAG")
         return 0
     if a.cmd == "version":
         from importlib.metadata import PackageNotFoundError, version
diff --git a/spineprep/confounds/__init__.py b/spineprep/confounds/__init__.py
new file mode 100644
index 00000000..44dc2308
--- /dev/null
+++ b/spineprep/confounds/__init__.py
@@ -0,0 +1,12 @@
+"""Confounds module: motion metrics, aCompCor, and censoring."""
+
+from spineprep.confounds.io import write_confounds_tsv_json
+from spineprep.confounds.motion import compute_dvars, compute_fd_power
+from spineprep.confounds.plot import plot_motion_png
+
+__all__ = [
+    "compute_fd_power",
+    "compute_dvars",
+    "write_confounds_tsv_json",
+    "plot_motion_png",
+]
diff --git a/spineprep/confounds/__pycache__/__init__.cpython-312.pyc b/spineprep/confounds/__pycache__/__init__.cpython-312.pyc
new file mode 100644
index 00000000..7fe56c5f
Binary files /dev/null and b/spineprep/confounds/__pycache__/__init__.cpython-312.pyc differ
diff --git a/spineprep/confounds/__pycache__/io.cpython-312.pyc b/spineprep/confounds/__pycache__/io.cpython-312.pyc
new file mode 100644
index 00000000..63fc00a6
Binary files /dev/null and b/spineprep/confounds/__pycache__/io.cpython-312.pyc differ
diff --git a/spineprep/confounds/__pycache__/motion.cpython-312.pyc b/spineprep/confounds/__pycache__/motion.cpython-312.pyc
new file mode 100644
index 00000000..d2b79a55
Binary files /dev/null and b/spineprep/confounds/__pycache__/motion.cpython-312.pyc differ
diff --git a/spineprep/confounds/__pycache__/plot.cpython-312.pyc b/spineprep/confounds/__pycache__/plot.cpython-312.pyc
new file mode 100644
index 00000000..8da06875
Binary files /dev/null and b/spineprep/confounds/__pycache__/plot.cpython-312.pyc differ
diff --git a/spineprep/confounds/io.py b/spineprep/confounds/io.py
new file mode 100644
index 00000000..1a599bd7
--- /dev/null
+++ b/spineprep/confounds/io.py
@@ -0,0 +1,125 @@
+"""Confounds I/O: write TSV and JSON with schema v1 compliance."""
+
+from __future__ import annotations
+
+import csv
+import json
+from pathlib import Path
+
+import numpy as np
+
+from spineprep import __version__
+
+
+def write_confounds_tsv_json(
+    tsv_path: Path | str,
+    json_path: Path | str,
+    motion_params: np.ndarray,
+    fd: np.ndarray,
+    dvars: np.ndarray,
+) -> None:
+    """
+    Write confounds TSV and JSON sidecar with schema v1 compliance.
+
+    Args:
+        tsv_path: Path to output TSV file
+        json_path: Path to output JSON sidecar file
+        motion_params: Array of shape (n_timepoints, 6) with columns:
+                       trans_x, trans_y, trans_z, rot_x, rot_y, rot_z
+        fd: Framewise displacement array (n_timepoints,)
+        dvars: DVARS array (n_timepoints,)
+    """
+    tsv_path = Path(tsv_path)
+    json_path = Path(json_path)
+
+    # Create output directories
+    tsv_path.parent.mkdir(parents=True, exist_ok=True)
+    json_path.parent.mkdir(parents=True, exist_ok=True)
+
+    n_timepoints = len(fd)
+
+    # Validate inputs
+    if motion_params.shape[0] != n_timepoints:
+        raise ValueError(
+            f"Motion params ({motion_params.shape[0]}) and FD ({n_timepoints}) length mismatch"
+        )
+    if len(dvars) != n_timepoints:
+        raise ValueError(
+            f"DVARS ({len(dvars)}) and FD ({n_timepoints}) length mismatch"
+        )
+
+    # Write TSV with canonical column order
+    with tsv_path.open("w", newline="") as f:
+        writer = csv.writer(f, delimiter="\t")
+        # Header
+        writer.writerow(
+            ["trans_x", "trans_y", "trans_z", "rot_x", "rot_y", "rot_z", "fd", "dvars"]
+        )
+        # Data rows
+        for i in range(n_timepoints):
+            row = [
+                float(motion_params[i, 0]),
+                float(motion_params[i, 1]),
+                float(motion_params[i, 2]),
+                float(motion_params[i, 3]),
+                float(motion_params[i, 4]),
+                float(motion_params[i, 5]),
+                float(fd[i]),
+                float(dvars[i]),
+            ]
+            writer.writerow(row)
+
+    # Write JSON sidecar with schema v1 metadata
+    metadata = {
+        "SpinePrepSchemaVersion": "1.0",
+        "SoftwareName": "SpinePrep",
+        "SoftwareVersion": __version__,
+        "trans_x": {
+            "LongName": "Translation X",
+            "Description": "Translational motion parameter along X axis (left-right)",
+            "Units": "mm",
+        },
+        "trans_y": {
+            "LongName": "Translation Y",
+            "Description": "Translational motion parameter along Y axis (anterior-posterior)",
+            "Units": "mm",
+        },
+        "trans_z": {
+            "LongName": "Translation Z",
+            "Description": "Translational motion parameter along Z axis (superior-inferior)",
+            "Units": "mm",
+        },
+        "rot_x": {
+            "LongName": "Rotation X (pitch)",
+            "Description": "Rotational motion parameter around X axis",
+            "Units": "radians",
+        },
+        "rot_y": {
+            "LongName": "Rotation Y (roll)",
+            "Description": "Rotational motion parameter around Y axis",
+            "Units": "radians",
+        },
+        "rot_z": {
+            "LongName": "Rotation Z (yaw)",
+            "Description": "Rotational motion parameter around Z axis",
+            "Units": "radians",
+        },
+        "fd": {
+            "LongName": "Framewise Displacement",
+            "Description": "Framewise displacement computed using Power et al. 2012 method",
+            "Units": "mm",
+            "Method": "Power et al. (2012): sum of absolute translational displacements + radius * sum of absolute rotational displacements",
+            "Reference": "Power JD et al. (2012). Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion. NeuroImage 59:2142-2154.",
+            "RotationRadius": "50mm",
+        },
+        "dvars": {
+            "LongName": "DVARS",
+            "Description": "Temporal derivative of RMS variance over voxels (within mask)",
+            "Units": "arbitrary",
+            "Method": "sqrt(mean(voxelwise squared temporal differences))",
+            "Reference": "Power JD et al. (2012). Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion. NeuroImage 59:2142-2154.",
+        },
+    }
+
+    with json_path.open("w") as f:
+        json.dump(metadata, f, indent=2)
diff --git a/spineprep/confounds/motion.py b/spineprep/confounds/motion.py
new file mode 100644
index 00000000..0945d3cf
--- /dev/null
+++ b/spineprep/confounds/motion.py
@@ -0,0 +1,109 @@
+"""Motion metrics: FD (Power) and DVARS computation."""
+
+from __future__ import annotations
+
+import numpy as np
+
+
+def compute_fd_power(motion_params: np.ndarray, radius_mm: float = 50.0) -> np.ndarray:
+    """
+    Compute framewise displacement using Power et al. 2012 method.
+
+    FD = sum of absolute translational displacements +
+         sum of absolute rotational displacements (converted to mm using radius)
+
+    Args:
+        motion_params: Array of shape (n_timepoints, 6) with columns:
+                       trans_x, trans_y, trans_z (mm), rot_x, rot_y, rot_z (radians)
+        radius_mm: Head/body radius for rotation-to-displacement conversion (default 50mm)
+
+    Returns:
+        FD array of shape (n_timepoints,) in float32, with FD[0] = 0
+
+    References:
+        Power JD et al. (2012). Spurious but systematic correlations in functional
+        connectivity MRI networks arise from subject motion. NeuroImage 59:2142-2154.
+    """
+    if motion_params.shape[1] != 6:
+        raise ValueError(
+            f"Expected 6 motion parameters (3 trans + 3 rot), got {motion_params.shape[1]}"
+        )
+
+    n_timepoints = motion_params.shape[0]
+    fd = np.zeros(n_timepoints, dtype=np.float32)
+
+    if n_timepoints < 2:
+        return fd
+
+    # Compute temporal differences
+    delta = np.diff(motion_params, axis=0).astype(np.float32)
+
+    # FD = sum of absolute translations + radius * sum of absolute rotations
+    # Power method: summation of absolute values
+    trans_sum = np.sum(np.abs(delta[:, :3]), axis=1)
+    rot_sum = np.sum(np.abs(delta[:, 3:6]), axis=1)
+
+    fd_vals = trans_sum + radius_mm * rot_sum
+
+    # First timepoint has FD=0, rest have computed values
+    fd[1:] = fd_vals.astype(np.float32)
+
+    return fd
+
+
+def compute_dvars(img_4d: np.ndarray, mask: np.ndarray | None = None) -> np.ndarray:
+    """
+    Compute DVARS (temporal derivative RMS over masked voxels).
+
+    DVARS = sqrt(mean(voxelwise squared differences)) within mask
+
+    Args:
+        img_4d: 4D array of shape (x, y, z, t) in float32 or float64
+        mask: Optional 3D boolean mask. If None, uses voxels above median of first volume
+
+    Returns:
+        DVARS array of shape (t,) in float32, with DVARS[0] = 0
+
+    References:
+        Power JD et al. (2012). Spurious but systematic correlations in functional
+        connectivity MRI networks arise from subject motion. NeuroImage 59:2142-2154.
+    """
+    if len(img_4d.shape) != 4:
+        raise ValueError(f"Expected 4D image data, got {len(img_4d.shape)}D")
+
+    n_timepoints = img_4d.shape[3]
+    dvars = np.zeros(n_timepoints, dtype=np.float32)
+
+    if n_timepoints < 2:
+        return dvars
+
+    # Create mask if not provided
+    if mask is None:
+        first_vol = img_4d[:, :, :, 0]
+        threshold = np.median(first_vol)
+        mask = first_vol > threshold
+
+    # Ensure mask is boolean
+    mask = mask.astype(bool)
+
+    # Validate mask has voxels
+    n_voxels = mask.sum()
+    if n_voxels == 0:
+        return dvars
+
+    # Compute temporal differences
+    delta_4d = np.diff(img_4d, axis=3).astype(np.float32)
+
+    # Compute DVARS for each timepoint
+    for t in range(n_timepoints - 1):
+        delta_vol = delta_4d[:, :, :, t]
+        masked_vals = delta_vol[mask]
+
+        # RMS within mask (robust to NaNs)
+        masked_vals = masked_vals[~np.isnan(masked_vals)]
+        if len(masked_vals) > 0:
+            dvars[t + 1] = np.sqrt(np.mean(masked_vals**2))
+        else:
+            dvars[t + 1] = 0.0
+
+    return dvars.astype(np.float32)
diff --git a/spineprep/confounds/plot.py b/spineprep/confounds/plot.py
new file mode 100644
index 00000000..a909b049
--- /dev/null
+++ b/spineprep/confounds/plot.py
@@ -0,0 +1,91 @@
+"""Motion QC plotting: create multi-panel PNG with trans/rot/FD/DVARS."""
+
+from __future__ import annotations
+
+from pathlib import Path
+
+import matplotlib.pyplot as plt
+import numpy as np
+
+
+def plot_motion_png(
+    png_path: Path | str,
+    motion_params: np.ndarray,
+    fd: np.ndarray,
+    dvars: np.ndarray,
+) -> None:
+    """
+    Create a multi-panel motion QC plot.
+
+    Generates a 4-panel figure:
+    - Panel 1: Translations (x, y, z)
+    - Panel 2: Rotations (x, y, z)
+    - Panel 3: Framewise Displacement (FD)
+    - Panel 4: DVARS
+
+    Args:
+        png_path: Output PNG file path
+        motion_params: Array of shape (n_timepoints, 6)
+        fd: Framewise displacement array (n_timepoints,)
+        dvars: DVARS array (n_timepoints,)
+    """
+    png_path = Path(png_path)
+    png_path.parent.mkdir(parents=True, exist_ok=True)
+
+    n_timepoints = len(fd)
+    timepoints = np.arange(n_timepoints)
+
+    # Create figure with 4 subplots
+    fig, axes = plt.subplots(4, 1, figsize=(10, 8), sharex=True)
+
+    # Panel 1: Translations
+    axes[0].plot(timepoints, motion_params[:, 0], label="X", linewidth=1.5, alpha=0.8)
+    axes[0].plot(timepoints, motion_params[:, 1], label="Y", linewidth=1.5, alpha=0.8)
+    axes[0].plot(timepoints, motion_params[:, 2], label="Z", linewidth=1.5, alpha=0.8)
+    axes[0].set_ylabel("Translation (mm)")
+    axes[0].set_title("Motion Parameters")
+    axes[0].legend(loc="upper right", framealpha=0.9)
+    axes[0].grid(True, alpha=0.3)
+
+    # Panel 2: Rotations (convert radians to degrees for display)
+    axes[1].plot(
+        timepoints,
+        np.rad2deg(motion_params[:, 3]),
+        label="X (pitch)",
+        linewidth=1.5,
+        alpha=0.8,
+    )
+    axes[1].plot(
+        timepoints,
+        np.rad2deg(motion_params[:, 4]),
+        label="Y (roll)",
+        linewidth=1.5,
+        alpha=0.8,
+    )
+    axes[1].plot(
+        timepoints,
+        np.rad2deg(motion_params[:, 5]),
+        label="Z (yaw)",
+        linewidth=1.5,
+        alpha=0.8,
+    )
+    axes[1].set_ylabel("Rotation (degrees)")
+    axes[1].legend(loc="upper right", framealpha=0.9)
+    axes[1].grid(True, alpha=0.3)
+
+    # Panel 3: Framewise Displacement
+    axes[2].plot(timepoints, fd, linewidth=1.5, color="red", alpha=0.8)
+    axes[2].axhline(y=0.5, color="orange", linestyle="--", linewidth=1, alpha=0.6)
+    axes[2].set_ylabel("FD (mm)")
+    axes[2].grid(True, alpha=0.3)
+
+    # Panel 4: DVARS
+    axes[3].plot(timepoints, dvars, linewidth=1.5, color="purple", alpha=0.8)
+    axes[3].set_ylabel("DVARS")
+    axes[3].set_xlabel("Timepoint")
+    axes[3].grid(True, alpha=0.3)
+
+    # Adjust layout and save
+    plt.tight_layout()
+    plt.savefig(png_path, dpi=100, bbox_inches="tight")
+    plt.close(fig)
diff --git a/spineprep/confounds/schema_v1.json b/spineprep/confounds/schema_v1.json
new file mode 100644
index 00000000..92e13a3a
--- /dev/null
+++ b/spineprep/confounds/schema_v1.json
@@ -0,0 +1,99 @@
+{
+  "$schema": "https://json-schema.org/draft/2020-12/schema",
+  "title": "SpinePrep Confounds Schema v1.0",
+  "description": "Column definitions and metadata for confounds timeseries TSV and JSON sidecar",
+  "type": "object",
+  "required": ["SpinePrepSchemaVersion", "SoftwareName", "SoftwareVersion"],
+  "properties": {
+    "SpinePrepSchemaVersion": {
+      "type": "string",
+      "const": "1.0",
+      "description": "Schema version for confounds metadata"
+    },
+    "SoftwareName": {
+      "type": "string",
+      "description": "Name of the software that generated the confounds"
+    },
+    "SoftwareVersion": {
+      "type": "string",
+      "description": "Version of the software"
+    },
+    "trans_x": {
+      "type": "object",
+      "required": ["LongName", "Description", "Units"],
+      "properties": {
+        "LongName": {"type": "string"},
+        "Description": {"type": "string"},
+        "Units": {"type": "string"}
+      }
+    },
+    "trans_y": {
+      "type": "object",
+      "required": ["LongName", "Description", "Units"],
+      "properties": {
+        "LongName": {"type": "string"},
+        "Description": {"type": "string"},
+        "Units": {"type": "string"}
+      }
+    },
+    "trans_z": {
+      "type": "object",
+      "required": ["LongName", "Description", "Units"],
+      "properties": {
+        "LongName": {"type": "string"},
+        "Description": {"type": "string"},
+        "Units": {"type": "string"}
+      }
+    },
+    "rot_x": {
+      "type": "object",
+      "required": ["LongName", "Description", "Units"],
+      "properties": {
+        "LongName": {"type": "string"},
+        "Description": {"type": "string"},
+        "Units": {"type": "string"}
+      }
+    },
+    "rot_y": {
+      "type": "object",
+      "required": ["LongName", "Description", "Units"],
+      "properties": {
+        "LongName": {"type": "string"},
+        "Description": {"type": "string"},
+        "Units": {"type": "string"}
+      }
+    },
+    "rot_z": {
+      "type": "object",
+      "required": ["LongName", "Description", "Units"],
+      "properties": {
+        "LongName": {"type": "string"},
+        "Description": {"type": "string"},
+        "Units": {"type": "string"}
+      }
+    },
+    "fd": {
+      "type": "object",
+      "required": ["LongName", "Description", "Units", "Method", "Reference"],
+      "properties": {
+        "LongName": {"type": "string"},
+        "Description": {"type": "string"},
+        "Units": {"type": "string"},
+        "Method": {"type": "string"},
+        "Reference": {"type": "string"},
+        "RotationRadius": {"type": "string"}
+      }
+    },
+    "dvars": {
+      "type": "object",
+      "required": ["LongName", "Description", "Units", "Method", "Reference"],
+      "properties": {
+        "LongName": {"type": "string"},
+        "Description": {"type": "string"},
+        "Units": {"type": "string"},
+        "Method": {"type": "string"},
+        "Reference": {"type": "string"}
+      }
+    }
+  }
+}
diff --git a/spineprep/doctor.py b/spineprep/doctor.py
index 142c319b..a7370841 100644
--- a/spineprep/doctor.py
+++ b/spineprep/doctor.py
@@ -7,6 +7,8 @@ import os
 import platform
 import shutil
 import subprocess
+import sys
+import tempfile
 from datetime import datetime, timezone
 from pathlib import Path
 from typing import Any
@@ -166,11 +168,48 @@ def detect_os_info() -> dict[str, Any]:
     return info


+def check_disk_space() -> int:
+    """
+    Check available disk space.
+
+    Returns:
+        Free bytes available on the filesystem containing cwd.
+    """
+    try:
+        stat = os.statvfs(Path.cwd())
+        return stat.f_bavail * stat.f_frsize
+    except Exception:
+        return 0
+
+
+def check_writeable(path: Path) -> bool:
+    """
+    Check if a directory is writeable.
+
+    Args:
+        path: Directory to check
+
+    Returns:
+        True if writeable, False otherwise.
+    """
+    try:
+        # Try to create a temporary file
+        test_file = path / f".spineprep_test_{os.getpid()}"
+        test_file.write_text("test")
+        test_file.unlink()
+        return True
+    except Exception:
+        return False
+
+
 def generate_report(
     sct_info: dict[str, Any],
     pam50_info: dict[str, Any],
     python_deps: dict[str, str],
     os_info: dict[str, Any],
+    disk_free: int,
+    cwd_writeable: bool,
+    tmp_writeable: bool,
     strict: bool = False,
 ) -> dict[str, Any]:
     """
@@ -181,10 +220,13 @@ def generate_report(
         pam50_info: PAM50 detection results
         python_deps: Python dependencies detection results
         os_info: OS information
+        disk_free: Free disk space in bytes
+        cwd_writeable: Whether cwd is writeable
+        tmp_writeable: Whether /tmp is writeable
         strict: If True, warnings become failures

     Returns:
-        Complete report dictionary.
+        Complete report dictionary matching ticket A1 format.
     """
     notes: list[str] = []
     status = "pass"
@@ -195,6 +237,10 @@ def generate_report(
         notes.append(
             "HARD FAIL: SCT not found. Install from https://github.com/spinalcordtoolbox/spinalcordtoolbox"
         )
+        notes.append(
+            "Remediation: Use Docker (docker pull sctorg/sct:latest) or "
+            "Apptainer/Singularity, or install locally via official installer."
+        )

     # Check PAM50 (hard requirement)
     if not pam50_info["found"]:
@@ -217,16 +263,31 @@ def generate_report(
     if strict and status == "warn":
         status = "fail"

+    # Build report matching ticket A1 format
     return {
+        "python": {"version": os_info["python"]},
         "spineprep": {"version": __version__},
+        "platform": {
+            "os": os_info["os"],
+            "kernel": os_info.get("kernel", "unknown"),
+            "cpu_count": os_info.get("cpu_count", 0),
+            "ram_gb": os_info.get("ram_gb", 0),
+        },
+        "packages": python_deps,
+        "sct": {
+            "present": sct_info["found"],
+            "version": sct_info.get("version", ""),
+            "path": sct_info.get("path", ""),
+        },
+        "pam50": {
+            "present": pam50_info["found"] and pam50_info.get("files_ok", False),
+            "path": pam50_info.get("path", ""),
+        },
+        "disk": {"free_bytes": disk_free},
+        "cwd_writeable": cwd_writeable,
+        "tmp_writeable": tmp_writeable,
         "timestamp": datetime.now(timezone.utc).astimezone().isoformat(),
         "status": status,
-        "platform": os_info,
-        "deps": {
-            "sct": sct_info,
-            "pam50": pam50_info,
-            "python": python_deps,
-        },
         "notes": notes,
     }

@@ -234,7 +295,7 @@ def generate_report(
 def write_doctor_report(
     report: dict[str, Any],
     out_dir: Path,
-    json_path: Path | None = None,
+    json_path: Path | str | None = None,
 ) -> Path:
     """
     Write doctor report to JSON file.
@@ -242,7 +303,7 @@ def write_doctor_report(
     Args:
         report: Report dictionary
         out_dir: Output directory for provenance files
-        json_path: Optional custom JSON path for additional copy
+        json_path: Optional custom JSON path for additional copy (Path or str, but not "-")

     Returns:
         Path to the main JSON report file.
@@ -260,9 +321,10 @@ def write_doctor_report(
         json.dump(report, f, indent=2)

     # Write additional copy if requested
-    if json_path:
-        json_path.parent.mkdir(parents=True, exist_ok=True)
-        with open(json_path, "w") as f:
+    if json_path and json_path != "-":
+        json_p = Path(json_path) if isinstance(json_path, str) else json_path
+        json_p.parent.mkdir(parents=True, exist_ok=True)
+        with open(json_p, "w") as f:
             json.dump(report, f, indent=2)

     return main_json
@@ -306,21 +368,21 @@ def print_doctor_table(report: dict[str, Any]) -> None:

     # Platform info
     plat = report["platform"]
+    py_ver = report["python"]["version"]
     print("Platform:")
     print(f"  OS:       {plat['os']} {plat.get('kernel', 'unknown')}")
-    print(f"  Python:   {plat['python']}")
+    print(f"  Python:   {py_ver}")
     print(f"  CPU:      {plat['cpu_count']} cores")
     if plat.get("ram_gb"):
         print(f"  RAM:      {plat['ram_gb']} GB")
     print()

     # Dependencies
-    deps = report["deps"]
     print("Dependencies:")

     # SCT
-    sct = deps["sct"]
-    if sct["found"]:
+    sct = report["sct"]
+    if sct["present"]:
         print(f"  [{GREEN}✓{RESET}] SCT:")
         print(f"      Version: {sct.get('version', 'unknown')}")
         print(f"      Path:    {sct.get('path', 'unknown')}")
@@ -329,20 +391,20 @@ def print_doctor_table(report: dict[str, Any]) -> None:
         print("      Status:  NOT FOUND")

     # PAM50
-    pam50 = deps["pam50"]
-    if pam50["found"] and pam50.get("files_ok"):
+    pam50 = report["pam50"]
+    if pam50["present"]:
         print(f"  [{GREEN}✓{RESET}] PAM50:")
         print(f"      Path:    {pam50.get('path', 'unknown')}")
-    elif pam50["found"]:
-        print(f"  [{RED}✗{RESET}] PAM50:")
-        print(f"      Path:    {pam50.get('path', 'unknown')}")
-        print("      Status:  INCOMPLETE (missing required files)")
     else:
         print(f"  [{RED}✗{RESET}] PAM50:")
-        print("      Status:  NOT FOUND")
+        if pam50.get("path"):
+            print(f"      Path:    {pam50.get('path', 'unknown')}")
+            print("      Status:  INCOMPLETE (missing required files)")
+        else:
+            print("      Status:  NOT FOUND")

     # Python packages
-    py_deps = deps["python"]
+    py_deps = report.get("packages", {})
     if py_deps:
         missing = [pkg for pkg, ver in py_deps.items() if not ver]
         if not missing:
@@ -355,6 +417,16 @@ def print_doctor_table(report: dict[str, Any]) -> None:
             else:
                 print(f"      {pkg:15} {RED}MISSING{RESET}")

+    # Disk & permissions
+    print()
+    print("Disk & Permissions:")
+    disk_gb = report["disk"]["free_bytes"] / 1e9
+    print(f"  Disk free:    {disk_gb:.1f} GB")
+    cwd_symbol = f"{GREEN}✓{RESET}" if report["cwd_writeable"] else f"{RED}✗{RESET}"
+    tmp_symbol = f"{GREEN}✓{RESET}" if report["tmp_writeable"] else f"{RED}✗{RESET}"
+    print(f"  CWD write:    [{cwd_symbol}]")
+    print(f"  /tmp write:   [{tmp_symbol}]")
+
     # Notes
     if report["notes"]:
         print()
@@ -362,6 +434,8 @@ def print_doctor_table(report: dict[str, Any]) -> None:
         for note in report["notes"]:
             if "HARD FAIL" in note:
                 print(f"  {RED}•{RESET} {note}")
+            elif "Remediation" in note:
+                print(f"  {YELLOW}•{RESET} {note}")
             elif "Missing" in note or "missing" in note:
                 print(f"  {YELLOW}•{RESET} {note}")
             else:
@@ -375,7 +449,7 @@ def print_doctor_table(report: dict[str, Any]) -> None:
 def cmd_doctor(
     out_dir: Path,
     pam50: str | None,
-    json_path: Path | None,
+    json_path: Path | str | None,
     strict: bool,
 ) -> int:
     """
@@ -384,7 +458,7 @@ def cmd_doctor(
     Args:
         out_dir: Output directory for provenance files
         pam50: Explicit PAM50 path (optional)
-        json_path: Optional custom JSON path
+        json_path: Optional custom JSON path (Path object, "-" for stdout, or None)
         strict: Treat warnings as errors

     Returns:
@@ -395,21 +469,42 @@ def cmd_doctor(
     pam50_info = detect_pam50(pam50)
     python_deps = detect_python_deps()
     os_info = detect_os_info()
+    disk_free = check_disk_space()
+    cwd_writeable = check_writeable(Path.cwd())
+    tmp_writeable = check_writeable(Path(tempfile.gettempdir()))

     # Generate report
-    report = generate_report(sct_info, pam50_info, python_deps, os_info, strict=strict)
-
-    # Write to disk
-    report_path = write_doctor_report(report, out_dir, json_path=json_path)
-
-    # Print table
-    print_doctor_table(report)
+    report = generate_report(
+        sct_info,
+        pam50_info,
+        python_deps,
+        os_info,
+        disk_free,
+        cwd_writeable,
+        tmp_writeable,
+        strict=strict,
+    )

-    # Print artifact location
-    print(f"Doctor report written to: {report_path}")
+    # Handle JSON output
     if json_path:
-        print(f"Additional copy written to: {json_path}")
-    print()
+        if str(json_path) == "-":
+            # Output to stdout
+            json.dump(report, sys.stdout, indent=2)
+            sys.stdout.write("\n")
+            return 0 if report["status"] == "pass" else 1
+        else:
+            # Write to specified file
+            report_path = write_doctor_report(report, out_dir, json_path=json_path)
+            print(f"Doctor report written to: {report_path}")
+            if json_path:
+                print(f"Additional copy written to: {json_path}")
+            print()
+    else:
+        # Normal mode: write to provenance and print table
+        report_path = write_doctor_report(report, out_dir, json_path=None)
+        print_doctor_table(report)
+        print(f"Doctor report written to: {report_path}")
+        print()

     # Exit code based on status
     if report["status"] == "fail":
diff --git a/spineprep/preproc/__init__.py b/spineprep/preproc/__init__.py
new file mode 100644
index 00000000..c6978af4
--- /dev/null
+++ b/spineprep/preproc/__init__.py
@@ -0,0 +1 @@
+"""Preprocessing modules for SpinePrep."""
diff --git a/spineprep/preproc/__pycache__/__init__.cpython-312.pyc b/spineprep/preproc/__pycache__/__init__.cpython-312.pyc
new file mode 100644
index 00000000..25b630d1
Binary files /dev/null and b/spineprep/preproc/__pycache__/__init__.cpython-312.pyc differ
diff --git a/spineprep/preproc/__pycache__/masking.cpython-312.pyc b/spineprep/preproc/__pycache__/masking.cpython-312.pyc
new file mode 100644
index 00000000..f94f3a28
Binary files /dev/null and b/spineprep/preproc/__pycache__/masking.cpython-312.pyc differ
diff --git a/spineprep/preproc/masking.py b/spineprep/preproc/masking.py
new file mode 100644
index 00000000..13f77e61
--- /dev/null
+++ b/spineprep/preproc/masking.py
@@ -0,0 +1,351 @@
+"""Spinal cord and CSF mask generation with QC overlays.
+
+This module creates:
+- Spinal cord mask from T2w using SCT deepseg
+- Optional CSF ring mask (dilation-based)
+- QC PNG overlay
+- Provenance JSON with SCT version and commands
+"""
+
+from __future__ import annotations
+
+import json
+import logging
+from datetime import datetime, timezone
+from pathlib import Path
+from typing import Any
+
+import matplotlib.pyplot as plt
+import nibabel as nib
+import numpy as np
+from scipy.ndimage import binary_dilation, label
+
+from spineprep._sct import run_sct_deepseg
+
+logger = logging.getLogger(__name__)
+
+
+def create_csf_ring_mask(
+    cord_mask_img: Any,  # nibabel image
+    inner_dilation: int = 1,
+    outer_dilation: int = 2,
+) -> Any:  # nibabel image
+    """
+    Create CSF ring mask by dilating cord mask.
+
+    Args:
+        cord_mask_img: Cord mask NIfTI image
+        inner_dilation: Inner dilation radius (voxels)
+        outer_dilation: Outer dilation radius (voxels)
+
+    Returns:
+        CSF ring mask as NIfTI image
+    """
+    cord_data = cord_mask_img.get_fdata().astype(bool)
+
+    # Create outer and inner dilations
+    outer = binary_dilation(cord_data, iterations=outer_dilation)
+    inner = binary_dilation(cord_data, iterations=inner_dilation)
+
+    # CSF ring = outer - inner
+    csf_ring = np.logical_and(outer, np.logical_not(inner)).astype(np.uint8)
+
+    # Clean small components (keep only largest connected component)
+    labeled, num_features = label(csf_ring)
+    if num_features > 1:
+        # Keep largest component
+        sizes = np.bincount(labeled.ravel())[1:]  # Skip background (0)
+        largest_label = sizes.argmax() + 1
+        csf_ring = (labeled == largest_label).astype(np.uint8)
+
+    return nib.Nifti1Image(csf_ring, cord_mask_img.affine, cord_mask_img.header)
+
+
+def create_qc_overlay(
+    t2w_path: str,
+    cord_mask_path: str,
+    csf_mask_path: str | None,
+    out_png: str,
+) -> None:
+    """
+    Create QC overlay PNG showing T2w with cord and CSF masks.
+
+    Args:
+        t2w_path: Path to T2w image
+        cord_mask_path: Path to cord mask
+        csf_mask_path: Path to CSF mask (optional)
+        out_png: Output PNG path
+    """
+    # Load images
+    t2w_img = nib.load(t2w_path)
+    t2w_data = t2w_img.get_fdata()
+
+    cord_img = nib.load(cord_mask_path)
+    cord_data = cord_img.get_fdata().astype(bool)
+
+    csf_data = None
+    if csf_mask_path and Path(csf_mask_path).exists():
+        csf_img = nib.load(csf_mask_path)
+        csf_data = csf_img.get_fdata().astype(bool)
+
+    # Ensure output directory exists
+    Path(out_png).parent.mkdir(parents=True, exist_ok=True)
+
+    # Find middle slices in each dimension with signal
+    def find_middle_slice(data, axis):
+        """Find middle slice with non-zero data along axis."""
+        projection = np.sum(data, axis=tuple(i for i in range(3) if i != axis))
+        nonzero = np.where(projection > 0)[0]
+        if len(nonzero) == 0:
+            return data.shape[axis] // 2
+        return nonzero[len(nonzero) // 2]
+
+    # Get middle slices
+    slice_sag = find_middle_slice(cord_data, 0)
+    slice_cor = find_middle_slice(cord_data, 1)
+    slice_ax = find_middle_slice(cord_data, 2)
+
+    # Create figure with 3 subplots
+    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
+
+    # Sagittal
+    ax = axes[0]
+    ax.imshow(t2w_data[slice_sag, :, :].T, cmap="gray", origin="lower")
+    if cord_data[slice_sag, :, :].any():
+        ax.contour(
+            cord_data[slice_sag, :, :].T,
+            levels=[0.5],
+            colors="red",
+            linewidths=2,
+            alpha=0.7,
+        )
+    if csf_data is not None and csf_data[slice_sag, :, :].any():
+        ax.contour(
+            csf_data[slice_sag, :, :].T,
+            levels=[0.5],
+            colors="blue",
+            linewidths=2,
+            alpha=0.7,
+        )
+    ax.set_title("Sagittal")
+    ax.axis("off")
+
+    # Coronal
+    ax = axes[1]
+    ax.imshow(t2w_data[:, slice_cor, :].T, cmap="gray", origin="lower")
+    if cord_data[:, slice_cor, :].any():
+        ax.contour(
+            cord_data[:, slice_cor, :].T,
+            levels=[0.5],
+            colors="red",
+            linewidths=2,
+            alpha=0.7,
+        )
+    if csf_data is not None and csf_data[:, slice_cor, :].any():
+        ax.contour(
+            csf_data[:, slice_cor, :].T,
+            levels=[0.5],
+            colors="blue",
+            linewidths=2,
+            alpha=0.7,
+        )
+    ax.set_title("Coronal")
+    ax.axis("off")
+
+    # Axial
+    ax = axes[2]
+    ax.imshow(t2w_data[:, :, slice_ax].T, cmap="gray", origin="lower")
+    if cord_data[:, :, slice_ax].any():
+        ax.contour(
+            cord_data[:, :, slice_ax].T,
+            levels=[0.5],
+            colors="red",
+            linewidths=2,
+            alpha=0.7,
+        )
+    if csf_data is not None and csf_data[:, :, slice_ax].any():
+        ax.contour(
+            csf_data[:, :, slice_ax].T,
+            levels=[0.5],
+            colors="blue",
+            linewidths=2,
+            alpha=0.7,
+        )
+    ax.set_title("Axial")
+    ax.axis("off")
+
+    # Add legend
+    from matplotlib.patches import Patch
+
+    legend_elements = [Patch(facecolor="red", alpha=0.7, label="Cord")]
+    if csf_data is not None:
+        legend_elements.append(Patch(facecolor="blue", alpha=0.7, label="CSF"))
+    fig.legend(handles=legend_elements, loc="upper right")
+
+    plt.tight_layout()
+    plt.savefig(out_png, dpi=150, bbox_inches="tight")
+    plt.close()
+
+
+def save_provenance_json(
+    out_path: str,
+    cmd: str,
+    sct_version: str,
+    inputs: dict[str, str],
+) -> None:
+    """
+    Save provenance JSON with SCT version and command.
+
+    Args:
+        out_path: Output JSON path
+        cmd: Full command executed
+        sct_version: SCT version string
+        inputs: Dictionary of input file paths
+    """
+    provenance = {
+        "cmd": cmd,
+        "sct_version": sct_version,
+        "time_utc": datetime.now(timezone.utc).isoformat(),
+        "inputs": inputs,
+    }
+
+    Path(out_path).parent.mkdir(parents=True, exist_ok=True)
+
+    with open(out_path, "w") as f:
+        json.dump(provenance, f, indent=2)
+
+
+def run_masks(
+    t2w_path: str,
+    subject: str,
+    out_root: str,
+    session: str | None = None,
+    make_csf: bool = True,
+    qc: bool = True,
+    logger_inst: logging.Logger | None = None,
+) -> dict[str, str]:
+    """
+    Generate cord and optional CSF masks with QC overlay.
+
+    Args:
+        t2w_path: Path to input T2w image
+        subject: Subject ID (e.g., "sub-01")
+        out_root: Output root directory (e.g., derivatives/spineprep)
+        session: Optional session ID (e.g., "ses-01")
+        make_csf: Whether to create CSF ring mask
+        qc: Whether to create QC overlay PNG
+        logger_inst: Optional logger instance
+
+    Returns:
+        Dictionary with output paths:
+        - cord_mask: cord mask NIfTI path
+        - csf_mask: CSF mask NIfTI path (if make_csf=True)
+        - cord_json: provenance JSON path
+        - qc_png: QC overlay PNG path (if qc=True)
+
+    Raises:
+        RuntimeError: If SCT deepseg fails or mask is empty
+    """
+    log = logger_inst or logger
+
+    # Build output paths
+    base = Path(out_root) / subject
+    if session:
+        base = base / session
+
+    anat_dir = base / "anat"
+    qc_dir = base / "qc"
+
+    anat_dir.mkdir(parents=True, exist_ok=True)
+    qc_dir.mkdir(parents=True, exist_ok=True)
+
+    # Build filenames
+    entities = subject
+    if session:
+        entities += f"_{session}"
+
+    cord_mask_path = anat_dir / f"{entities}_desc-cord_mask.nii.gz"
+    csf_mask_path = anat_dir / f"{entities}_desc-csf_mask.nii.gz" if make_csf else None
+    cord_json_path = anat_dir / f"{entities}_desc-cord_mask.json"
+    qc_png_path = qc_dir / f"{entities}_anat_mask_overlay.png" if qc else None
+
+    # Track outputs for cleanup on failure
+    created_files = []
+
+    try:
+        # 1. Run SCT deepseg for cord mask
+        log.info(f"Running SCT deepseg on {t2w_path}")
+        result = run_sct_deepseg(
+            t2w_path=t2w_path,
+            out_mask=str(cord_mask_path),
+            contrast="t2",
+        )
+
+        created_files.append(cord_mask_path)
+
+        if not result["success"]:
+            raise RuntimeError(
+                f"SCT deepseg failed (exit {result['return_code']}): {result['stderr']}"
+            )
+
+        log.info(f"Cord mask created: {cord_mask_path}")
+
+        # Check that mask is not empty
+        cord_img = nib.load(cord_mask_path)
+        cord_data = cord_img.get_fdata()
+        if cord_data.sum() == 0:
+            raise RuntimeError(
+                f"SCT deepseg produced empty mask for {t2w_path}. "
+                "Check input image quality and contrast."
+            )
+
+        # 2. Create CSF ring mask if requested
+        if make_csf and csf_mask_path is not None:
+            log.info("Creating CSF ring mask")
+            csf_img = create_csf_ring_mask(cord_img, inner_dilation=1, outer_dilation=2)
+            nib.save(csf_img, str(csf_mask_path))
+            created_files.append(csf_mask_path)
+            log.info(f"CSF mask created: {csf_mask_path}")
+
+        # 3. Save provenance JSON
+        save_provenance_json(
+            out_path=str(cord_json_path),
+            cmd=result["cmd"],
+            sct_version=result["sct_version"],
+            inputs={"t2w": t2w_path},
+        )
+        created_files.append(cord_json_path)
+        log.info(f"Provenance saved: {cord_json_path}")
+
+        # 4. Create QC overlay
+        if qc:
+            log.info("Creating QC overlay")
+            create_qc_overlay(
+                t2w_path=t2w_path,
+                cord_mask_path=str(cord_mask_path),
+                csf_mask_path=str(csf_mask_path) if make_csf else None,
+                out_png=str(qc_png_path),
+            )
+            created_files.append(qc_png_path)
+            log.info(f"QC overlay saved: {qc_png_path}")
+
+        # Build return dictionary
+        outputs = {
+            "cord_mask": str(cord_mask_path),
+            "cord_json": str(cord_json_path),
+        }
+        if make_csf:
+            outputs["csf_mask"] = str(csf_mask_path)
+        if qc:
+            outputs["qc_png"] = str(qc_png_path)
+
+        return outputs
+
+    except Exception as e:
+        # Clean up partial outputs on failure
+        log.error(f"Masking failed: {e}")
+        for filepath in created_files:
+            if filepath.exists():
+                log.warning(f"Removing partial output: {filepath}")
+                filepath.unlink()
+        raise
diff --git a/spineprep/qc/__init__.py b/spineprep/qc/__init__.py
new file mode 100644
index 00000000..1e777bf7
--- /dev/null
+++ b/spineprep/qc/__init__.py
@@ -0,0 +1,8 @@
+"""QC HTML generation for SpinePrep."""
+
+from __future__ import annotations
+
+from spineprep.qc.legacy import build_qc_index, build_subject_qc
+from spineprep.qc.report import write_qc
+
+__all__ = ["build_qc_index", "build_subject_qc", "write_qc"]
diff --git a/spineprep/qc/__pycache__/__init__.cpython-312.pyc b/spineprep/qc/__pycache__/__init__.cpython-312.pyc
new file mode 100644
index 00000000..946f5501
Binary files /dev/null and b/spineprep/qc/__pycache__/__init__.cpython-312.pyc differ
diff --git a/spineprep/qc/__pycache__/legacy.cpython-312.pyc b/spineprep/qc/__pycache__/legacy.cpython-312.pyc
new file mode 100644
index 00000000..d865f8f0
Binary files /dev/null and b/spineprep/qc/__pycache__/legacy.cpython-312.pyc differ
diff --git a/spineprep/qc/__pycache__/report.cpython-312.pyc b/spineprep/qc/__pycache__/report.cpython-312.pyc
new file mode 100644
index 00000000..1bdb91b7
Binary files /dev/null and b/spineprep/qc/__pycache__/report.cpython-312.pyc differ
diff --git a/spineprep/qc/assets/base.css b/spineprep/qc/assets/base.css
new file mode 100644
index 00000000..d55a9d48
--- /dev/null
+++ b/spineprep/qc/assets/base.css
@@ -0,0 +1,60 @@
+/* Base CSS for SpinePrep QC Reports */
+body {
+    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
+    max-width: 1400px;
+    margin: 0 auto;
+    padding: 20px;
+    background: #f5f5f5;
+    color: #333;
+}
+
+h1, h2, h3 {
+    color: #2c3e50;
+    margin-top: 0;
+}
+
+.header {
+    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
+    color: white;
+    padding: 30px;
+    border-radius: 8px;
+    margin-bottom: 20px;
+}
+
+.card {
+    background: white;
+    padding: 20px;
+    margin: 15px 0;
+    border-radius: 8px;
+    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
+}
+
+.overlay-img {
+    max-width: 100%;
+    height: auto;
+    border: 1px solid #ddd;
+    border-radius: 4px;
+    margin: 10px 0;
+}
+
+table {
+    width: 100%;
+    border-collapse: collapse;
+    margin: 15px 0;
+    font-size: 14px;
+}
+
+th, td {
+    padding: 8px 12px;
+    text-align: left;
+    border-bottom: 1px solid #e0e0e0;
+}
+
+th {
+    background: #f8f9fa;
+    font-weight: 600;
+}
+
+.status-good { color: #27ae60; font-weight: bold; }
+.status-warn { color: #f39c12; font-weight: bold; }
+.status-fail { color: #e74c3c; font-weight: bold; }
diff --git a/spineprep/qc/assets/tabs.js b/spineprep/qc/assets/tabs.js
new file mode 100644
index 00000000..e51800ef
--- /dev/null
+++ b/spineprep/qc/assets/tabs.js
@@ -0,0 +1,30 @@
+/* Minimal tab switching for QC overlays */
+function switchTab(tabName) {
+    // Hide all tab contents
+    document.querySelectorAll('.tab-content').forEach(el => {
+        el.classList.remove('active');
+    });
+    // Remove active class from all tabs
+    document.querySelectorAll('.tab').forEach(el => {
+        el.classList.remove('active');
+    });
+    // Show selected tab content
+    const content = document.getElementById(tabName);
+    if (content) {
+        content.classList.add('active');
+    }
+    // Activate selected tab
+    const tab = document.querySelector(`[data-tab="${tabName}"]`);
+    if (tab) {
+        tab.classList.add('active');
+    }
+}
+
+// Activate first tab on load
+document.addEventListener('DOMContentLoaded', () => {
+    const firstTab = document.querySelector('.tab');
+    if (firstTab) {
+        const tabName = firstTab.getAttribute('data-tab');
+        switchTab(tabName);
+    }
+});
diff --git a/spineprep/qc.py b/spineprep/qc/legacy.py
similarity index 100%
rename from spineprep/qc.py
rename to spineprep/qc/legacy.py
diff --git a/spineprep/qc/report.py b/spineprep/qc/report.py
new file mode 100644
index 00000000..ed03636b
--- /dev/null
+++ b/spineprep/qc/report.py
@@ -0,0 +1,506 @@
+"""Per-subject QC HTML report generation with overlays, motion, and confounds."""
+
+from __future__ import annotations
+
+import csv
+import json
+from datetime import datetime
+from pathlib import Path
+from typing import Any
+
+# Inline CSS for self-contained HTML
+CSS = """
+body {
+    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
+    max-width: 1400px;
+    margin: 0 auto;
+    padding: 20px;
+    background: #f5f5f5;
+    color: #333;
+}
+h1, h2, h3 { color: #2c3e50; margin-top: 0; }
+.header {
+    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
+    color: white;
+    padding: 30px;
+    border-radius: 8px;
+    margin-bottom: 20px;
+}
+.card {
+    background: white;
+    padding: 20px;
+    margin: 15px 0;
+    border-radius: 8px;
+    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
+}
+.tabs {
+    display: flex;
+    gap: 10px;
+    margin-bottom: 15px;
+    border-bottom: 2px solid #e0e0e0;
+}
+.tab {
+    padding: 10px 20px;
+    cursor: pointer;
+    background: #f0f0f0;
+    border: none;
+    border-radius: 4px 4px 0 0;
+    font-size: 14px;
+    transition: background 0.2s;
+}
+.tab:hover {
+    background: #e0e0e0;
+}
+.tab.active {
+    background: white;
+    font-weight: 600;
+    border-bottom: 2px solid #667eea;
+}
+.tab-content {
+    display: none;
+}
+.tab-content.active {
+    display: block;
+}
+.overlay-img {
+    max-width: 100%;
+    height: auto;
+    border: 1px solid #ddd;
+    border-radius: 4px;
+    margin: 10px 0;
+}
+table {
+    width: 100%;
+    border-collapse: collapse;
+    margin: 15px 0;
+    font-size: 14px;
+}
+th, td {
+    padding: 8px 12px;
+    text-align: left;
+    border-bottom: 1px solid #e0e0e0;
+}
+th {
+    background: #f8f9fa;
+    font-weight: 600;
+    color: #555;
+}
+tr:hover {
+    background: #f9f9f9;
+}
+.metric-grid {
+    display: grid;
+    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
+    gap: 15px;
+    margin: 15px 0;
+}
+.metric-box {
+    background: #f8f9fa;
+    padding: 15px;
+    border-radius: 6px;
+    border-left: 4px solid #667eea;
+}
+.metric-label {
+    font-size: 12px;
+    color: #666;
+    text-transform: uppercase;
+    letter-spacing: 0.5px;
+}
+.metric-value {
+    font-size: 24px;
+    font-weight: 600;
+    color: #2c3e50;
+    margin-top: 5px;
+}
+.status-good { color: #27ae60; font-weight: bold; }
+.status-warn { color: #f39c12; font-weight: bold; }
+.status-fail { color: #e74c3c; font-weight: bold; }
+.footer {
+    text-align: center;
+    color: #7f8c8d;
+    margin-top: 40px;
+    padding-top: 20px;
+    border-top: 1px solid #ddd;
+    font-size: 14px;
+}
+"""
+
+# Minimal JavaScript for tabs
+TABS_JS = """
+function switchTab(tabName) {
+    // Hide all tab contents
+    document.querySelectorAll('.tab-content').forEach(el => {
+        el.classList.remove('active');
+    });
+    // Remove active class from all tabs
+    document.querySelectorAll('.tab').forEach(el => {
+        el.classList.remove('active');
+    });
+    // Show selected tab content
+    const content = document.getElementById(tabName);
+    if (content) {
+        content.classList.add('active');
+    }
+    // Activate selected tab
+    const tab = document.querySelector(`[data-tab="${tabName}"]`);
+    if (tab) {
+        tab.classList.add('active');
+    }
+}
+// Activate first tab on load
+document.addEventListener('DOMContentLoaded', () => {
+    const firstTab = document.querySelector('.tab');
+    if (firstTab) {
+        const tabName = firstTab.getAttribute('data-tab');
+        switchTab(tabName);
+    }
+});
+"""
+
+
+def load_confounds(confounds_tsv: Path) -> tuple[list[dict[str, Any]], list[str]]:
+    """
+    Load confounds TSV file.
+
+    Args:
+        confounds_tsv: Path to confounds TSV
+
+    Returns:
+        Tuple of (rows as list of dicts, column names)
+    """
+    if not confounds_tsv.exists():
+        return [], []
+
+    rows: list[dict[str, Any]] = []
+    cols: list[str] = []
+
+    with confounds_tsv.open() as f:
+        reader = csv.DictReader(f, delimiter="\t")
+        rows = list(reader)  # type: ignore[assignment]
+        cols = list(reader.fieldnames) if reader.fieldnames else []
+
+    return rows, cols
+
+
+def load_doctor_json(doctor_json: Path | None) -> dict[str, Any]:
+    """
+    Load doctor.json environment report.
+
+    Args:
+        doctor_json: Path to doctor JSON file
+
+    Returns:
+        Doctor report dictionary or empty dict if not found
+    """
+    if not doctor_json or not doctor_json.exists():
+        return {}
+
+    try:
+        with doctor_json.open() as f:
+            return json.load(f)
+    except Exception:
+        return {}
+
+
+def build_motion_summary_html(rows: list[dict[str, Any]]) -> str:
+    """Build motion summary metrics HTML."""
+    if not rows:
+        return "<p>No motion data available.</p>"
+
+    # Compute summary stats
+    fd_values = [float(r.get("fd", 0)) for r in rows if r.get("fd")]
+    dvars_values = [float(r.get("dvars", 0)) for r in rows if r.get("dvars")]
+    censored = sum(1 for r in rows if r.get("censor") == "1")
+
+    max_fd = max(fd_values) if fd_values else 0
+    mean_fd = sum(fd_values) / len(fd_values) if fd_values else 0
+    max_dvars = max(dvars_values) if dvars_values else 0
+    mean_dvars = sum(dvars_values) / len(dvars_values) if dvars_values else 0
+
+    # Color coding
+    fd_status = (
+        "status-good"
+        if max_fd < 0.5
+        else "status-warn"
+        if max_fd < 1.0
+        else "status-fail"
+    )
+    censor_status = (
+        "status-good"
+        if censored == 0
+        else "status-warn"
+        if censored < len(rows) * 0.1
+        else "status-fail"
+    )
+
+    html = f"""
+    <div class="metric-grid">
+        <div class="metric-box">
+            <div class="metric-label">Max FD</div>
+            <div class="metric-value {fd_status}">{max_fd:.3f} mm</div>
+        </div>
+        <div class="metric-box">
+            <div class="metric-label">Mean FD</div>
+            <div class="metric-value">{mean_fd:.3f} mm</div>
+        </div>
+        <div class="metric-box">
+            <div class="metric-label">Max DVARS</div>
+            <div class="metric-value">{max_dvars:.3f}</div>
+        </div>
+        <div class="metric-box">
+            <div class="metric-label">Mean DVARS</div>
+            <div class="metric-value">{mean_dvars:.3f}</div>
+        </div>
+        <div class="metric-box">
+            <div class="metric-label">Censored Volumes</div>
+            <div class="metric-value {censor_status}">{censored} / {len(rows)}</div>
+        </div>
+    </div>
+    """
+
+    return html
+
+
+def build_acompcor_table_html(rows: list[dict[str, Any]], cols: list[str]) -> str:
+    """Build aCompCor summary table HTML."""
+    acompcor_cols = [c for c in cols if c.startswith("acompcor")]
+
+    if not acompcor_cols or not rows:
+        return "<p>No aCompCor components found.</p>"
+
+    # Show first 10 timepoints
+    html = ["<table>"]
+    html.append("<tr><th>Timepoint</th>")
+    for col in acompcor_cols:
+        html.append(f"<th>{col}</th>")
+    html.append("</tr>")
+
+    for i, row in enumerate(rows[:10]):
+        html.append(f"<tr><td>{i}</td>")
+        for col in acompcor_cols:
+            val = row.get(col, "N/A")
+            if val != "N/A":
+                try:
+                    val = f"{float(val):.4f}"
+                except ValueError:
+                    pass
+            html.append(f"<td>{val}</td>")
+        html.append("</tr>")
+
+    if len(rows) > 10:
+        html.append(
+            f"<tr><td colspan='{len(acompcor_cols) + 1}' style='text-align:center; color:#999;'>... ({len(rows) - 10} more timepoints)</td></tr>"
+        )
+
+    html.append("</table>")
+
+    return "\n".join(html)
+
+
+def build_environment_html(doctor_info: dict[str, Any]) -> str:
+    """Build environment info HTML from doctor.json."""
+    if not doctor_info:
+        return "<p>No environment information available.</p>"
+
+    platform = doctor_info.get("platform", {})
+    deps = doctor_info.get("deps", {})
+    status = doctor_info.get("status", "unknown")
+
+    status_class = (
+        f"status-{status}"
+        if status in ["good", "pass"]
+        else "status-warn"
+        if status == "warn"
+        else "status-fail"
+    )
+
+    html = [
+        f"<p><strong>Status:</strong> <span class='{status_class}'>{status.upper()}</span></p>",
+        "<table>",
+        "<tr><th>Component</th><th>Value</th></tr>",
+    ]
+
+    # Platform info
+    if platform:
+        html.append(f"<tr><td>OS</td><td>{platform.get('os', 'unknown')}</td></tr>")
+        html.append(
+            f"<tr><td>Python</td><td>{platform.get('python', 'unknown')}</td></tr>"
+        )
+        html.append(
+            f"<tr><td>CPU Cores</td><td>{platform.get('cpu_count', 'unknown')}</td></tr>"
+        )
+
+    # SCT
+    sct = deps.get("sct", {})
+    if sct.get("found"):
+        html.append(
+            f"<tr><td>SCT</td><td>✓ {sct.get('version', 'unknown')}</td></tr>"
+        )
+    else:
+        html.append("<tr><td>SCT</td><td class='status-fail'>✗ Not found</td></tr>")
+
+    # PAM50
+    pam50 = deps.get("pam50", {})
+    if pam50.get("found"):
+        html.append(f"<tr><td>PAM50</td><td>✓ {pam50.get('path', 'found')}</td></tr>")
+    else:
+        html.append("<tr><td>PAM50</td><td class='status-fail'>✗ Not found</td></tr>")
+
+    html.append("</table>")
+
+    return "\n".join(html)
+
+
+def build_overlays_html(qc_dir: Path) -> str:
+    """Build overlay images HTML with tabs."""
+    # Look for common overlay PNGs
+    overlays = []
+
+    for pattern in [
+        "overlay_epi_t2w.png",
+        "overlay_t2w_template.png",
+        "overlay_epi_template.png",
+        "registration_*.png",
+    ]:
+        for img_path in qc_dir.glob(pattern):
+            overlays.append(
+                {
+                    "name": img_path.stem.replace("overlay_", "")
+                    .replace("_", " → ")
+                    .title(),
+                    "path": img_path.name,
+                }
+            )
+
+    if not overlays:
+        return "<p>No overlay images found. Run registration steps to generate overlays.</p>"
+
+    # Build tabs
+    html = ["<div class='tabs'>"]
+    for i, overlay in enumerate(overlays):
+        tab_id = f"overlay-{i}"
+        html.append(
+            f"<button class='tab' data-tab='{tab_id}' onclick='switchTab(\"{tab_id}\")'>{overlay['name']}</button>"
+        )
+    html.append("</div>")
+
+    # Build tab contents
+    for i, overlay in enumerate(overlays):
+        tab_id = f"overlay-{i}"
+        html.append(f"<div id='{tab_id}' class='tab-content'>")
+        html.append(
+            f"<img src='{overlay['path']}' alt='{overlay['name']}' class='overlay-img' />"
+        )
+        html.append("</div>")
+
+    return "\n".join(html)
+
+
+def write_qc(subject_dir: Path, assets: dict[str, Any]) -> Path:
+    """
+    Write per-subject QC HTML report.
+
+    Args:
+        subject_dir: Subject directory (e.g., out/sub-01)
+        assets: Dictionary with keys:
+            - confounds_tsv: Path to confounds TSV
+            - doctor_json: Path to doctor.json (optional)
+            - motion_png: Path to motion plot PNG (optional)
+
+    Returns:
+        Path to generated index.html
+    """
+    qc_dir = subject_dir / "qc"
+    qc_dir.mkdir(parents=True, exist_ok=True)
+
+    # Load assets
+    confounds_tsv = assets.get("confounds_tsv")
+    doctor_json = assets.get("doctor_json")
+
+    rows, cols = load_confounds(confounds_tsv) if confounds_tsv else ([], [])
+    doctor_info = load_doctor_json(doctor_json)
+
+    subject_id = subject_dir.name
+
+    # Build HTML parts
+    html_parts = [
+        "<!DOCTYPE html>",
+        "<html lang='en'>",
+        "<head>",
+        "  <meta charset='UTF-8'>",
+        "  <meta name='viewport' content='width=device-width, initial-scale=1.0'>",
+        f"  <title>{subject_id} QC Report</title>",
+        f"  <style>{CSS}</style>",
+        "</head>",
+        "<body>",
+        "  <div class='header'>",
+        f"    <h1>{subject_id} Quality Control Report</h1>",
+        f"    <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>",
+        "  </div>",
+    ]
+
+    # Registration overlays section
+    html_parts.extend(
+        [
+            "  <div class='card'>",
+            "    <h2>Registration Overlays</h2>",
+            "    " + build_overlays_html(qc_dir),
+            "  </div>",
+        ]
+    )
+
+    # Motion section
+    html_parts.extend(
+        [
+            "  <div class='card'>",
+            "    <h2>Motion Summary</h2>",
+            "    " + build_motion_summary_html(rows),
+        ]
+    )
+
+    # Add motion plot if available
+    motion_png = qc_dir / "motion_fd_dvars.png"
+    if motion_png.exists():
+        html_parts.append(
+            f"    <img src='motion_fd_dvars.png' alt='Motion FD/DVARS Plot' class='overlay-img' />"
+        )
+
+    html_parts.append("  </div>")
+
+    # aCompCor section
+    html_parts.extend(
+        [
+            "  <div class='card'>",
+            "    <h2>aCompCor Components</h2>",
+            "    " + build_acompcor_table_html(rows, cols),
+            "  </div>",
+        ]
+    )
+
+    # Environment section
+    html_parts.extend(
+        [
+            "  <div class='card'>",
+            "    <h2>Environment</h2>",
+            "    " + build_environment_html(doctor_info),
+            "  </div>",
+        ]
+    )
+
+    # Footer
+    html_parts.extend(
+        [
+            "  <div class='footer'>",
+            "    <p>Generated by SpinePrep</p>",
+            "  </div>",
+            f"  <script>{TABS_JS}</script>",
+            "</body>",
+            "</html>",
+        ]
+    )
+
+    # Write HTML
+    html_path = qc_dir / "index.html"
+    html_path.write_text("\n".join(html_parts))
+
+    return html_path
diff --git a/spineprep/registration/__init__.py b/spineprep/registration/__init__.py
new file mode 100644
index 00000000..2a1a14f1
--- /dev/null
+++ b/spineprep/registration/__init__.py
@@ -0,0 +1,31 @@
+"""Registration utilities for SpinePrep (EPI→PAM50)."""
+
+from .header import (
+    check_affines_match,
+    check_form_codes,
+    check_header,
+    validate_registration_output,
+)
+from .metrics import compute_psnr, compute_ssim, normalize_intensity
+from .sct import (
+    build_sct_register_cmd,
+    check_doctor_status,
+    create_output_paths,
+    map_sct_error,
+    run_sct_register,
+)
+
+__all__ = [
+    "check_affines_match",
+    "check_form_codes",
+    "check_header",
+    "validate_registration_output",
+    "compute_psnr",
+    "compute_ssim",
+    "normalize_intensity",
+    "build_sct_register_cmd",
+    "check_doctor_status",
+    "create_output_paths",
+    "map_sct_error",
+    "run_sct_register",
+]
diff --git a/spineprep/registration/header.py b/spineprep/registration/header.py
new file mode 100644
index 00000000..6810c3bc
--- /dev/null
+++ b/spineprep/registration/header.py
@@ -0,0 +1,156 @@
+"""NIfTI header validation and sanity checks for registration outputs."""
+
+from __future__ import annotations
+
+from typing import Any
+
+import nibabel as nib
+import numpy as np
+
+
+def check_header(img: nib.Nifti1Image) -> dict[str, Any]:
+    """
+    Extract and validate basic header information from a NIfTI image.
+
+    Args:
+        img: NIfTI image object
+
+    Returns:
+        Dictionary with header information:
+        - valid: bool indicating overall validity
+        - shape: list of dimensions
+        - zooms: list of voxel sizes
+        - sform_code: int
+        - qform_code: int
+        - dtype: data type string
+    """
+    header = img.header
+    shape = list(img.shape)
+    zooms = list(img.header.get_zooms()[:3])  # Get first 3 dimensions (x, y, z)
+
+    sform_code = int(header["sform_code"])
+    qform_code = int(header["qform_code"])
+
+    return {
+        "valid": True,
+        "shape": shape,
+        "zooms": zooms,
+        "sform_code": sform_code,
+        "qform_code": qform_code,
+        "dtype": str(img.get_data_dtype()),
+    }
+
+
+def check_affines_match(
+    affine1: np.ndarray,
+    affine2: np.ndarray,
+    atol: float = 1e-6,
+) -> bool:
+    """
+    Check if two affine matrices match within tolerance.
+
+    Args:
+        affine1: First affine matrix (4x4)
+        affine2: Second affine matrix (4x4)
+        atol: Absolute tolerance for comparison
+
+    Returns:
+        True if affines match within tolerance
+    """
+    return bool(np.allclose(affine1, affine2, atol=atol, rtol=0))
+
+
+def check_form_codes(img: nib.Nifti1Image) -> dict[str, Any]:
+    """
+    Check NIfTI form codes (sform/qform) and their consistency.
+
+    Args:
+        img: NIfTI image object
+
+    Returns:
+        Dictionary with:
+        - sform_code: int
+        - qform_code: int
+        - forms_match: bool (True if sform and qform are consistent)
+        - has_unknown_codes: bool (True if either code is 0/unknown)
+    """
+    header = img.header
+    sform_code = int(header["sform_code"])
+    qform_code = int(header["qform_code"])
+
+    # Get the actual affines
+    sform = header.get_sform()
+    qform = header.get_qform()
+
+    # Check if forms match (both codes and affines should match)
+    codes_match = sform_code == qform_code
+    affines_match = check_affines_match(sform, qform, atol=1e-5)
+    forms_match = codes_match and affines_match
+
+    # Check for unknown codes
+    has_unknown = sform_code == 0 or qform_code == 0
+
+    return {
+        "sform_code": sform_code,
+        "qform_code": qform_code,
+        "forms_match": forms_match,
+        "has_unknown_codes": has_unknown,
+    }
+
+
+def validate_registration_output(
+    img: nib.Nifti1Image,
+    expected_zooms: tuple[float, float, float] | None = None,
+    expected_shape: tuple[int, ...] | None = None,
+) -> dict[str, Any]:
+    """
+    Validate a registration output image meets expected criteria.
+
+    Args:
+        img: Registered NIfTI image
+        expected_zooms: Expected voxel sizes (x, y, z)
+        expected_shape: Expected image dimensions
+
+    Returns:
+        Dictionary with validation results:
+        - valid: bool (overall pass/fail)
+        - header: dict from check_header
+        - forms: dict from check_form_codes
+        - zooms_match: bool (if expected_zooms provided)
+        - shape_match: bool (if expected_shape provided)
+    """
+    header_info = check_header(img)
+    form_info = check_form_codes(img)
+
+    result: dict[str, Any] = {
+        "valid": True,
+        "header": header_info,
+        "forms": form_info,
+    }
+
+    # Check zooms if expected
+    if expected_zooms is not None:
+        actual_zooms = header_info["zooms"]
+        zooms_match = all(
+            abs(a - e) < 1e-3 for a, e in zip(actual_zooms, expected_zooms)
+        )
+        result["zooms_match"] = zooms_match
+        if not zooms_match:
+            result["valid"] = False
+
+    # Check shape if expected
+    if expected_shape is not None:
+        actual_shape = tuple(header_info["shape"])
+        shape_match = actual_shape == expected_shape
+        result["shape_match"] = shape_match
+        if not shape_match:
+            result["valid"] = False
+
+    # Flag if forms don't match or have unknown codes
+    if not form_info["forms_match"]:
+        result["warning"] = "sform and qform do not match"
+
+    if form_info["has_unknown_codes"]:
+        result["warning"] = "Image has unknown form codes (0)"
+
+    return result
diff --git a/spineprep/registration/metrics.py b/spineprep/registration/metrics.py
new file mode 100644
index 00000000..932058e9
--- /dev/null
+++ b/spineprep/registration/metrics.py
@@ -0,0 +1,134 @@
+"""Image quality metrics for registration (SSIM/PSNR) with normalization."""
+
+from __future__ import annotations
+
+import numpy as np
+from skimage.metrics import peak_signal_noise_ratio, structural_similarity
+
+
+def normalize_intensity(
+    img: np.ndarray,
+    mask: np.ndarray | None = None,
+    clip_percentiles: tuple[float, float] | None = None,
+) -> np.ndarray:
+    """
+    Normalize image intensity to zero mean and unit variance.
+
+    Args:
+        img: Input image array
+        mask: Optional binary mask to constrain normalization region
+        clip_percentiles: Optional (low, high) percentiles for clipping before normalization
+
+    Returns:
+        Normalized image array (z-scored)
+    """
+    img_copy = img.copy().astype(np.float64)
+
+    # Remove NaNs for computation
+    valid_mask = np.isfinite(img_copy)
+    if mask is not None:
+        valid_mask = valid_mask & mask
+
+    if not np.any(valid_mask):
+        return img_copy
+
+    # Clip outliers if requested
+    if clip_percentiles is not None:
+        low, high = clip_percentiles
+        p_low = np.percentile(img_copy[valid_mask], low)
+        p_high = np.percentile(img_copy[valid_mask], high)
+        img_copy = np.clip(img_copy, p_low, p_high)
+
+    # Compute mean and std from valid/masked region
+    mean_val = np.mean(img_copy[valid_mask])
+    std_val = np.std(img_copy[valid_mask])
+
+    if std_val < 1e-10:  # Avoid division by zero
+        return img_copy - mean_val
+
+    # Z-score normalization
+    normalized = (img_copy - mean_val) / std_val
+
+    # Preserve NaNs in original positions
+    normalized[~np.isfinite(img)] = np.nan
+
+    return normalized.astype(img.dtype)
+
+
+def compute_ssim(
+    img1: np.ndarray,
+    img2: np.ndarray,
+    mask: np.ndarray | None = None,
+) -> float:
+    """
+    Compute Structural Similarity Index (SSIM) between two images.
+
+    For 3D volumes, computes slice-wise SSIM and returns the mean.
+
+    Args:
+        img1: First image array
+        img2: Second image array
+        mask: Optional mask to constrain comparison region
+
+    Returns:
+        SSIM value (0-1, higher is better)
+
+    Raises:
+        ValueError: If image shapes don't match
+    """
+    if img1.shape != img2.shape:
+        raise ValueError(f"Shape mismatch: img1={img1.shape}, img2={img2.shape}")
+
+    # Handle 3D volumes (compute slice-wise and average)
+    if len(img1.shape) == 3:
+        ssim_vals = []
+        for z in range(img1.shape[2]):
+            slice1 = img1[:, :, z]
+            slice2 = img2[:, :, z]
+
+            # Skip empty slices
+            if slice1.max() == 0 or slice2.max() == 0:
+                continue
+
+            # Compute SSIM for this slice
+            data_range = max(slice1.max() - slice1.min(), slice2.max() - slice2.min())
+            if data_range > 0:
+                ssim = structural_similarity(slice1, slice2, data_range=data_range)
+                ssim_vals.append(ssim)
+
+        return float(np.mean(ssim_vals)) if ssim_vals else 0.0
+
+    # For 2D images
+    data_range = max(img1.max() - img1.min(), img2.max() - img2.min())
+    if data_range == 0:
+        return 1.0 if np.allclose(img1, img2) else 0.0
+
+    return float(structural_similarity(img1, img2, data_range=data_range))
+
+
+def compute_psnr(
+    img1: np.ndarray,
+    img2: np.ndarray,
+) -> float:
+    """
+    Compute Peak Signal-to-Noise Ratio (PSNR) between two images.
+
+    Args:
+        img1: First image array
+        img2: Second image array
+
+    Returns:
+        PSNR value in dB (higher is better, typically 20-50 dB)
+
+    Raises:
+        ValueError: If image shapes don't match
+    """
+    if img1.shape != img2.shape:
+        raise ValueError(f"Shape mismatch: img1={img1.shape}, img2={img2.shape}")
+
+    data_range = max(img1.max() - img1.min(), img2.max() - img2.min())
+    if data_range == 0:
+        # Identical constant images
+        return np.inf
+
+    return float(peak_signal_noise_ratio(img1, img2, data_range=data_range))
diff --git a/spineprep/registration/sct.py b/spineprep/registration/sct.py
new file mode 100644
index 00000000..25429ac5
--- /dev/null
+++ b/spineprep/registration/sct.py
@@ -0,0 +1,273 @@
+"""Spinal Cord Toolbox (SCT) wrapper for EPI→PAM50 registration."""
+
+from __future__ import annotations
+
+import json
+import subprocess
+from pathlib import Path
+from typing import Any
+
+
+def build_sct_register_cmd(
+    src_image: str,
+    dest_image: str,
+    out_warp: str,
+    out_resampled: str | None = None,
+    param: str | None = None,
+) -> list[str]:
+    """
+    Build sct_register_multimodal command.
+
+    Args:
+        src_image: Source image path (e.g., EPI reference)
+        dest_image: Destination/template image path (e.g., PAM50)
+        out_warp: Output warp field path
+        out_resampled: Output resampled image path (optional)
+        param: Registration parameters (default: rigid+affine)
+
+    Returns:
+        Command list for subprocess.run()
+    """
+    if param is None:
+        # Conservative defaults: rigid + affine only (no deformable)
+        param = "step=1,type=im,algo=rigid:step=2,type=im,algo=affine"
+
+    cmd = [
+        "sct_register_multimodal",
+        "-i",
+        src_image,
+        "-d",
+        dest_image,
+        "-owarp",
+        out_warp,
+        "-param",
+        param,
+    ]
+
+    if out_resampled:
+        cmd.extend(["-o", out_resampled])
+
+    return cmd
+
+
+def run_sct_register(
+    src_image: str,
+    dest_image: str,
+    out_warp: str,
+    out_resampled: str | None = None,
+    param: str | None = None,
+) -> dict[str, Any]:
+    """
+    Run SCT registration with error handling.
+
+    Args:
+        src_image: Source image path
+        dest_image: Destination image path
+        out_warp: Output warp path
+        out_resampled: Output resampled image path (optional)
+        param: Custom registration parameters (optional)
+
+    Returns:
+        Dictionary with:
+        - success: bool
+        - return_code: int
+        - stdout: str
+        - stderr: str
+        - sct_version: str
+        - command: str
+    """
+    # Get SCT version for provenance
+    sct_version = "unknown"
+    try:
+        version_proc = subprocess.run(
+            ["sct_version"],
+            capture_output=True,
+            text=True,
+            timeout=10,
+            check=False,
+        )
+        if version_proc.returncode == 0:
+            # Parse version from output (first line typically)
+            sct_version = version_proc.stdout.strip().split("\n")[0]
+    except Exception:
+        pass
+
+    # Build command
+    cmd = build_sct_register_cmd(src_image, dest_image, out_warp, out_resampled, param)
+
+    # Ensure output directories exist
+    Path(out_warp).parent.mkdir(parents=True, exist_ok=True)
+    if out_resampled:
+        Path(out_resampled).parent.mkdir(parents=True, exist_ok=True)
+
+    # Run registration
+    try:
+        result = subprocess.run(
+            cmd,
+            capture_output=True,
+            text=True,
+            check=False,
+            timeout=600,  # 10 minute timeout
+        )
+
+        return {
+            "success": result.returncode == 0,
+            "return_code": result.returncode,
+            "stdout": result.stdout,
+            "stderr": result.stderr,
+            "sct_version": sct_version,
+            "command": " ".join(cmd),
+        }
+
+    except subprocess.TimeoutExpired:
+        return {
+            "success": False,
+            "return_code": -1,
+            "stdout": "",
+            "stderr": "Registration timed out after 10 minutes",
+            "sct_version": sct_version,
+            "command": " ".join(cmd),
+        }
+    except Exception as e:
+        return {
+            "success": False,
+            "return_code": -1,
+            "stdout": "",
+            "stderr": str(e),
+            "sct_version": sct_version,
+            "command": " ".join(cmd),
+        }
+
+
+def check_doctor_status(doctor_json_path: str) -> None:
+    """
+    Check doctor JSON for SCT and PAM50 availability.
+
+    Args:
+        doctor_json_path: Path to doctor.json file
+
+    Raises:
+        FileNotFoundError: If doctor.json doesn't exist
+        RuntimeError: If required checks are not green
+    """
+    doctor_path = Path(doctor_json_path)
+    if not doctor_path.exists():
+        raise FileNotFoundError(
+            f"Doctor report not found at {doctor_json_path}. "
+            "Run 'spineprep doctor' first to verify SCT installation."
+        )
+
+    try:
+        with open(doctor_path) as f:
+            doctor_data = json.load(f)
+    except json.JSONDecodeError as e:
+        raise RuntimeError(f"Invalid doctor JSON: {e}") from e
+
+    checks = doctor_data.get("checks", {})
+
+    # Check for SCT-related statuses
+    required_checks = ["sct_installed", "pam50_available"]
+    failed = []
+
+    for check_name in required_checks:
+        if check_name in checks:
+            status = checks[check_name].get("status", "unknown")
+            if status != "green":
+                msg = checks[check_name].get("message", "No details")
+                failed.append(f"{check_name}: {status} ({msg})")
+
+    if failed:
+        raise RuntimeError(
+            "Doctor checks failed for SCT/PAM50 (red status):\n"
+            + "\n".join(f"  - {f}" for f in failed)
+            + "\n\nRun 'spineprep doctor' to diagnose and fix."
+        )
+
+
+def map_sct_error(return_code: int, stderr: str) -> str:
+    """
+    Map SCT error to actionable message.
+
+    Args:
+        return_code: Process return code
+        stderr: Standard error output
+
+    Returns:
+        Actionable error message
+    """
+    stderr_lower = stderr.lower()
+
+    if "command not found" in stderr_lower or "no such file" in stderr_lower:
+        return (
+            "SCT command not found. Please ensure Spinal Cord Toolbox is installed "
+            "and available in PATH. See: https://spinalcordtoolbox.com/user_section/installation.html"
+        )
+
+    if "cannot open" in stderr_lower or "does not exist" in stderr_lower:
+        return (
+            "SCT cannot open input file. Check that image paths are correct "
+            "and files exist."
+        )
+
+    if "template" in stderr_lower or "pam50" in stderr_lower:
+        return (
+            "SCT cannot find PAM50 template. Ensure SCT_DIR environment variable "
+            "is set and templates are installed."
+        )
+
+    if "registration failed" in stderr_lower or "convergence" in stderr_lower:
+        return (
+            "Registration did not converge. Input images may have poor contrast, "
+            "misaligned orientations, or insufficient overlap."
+        )
+
+    # Generic error
+    return f"SCT registration failed (exit {return_code}): {stderr[:200]}"
+
+
+def create_output_paths(
+    subject: str,
+    task: str,
+    out_dir: str,
+    session: str | None = None,
+) -> dict[str, str]:
+    """
+    Create BIDS-compliant output paths for registration derivatives.
+
+    Args:
+        subject: Subject ID (e.g., 'sub-01')
+        task: Task name (e.g., 'rest')
+        out_dir: Output derivatives directory
+        session: Optional session ID (e.g., 'ses-01')
+
+    Returns:
+        Dictionary of output paths:
+        - warp: transformation file
+        - registered: registered BOLD reference in PAM50 space
+        - metrics: metrics JSON
+        - mosaic_before: before mosaic PNG
+        - mosaic_after: after mosaic PNG
+    """
+    # Build base path
+    base = Path(out_dir) / "spineprep" / subject
+    if session:
+        base = base / session
+
+    # Construct BIDS entities
+    entities = subject
+    if session:
+        entities += f"_{session}"
+    entities += f"_task-{task}"
+
+    # Output paths
+    xfm_dir = base / "xfm"
+    func_dir = base / "func"
+    qc_dir = base / "qc" / "registration"
+
+    return {
+        "warp": str(xfm_dir / f"{entities}_from-epi_to-PAM50_desc-sct_xfm.nii.gz"),
+        "registered": str(func_dir / f"{entities}_space-PAM50_desc-ref_bold.nii.gz"),
+        "metrics": str(qc_dir / f"{entities}_desc-registration_metrics.json"),
+        "mosaic_before": str(qc_dir / f"{entities}_desc-before_mosaic.png"),
+        "mosaic_after": str(qc_dir / f"{entities}_desc-after_mosaic.png"),
+    }
diff --git a/tests/confounds/__init__.py b/tests/confounds/__init__.py
new file mode 100644
index 00000000..d7d7fa6c
--- /dev/null
+++ b/tests/confounds/__init__.py
@@ -0,0 +1 @@
+"""Tests for confounds module."""
diff --git a/tests/confounds/__pycache__/__init__.cpython-312.pyc b/tests/confounds/__pycache__/__init__.cpython-312.pyc
new file mode 100644
index 00000000..f33dca8e
Binary files /dev/null and b/tests/confounds/__pycache__/__init__.cpython-312.pyc differ
diff --git a/tests/confounds/__pycache__/test_motion.cpython-312-pytest-8.4.2.pyc b/tests/confounds/__pycache__/test_motion.cpython-312-pytest-8.4.2.pyc
new file mode 100644
index 00000000..ffac55f0
Binary files /dev/null and b/tests/confounds/__pycache__/test_motion.cpython-312-pytest-8.4.2.pyc differ
diff --git a/tests/confounds/test_motion.py b/tests/confounds/test_motion.py
new file mode 100644
index 00000000..7a8102ff
--- /dev/null
+++ b/tests/confounds/test_motion.py
@@ -0,0 +1,263 @@
+"""Tests for confounds motion metrics (FD and DVARS)."""
+
+from __future__ import annotations
+
+import json
+from pathlib import Path
+
+import nibabel as nib
+import numpy as np
+import pytest
+
+from spineprep.confounds.io import write_confounds_tsv_json
+from spineprep.confounds.motion import compute_dvars, compute_fd_power
+from spineprep.confounds.plot import plot_motion_png
+
+
+def test_fd_power_exactness():
+    """Test FD computation (Power) with known synthetic motion parameters."""
+    # Create synthetic motion: 10 timepoints, 6 parameters
+    # Known deltas to produce predictable FD
+    motion = np.array(
+        [
+            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # t=0
+            [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # t=1: delta_x=1mm → FD=1.0
+            [1.0, 2.0, 0.0, 0.0, 0.0, 0.0],  # t=2: delta_y=2mm → FD=2.0
+            [1.0, 2.0, 0.0, 0.01, 0.0, 0.0],  # t=3: delta_rot_x=0.01rad → FD=0.5
+            [1.0, 2.0, 0.0, 0.01, 0.0, 0.0],  # t=4: no change → FD=0.0
+            [2.0, 3.0, 1.0, 0.02, 0.01, 0.0],  # t=5: complex
+            [2.0, 3.0, 1.0, 0.02, 0.01, 0.0],  # t=6: no change → FD=0.0
+            [2.0, 3.0, 1.0, 0.02, 0.01, 0.0],  # t=7: no change → FD=0.0
+            [3.0, 3.0, 1.0, 0.02, 0.01, 0.0],  # t=8: delta_x=1mm → FD=1.0
+            [3.0, 3.0, 2.0, 0.02, 0.01, 0.0],  # t=9: delta_z=1mm → FD=1.0
+        ],
+        dtype=np.float32,
+    )
+
+    fd = compute_fd_power(motion, radius_mm=50.0)
+
+    # Expected FD values (Power: sum of absolute differences)
+    expected = np.array(
+        [
+            0.0,  # t=0 (always 0)
+            1.0,  # t=1: |1-0| = 1.0
+            2.0,  # t=2: |2-0| = 2.0
+            0.5,  # t=3: 50mm * 0.01rad = 0.5mm
+            0.0,  # t=4: no change
+            4.0,  # t=5: |1|+|1|+|1| + 50*(|0.01|+|0.01|+|0|) = 3 + 1.0 = 4.0
+            0.0,  # t=6
+            0.0,  # t=7
+            1.0,  # t=8
+            1.0,  # t=9
+        ],
+        dtype=np.float32,
+    )
+
+    # Assert exactness within tolerance 1e-6
+    np.testing.assert_allclose(fd, expected, atol=1e-6)
+    assert fd.dtype == np.float32
+
+
+def test_fd_power_determinism():
+    """Test that FD computation is deterministic."""
+    motion = np.random.RandomState(42).randn(20, 6).astype(np.float32) * 0.5
+
+    fd1 = compute_fd_power(motion)
+    fd2 = compute_fd_power(motion)
+
+    np.testing.assert_array_equal(fd1, fd2)
+
+
+def test_dvars_determinism():
+    """Test DVARS with synthetic 4D volume for determinism."""
+    # Create small synthetic 4D array with fixed seed
+    np.random.seed(42)
+    data = np.random.randn(4, 4, 4, 10).astype(np.float32)
+
+    # Insert a known step change at t=5
+    data[:, :, :, 5:] += 10.0
+
+    dvars = compute_dvars(data)
+
+    # DVARS[0] should be 0
+    assert dvars[0] == 0.0
+    assert dvars.dtype == np.float32
+
+    # DVARS[5] should be large (step change)
+    assert dvars[5] > dvars[4]
+    assert dvars[5] > 1.0  # Significant change
+
+    # Later timepoints should have smaller DVARS (noise only)
+    assert dvars[6] < dvars[5]
+
+    # Test determinism
+    dvars2 = compute_dvars(data)
+    np.testing.assert_array_equal(dvars, dvars2)
+
+
+def test_dvars_with_mask():
+    """Test DVARS with explicit mask."""
+    np.random.seed(42)
+    data = np.random.randn(8, 8, 4, 10).astype(np.float32)
+
+    # Create a mask
+    mask = np.zeros((8, 8, 4), dtype=bool)
+    mask[2:6, 2:6, :] = True
+
+    dvars = compute_dvars(data, mask=mask)
+
+    assert len(dvars) == 10
+    assert dvars[0] == 0.0
+    assert dvars.dtype == np.float32
+
+
+def test_confounds_tsv_json_schema(tmp_path: Path):
+    """Test that confounds TSV and JSON match schema v1."""
+    # Create synthetic motion params
+    motion = np.array(
+        [
+            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
+            [0.1, 0.0, 0.0, 0.0, 0.0, 0.0],
+            [0.1, 0.2, 0.0, 0.001, 0.0, 0.0],
+        ],
+        dtype=np.float32,
+    )
+
+    fd = compute_fd_power(motion, radius_mm=50.0)
+    dvars = np.array([0.0, 1.2, 0.8], dtype=np.float32)
+
+    # Write TSV and JSON
+    tsv_path = tmp_path / "sub-01_task-rest_desc-confounds_timeseries.tsv"
+    json_path = tmp_path / "sub-01_task-rest_desc-confounds_timeseries.json"
+
+    write_confounds_tsv_json(
+        tsv_path=tsv_path,
+        json_path=json_path,
+        motion_params=motion,
+        fd=fd,
+        dvars=dvars,
+    )
+
+    # Verify TSV exists and has correct structure
+    assert tsv_path.exists()
+    assert json_path.exists()
+
+    # Read TSV
+    import csv
+
+    with tsv_path.open() as f:
+        reader = csv.DictReader(f, delimiter="\t")
+        rows = list(reader)
+        cols = reader.fieldnames
+
+    # Verify column order and names
+    expected_cols = [
+        "trans_x",
+        "trans_y",
+        "trans_z",
+        "rot_x",
+        "rot_y",
+        "rot_z",
+        "fd",
+        "dvars",
+    ]
+    assert cols == expected_cols
+
+    # Verify row count
+    assert len(rows) == 3
+
+    # Verify first row values
+    assert float(rows[0]["trans_x"]) == 0.0
+    assert float(rows[0]["fd"]) == 0.0
+    assert float(rows[0]["dvars"]) == 0.0
+
+    # Verify second row values
+    assert float(rows[1]["trans_x"]) == pytest.approx(0.1, abs=1e-6)
+    assert float(rows[1]["fd"]) == pytest.approx(0.1, abs=1e-6)
+
+    # Read JSON and verify schema v1 compliance
+    with json_path.open() as f:
+        meta = json.load(f)
+
+    # Verify required top-level keys
+    assert "SpinePrepSchemaVersion" in meta
+    assert meta["SpinePrepSchemaVersion"] == "1.0"
+
+    # Verify column descriptions exist
+    for col in expected_cols:
+        assert col in meta, f"Column '{col}' missing from JSON sidecar"
+        col_meta = meta[col]
+        assert "LongName" in col_meta
+        assert "Description" in col_meta
+        assert "Units" in col_meta
+
+    # Verify FD metadata
+    fd_meta = meta["fd"]
+    assert "Power" in fd_meta["Method"]
+    assert "mm" in fd_meta["Units"]
+    assert "SoftwareName" in meta
+    assert "SoftwareVersion" in meta
+
+
+def test_plot_motion_png(tmp_path: Path):
+    """Test that motion plot PNG is created."""
+    # Synthetic data
+    motion = np.random.RandomState(42).randn(20, 6).astype(np.float32) * 0.5
+    fd = compute_fd_power(motion)
+    dvars = np.abs(np.random.RandomState(43).randn(20).astype(np.float32)) * 2.0
+    dvars[0] = 0.0
+
+    png_path = tmp_path / "sub-01_task-rest_motion.png"
+
+    plot_motion_png(
+        png_path=png_path,
+        motion_params=motion,
+        fd=fd,
+        dvars=dvars,
+    )
+
+    # Verify PNG exists
+    assert png_path.exists()
+    assert png_path.stat().st_size > 1000  # Should be a real PNG
+
+
+def test_full_pipeline_io(tmp_path: Path):
+    """Test full pipeline: compute metrics, write TSV/JSON, plot."""
+    # Create synthetic 4D NIfTI
+    np.random.seed(100)
+    data = np.random.randn(4, 4, 4, 15).astype(np.float32) + 100.0
+    img = nib.Nifti1Image(data, affine=np.eye(4))
+    nifti_path = tmp_path / "sub-01_task-rest_bold.nii.gz"
+    nib.save(img, nifti_path)
+
+    # Synthetic motion params
+    motion = np.random.RandomState(200).randn(15, 6).astype(np.float32) * 0.3
+
+    # Compute metrics
+    fd = compute_fd_power(motion, radius_mm=50.0)
+    dvars = compute_dvars(data)
+
+    # Write outputs
+    tsv_path = tmp_path / "sub-01_task-rest_desc-confounds_timeseries.tsv"
+    json_path = tmp_path / "sub-01_task-rest_desc-confounds_timeseries.json"
+    png_path = tmp_path / "sub-01_task-rest_motion.png"
+
+    write_confounds_tsv_json(
+        tsv_path=tsv_path,
+        json_path=json_path,
+        motion_params=motion,
+        fd=fd,
+        dvars=dvars,
+    )
+
+    plot_motion_png(
+        png_path=png_path,
+        motion_params=motion,
+        fd=fd,
+        dvars=dvars,
+    )
+
+    # Verify all outputs exist
+    assert tsv_path.exists()
+    assert json_path.exists()
+    assert png_path.exists()
diff --git a/tests/data/motion_params.tsv b/tests/data/motion_params.tsv
new file mode 100644
index 00000000..7666d8a0
--- /dev/null
+++ b/tests/data/motion_params.tsv
@@ -0,0 +1,11 @@
+trans_x	trans_y	trans_z	rot_x	rot_y	rot_z
+0.0	0.0	0.0	0.0	0.0	0.0
+1.0	0.0	0.0	0.0	0.0	0.0
+1.0	2.0	0.0	0.0	0.0	0.0
+1.0	2.0	0.0	0.01	0.0	0.0
+1.0	2.0	0.0	0.01	0.0	0.0
+2.0	3.0	1.0	0.02	0.01	0.0
+2.0	3.0	1.0	0.02	0.01	0.0
+2.0	3.0	1.0	0.02	0.01	0.0
+3.0	3.0	1.0	0.02	0.01	0.0
+3.0	3.0	2.0	0.02	0.01	0.0
diff --git a/tests/test_config_precedence.py b/tests/test_config_precedence.py
new file mode 100644
index 00000000..ea6f6961
--- /dev/null
+++ b/tests/test_config_precedence.py
@@ -0,0 +1,237 @@
+"""Tests for config precedence: defaults < YAML < CLI."""
+
+from __future__ import annotations
+
+import tempfile
+from pathlib import Path
+
+import pytest
+import yaml
+
+from spineprep.config import load_defaults, resolve_config
+
+
+def test_load_defaults():
+    """Test that defaults can be loaded."""
+    cfg = load_defaults()
+
+    assert cfg is not None
+    assert isinstance(cfg, dict)
+
+    # Should have required top-level keys per ticket
+    assert "motion" in cfg
+    assert "confounds" in cfg
+    assert "registration" in cfg
+    assert "qc" in cfg
+
+
+def test_defaults_have_required_values():
+    """Test that defaults contain all required config values per ticket."""
+    cfg = load_defaults()
+
+    # Motion defaults (per ticket)
+    assert cfg["motion"]["fd"]["threshold"] == 0.5
+    assert cfg["motion"]["dvars"]["threshold"] == 1.5
+    assert cfg["motion"]["highpass"]["hz"] == 0.008
+
+    # Confounds defaults (per ticket)
+    assert cfg["confounds"]["acompcor"]["n_components"] == 6
+    assert cfg["confounds"]["tcompcor"]["n_components"] == 0
+    assert cfg["confounds"]["censor"]["method"] == "fd"
+
+    # Registration defaults (per ticket)
+    assert cfg["registration"]["pam50"]["spacing"] == "0.5mm"
+    assert cfg["registration"]["resample"]["to"] == "native"
+    assert cfg["registration"]["deformable"]["enabled"] is False
+
+    # QC defaults (per ticket)
+    assert cfg["qc"]["ssim"]["min"] == 0.85
+    assert cfg["qc"]["psnr"]["min"] == 20.0
+    assert cfg["qc"]["report"]["embed_assets"] is True
+
+
+def test_yaml_overrides_defaults():
+    """Test that YAML config overrides defaults."""
+    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
+        yaml.dump(
+            {
+                "motion": {
+                    "fd": {"threshold": 0.8},  # Override default 0.5
+                },
+                "qc": {
+                    "ssim": {"min": 0.9},  # Override default 0.85
+                },
+            },
+            f,
+        )
+        yaml_path = Path(f.name)
+
+    try:
+        cfg = resolve_config(
+            bids_root="/tmp/bids",
+            out_dir="/tmp/out",
+            yaml_path=yaml_path,
+        )
+
+        # YAML should override defaults
+        assert cfg["motion"]["fd"]["threshold"] == 0.8
+        assert cfg["qc"]["ssim"]["min"] == 0.9
+
+        # But other defaults should remain
+        assert cfg["motion"]["dvars"]["threshold"] == 1.5
+        assert cfg["confounds"]["acompcor"]["n_components"] == 6
+    finally:
+        yaml_path.unlink(missing_ok=True)
+
+
+def test_cli_overrides_yaml_and_defaults():
+    """Test that CLI overrides trump YAML and defaults."""
+    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
+        yaml.dump(
+            {
+                "motion": {
+                    "fd": {"threshold": 0.8},
+                },
+            },
+            f,
+        )
+        yaml_path = Path(f.name)
+
+    try:
+        cfg = resolve_config(
+            bids_root="/tmp/bids",
+            out_dir="/tmp/out",
+            yaml_path=yaml_path,
+            cli_overrides={
+                "motion": {
+                    "fd": {"threshold": 1.2},  # Override YAML value
+                },
+            },
+        )
+
+        # CLI should override both YAML and defaults
+        assert cfg["motion"]["fd"]["threshold"] == 1.2
+    finally:
+        yaml_path.unlink(missing_ok=True)
+
+
+def test_precedence_order_full():
+    """Test full precedence chain: defaults < YAML < CLI."""
+    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
+        yaml.dump(
+            {
+                "motion": {
+                    "fd": {"threshold": 0.7},
+                    "dvars": {"threshold": 2.0},
+                },
+                "qc": {
+                    "psnr": {"min": 25.0},
+                },
+            },
+            f,
+        )
+        yaml_path = Path(f.name)
+
+    try:
+        cfg = resolve_config(
+            bids_root="/tmp/bids",
+            out_dir="/tmp/out",
+            yaml_path=yaml_path,
+            cli_overrides={
+                "motion": {
+                    "fd": {"threshold": 0.9},  # CLI wins
+                },
+            },
+        )
+
+        # CLI overrides YAML
+        assert cfg["motion"]["fd"]["threshold"] == 0.9
+
+        # YAML overrides defaults
+        assert cfg["motion"]["dvars"]["threshold"] == 2.0
+        assert cfg["qc"]["psnr"]["min"] == 25.0
+
+        # Defaults remain for untouched values
+        assert cfg["confounds"]["acompcor"]["n_components"] == 6
+        assert cfg["registration"]["pam50"]["spacing"] == "0.5mm"
+    finally:
+        yaml_path.unlink(missing_ok=True)
+
+
+def test_bids_root_and_output_dir_injection():
+    """Test that bids_root and output_dir are injected from CLI args."""
+    cfg = resolve_config(
+        bids_root="/path/to/bids",
+        out_dir="/path/to/out",
+        yaml_path=None,
+    )
+
+    assert cfg["bids_root"] == "/path/to/bids"
+    assert cfg["output_dir"] == "/path/to/out"
+
+
+def test_config_validates_against_schema():
+    """Test that resolved config is validated against JSON schema."""
+    # Valid config should pass
+    cfg = resolve_config(
+        bids_root="/tmp/bids",
+        out_dir="/tmp/out",
+        yaml_path=None,
+    )
+
+    # Should not raise ValidationError
+    assert cfg is not None
+
+    # Invalid config should raise ValidationError
+    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
+        yaml.dump(
+            {
+                "motion": {
+                    "fd": {"threshold": "not_a_number"},  # Invalid type
+                },
+            },
+            f,
+        )
+        yaml_path = Path(f.name)
+
+    try:
+        from jsonschema import ValidationError
+
+        with pytest.raises(ValidationError):
+            resolve_config(
+                bids_root="/tmp/bids",
+                out_dir="/tmp/out",
+                yaml_path=yaml_path,
+            )
+    finally:
+        yaml_path.unlink(missing_ok=True)
+
+
+def test_deep_merge_preserves_nested_values():
+    """Test that deep merge preserves nested values not being overridden."""
+    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
+        yaml.dump(
+            {
+                "motion": {
+                    "fd": {"threshold": 0.7},  # Override only fd.threshold
+                    # dvars.threshold should remain from defaults
+                },
+            },
+            f,
+        )
+        yaml_path = Path(f.name)
+
+    try:
+        cfg = resolve_config(
+            bids_root="/tmp/bids",
+            out_dir="/tmp/out",
+            yaml_path=yaml_path,
+        )
+
+        # Overridden value
+        assert cfg["motion"]["fd"]["threshold"] == 0.7
+
+        # Default value should still be there
+        assert cfg["motion"]["dvars"]["threshold"] == 1.5
+    finally:
+        yaml_path.unlink(missing_ok=True)
diff --git a/tests/test_doctor_json.py b/tests/test_doctor_json.py
new file mode 100644
index 00000000..a04b244c
--- /dev/null
+++ b/tests/test_doctor_json.py
@@ -0,0 +1,160 @@
+"""Tests for doctor command JSON output per ticket A1."""
+
+from __future__ import annotations
+
+import json
+import subprocess
+from pathlib import Path
+
+
+def test_doctor_json_valid_structure(tmp_path):
+    """Doctor --json outputs valid JSON with all required fields."""
+    json_out = tmp_path / "doctor.json"
+
+    result = subprocess.run(
+        ["spineprep", "doctor", "--json", str(json_out)],
+        capture_output=True,
+        text=True,
+        check=False,
+    )
+
+    # Should complete (may fail if SCT missing, but should still write JSON)
+    assert result.returncode in [0, 1, 2], f"Unexpected exit code: {result.returncode}"
+
+    # JSON file must exist and be non-empty
+    assert json_out.exists(), "doctor.json not created"
+    assert json_out.stat().st_size > 0, "doctor.json is empty"
+
+    # Must be valid JSON
+    with open(json_out) as f:
+        data = json.load(f)
+
+    # Required fields per ticket A1
+    assert "python" in data, "Missing python field"
+    assert "version" in data["python"], "Missing python.version"
+
+    assert "spineprep" in data, "Missing spineprep field"
+    assert "version" in data["spineprep"], "Missing spineprep.version"
+
+    assert "platform" in data, "Missing platform field"
+
+    assert "packages" in data, "Missing packages field"
+    assert isinstance(data["packages"], dict), "packages should be dict"
+
+    assert "sct" in data, "Missing sct field"
+    assert "present" in data["sct"], "Missing sct.present"
+    assert "version" in data["sct"], "Missing sct.version"
+    assert "path" in data["sct"], "Missing sct.path"
+
+    assert "pam50" in data, "Missing pam50 field"
+    assert "present" in data["pam50"], "Missing pam50.present"
+
+    assert "disk" in data, "Missing disk field"
+    assert "free_bytes" in data["disk"], "Missing disk.free_bytes"
+    assert isinstance(
+        data["disk"]["free_bytes"], (int, float)
+    ), "free_bytes should be numeric"
+
+    assert "cwd_writeable" in data, "Missing cwd_writeable"
+    assert isinstance(data["cwd_writeable"], bool), "cwd_writeable should be boolean"
+
+    assert "tmp_writeable" in data, "Missing tmp_writeable"
+    assert isinstance(data["tmp_writeable"], bool), "tmp_writeable should be boolean"
+
+
+def test_doctor_json_to_stdout(tmp_path):
+    """Doctor --json can output to stdout via redirect."""
+    result = subprocess.run(
+        ["spineprep", "doctor", "--json", "-"],
+        capture_output=True,
+        text=True,
+        check=False,
+        cwd=tmp_path,
+    )
+
+    # Should complete
+    assert result.returncode in [0, 1, 2], f"Unexpected exit code: {result.returncode}"
+
+    # If --json=-, output should go to stdout
+    # For now, just verify it completes without crashing
+    # (implementation may choose to write to stdout or default location)
+
+
+def test_doctor_sct_missing_remediation(tmp_path):
+    """When SCT is missing, doctor exits non-zero and provides remediation."""
+    # This test verifies that when SCT is absent, proper remediation is shown
+    # We simulate missing SCT by removing it from PATH and unsetting SCT_DIR
+    import os
+    import shutil
+    import sys
+
+    import pytest
+
+    spineprep_path = shutil.which("spineprep")
+    if not spineprep_path:
+        pytest.skip("spineprep not found in PATH")
+
+    # Create a temporary environment without SCT
+    env = os.environ.copy()
+    # Remove SCT_DIR to simulate missing SCT
+    env.pop("SCT_DIR", None)
+
+    # Build a minimal PATH that includes Python and spineprep's directory
+    # but explicitly excludes typical SCT locations
+    python_bin = Path(sys.executable).parent
+    spineprep_bin = Path(spineprep_path).parent
+    minimal_path = f"{python_bin}:{spineprep_bin}"
+    env["PATH"] = minimal_path
+
+    json_out = tmp_path / "doctor.json"
+
+    result = subprocess.run(
+        [spineprep_path, "doctor", "--json", str(json_out)],
+        capture_output=True,
+        text=True,
+        check=False,
+        env=env,
+    )
+
+    # Check if the JSON was created and is valid
+    assert json_out.exists(), "JSON output should be created"
+
+    import json
+
+    with open(json_out) as f:
+        data = json.load(f)
+
+    # Verify JSON structure
+    assert "sct" in data, "JSON should have sct field"
+    assert "present" in data["sct"], "sct should have present field"
+
+    # If SCT is not present, verify error handling and remediation
+    if not data["sct"]["present"]:
+        # Exit code should be non-zero
+        assert result.returncode != 0, "doctor should fail when SCT is missing"
+
+        # Check for remediation in notes
+        notes = data.get("notes", [])
+        notes_text = " ".join(notes).lower()
+        remediation_keywords = ["docker", "apptainer", "remediation"]
+        has_remediation = any(kw in notes_text for kw in remediation_keywords)
+        assert has_remediation, f"No remediation text found in notes:\n{notes}"
+    else:
+        # If SCT is still present (couldn't simulate missing),
+        # skip the exit code and remediation checks
+        pytest.skip(
+            "SCT found despite PATH manipulation, cannot test missing SCT scenario"
+        )
+
+
+def test_doctor_help_works():
+    """spineprep doctor --help should work."""
+    result = subprocess.run(
+        ["spineprep", "doctor", "--help"],
+        capture_output=True,
+        text=True,
+        check=False,
+    )
+
+    assert result.returncode == 0, "doctor --help should succeed"
+    assert "doctor" in result.stdout.lower(), "Help text should mention doctor"
diff --git a/tests/test_masking.py b/tests/test_masking.py
new file mode 100644
index 00000000..221dd7db
--- /dev/null
+++ b/tests/test_masking.py
@@ -0,0 +1,328 @@
+"""Tests for spinal cord and CSF masking module."""
+
+from __future__ import annotations
+
+import json
+from pathlib import Path
+from unittest.mock import MagicMock, patch
+
+import nibabel as nib
+import numpy as np
+import pytest
+
+
+@pytest.fixture
+def mock_t2w_image(tmp_path):
+    """Create a mock T2w NIfTI image."""
+    # Create a simple 3D volume (40x40x20)
+    data = np.zeros((40, 40, 20), dtype=np.float32)
+    # Add some signal in the center (spinal cord region)
+    data[15:25, 15:25, 5:15] = 100.0
+
+    affine = np.eye(4)
+    affine[0, 0] = 0.5  # 0.5mm voxel size
+    affine[1, 1] = 0.5
+    affine[2, 2] = 2.5
+
+    img = nib.Nifti1Image(data, affine)
+    img_path = tmp_path / "sub-01_T2w.nii.gz"
+    nib.save(img, img_path)
+    return str(img_path)
+
+
+@pytest.fixture
+def mock_cord_mask(tmp_path):
+    """Create a mock cord mask."""
+    # Cord mask: small region in center
+    data = np.zeros((40, 40, 20), dtype=np.uint8)
+    data[17:23, 17:23, 7:13] = 1
+
+    affine = np.eye(4)
+    affine[0, 0] = 0.5
+    affine[1, 1] = 0.5
+    affine[2, 2] = 2.5
+
+    img = nib.Nifti1Image(data, affine)
+    img_path = tmp_path / "cord_mask.nii.gz"
+    nib.save(img, img_path)
+    return str(img_path)
+
+
+def test_run_sct_deepseg_basic(tmp_path, mock_t2w_image):
+    """Test basic SCT deepseg command execution."""
+    from spineprep._sct import run_sct_deepseg
+
+    out_mask = tmp_path / "cord_seg.nii.gz"
+
+    with patch("subprocess.run") as mock_run:
+        # Mock version check
+        def side_effect(*args, **kwargs):
+            cmd = args[0]
+            if "sct_version" in cmd:
+                result = MagicMock()
+                result.returncode = 0
+                result.stdout = "6.0.0"
+                return result
+            else:
+                # Mock deepseg - create output file
+                result = MagicMock()
+                result.returncode = 0
+                result.stdout = "Segmentation completed"
+                result.stderr = ""
+                # Create mock output
+                mock_data = np.zeros((40, 40, 20), dtype=np.uint8)
+                mock_data[17:23, 17:23, 7:13] = 1
+                nib.save(nib.Nifti1Image(mock_data, np.eye(4)), out_mask)
+                return result
+
+        mock_run.side_effect = side_effect
+
+        result = run_sct_deepseg(
+            t2w_path=mock_t2w_image,
+            out_mask=str(out_mask),
+            contrast="t2",
+        )
+
+        assert result["success"] is True
+        assert result["return_code"] == 0
+        assert "sct_version" in result
+        assert result["sct_version"] == "6.0.0"
+        assert "cmd" in result
+
+
+def test_run_sct_deepseg_failure(tmp_path, mock_t2w_image):
+    """Test SCT deepseg failure handling."""
+    from spineprep._sct import run_sct_deepseg
+
+    out_mask = tmp_path / "cord_seg.nii.gz"
+
+    with patch("subprocess.run") as mock_run:
+        # Mock failure
+        mock_result = MagicMock()
+        mock_result.returncode = 1
+        mock_result.stdout = ""
+        mock_result.stderr = "Error: segmentation failed"
+        mock_run.return_value = mock_result
+
+        result = run_sct_deepseg(
+            t2w_path=mock_t2w_image,
+            out_mask=str(out_mask),
+            contrast="t2",
+        )
+
+        assert result["success"] is False
+        assert result["return_code"] == 1
+
+
+def test_create_csf_ring_mask(mock_cord_mask):
+    """Test CSF ring mask generation from cord mask."""
+    from spineprep.preproc.masking import create_csf_ring_mask
+
+    cord_img = nib.load(mock_cord_mask)
+    csf_img = create_csf_ring_mask(cord_img, inner_dilation=1, outer_dilation=2)
+
+    csf_data = csf_img.get_fdata()
+    cord_data = cord_img.get_fdata()
+
+    # CSF should not overlap with cord
+    assert np.sum(csf_data * cord_data) == 0
+
+    # CSF should have some voxels
+    assert np.sum(csf_data) > 0
+
+    # CSF should be around the cord (check a few voxels)
+    # This is a simple sanity check
+    assert csf_data.max() == 1
+
+
+def test_create_qc_overlay(tmp_path, mock_t2w_image, mock_cord_mask):
+    """Test QC overlay PNG generation."""
+    from spineprep.preproc.masking import create_qc_overlay
+
+    out_png = tmp_path / "overlay.png"
+
+    create_qc_overlay(
+        t2w_path=mock_t2w_image,
+        cord_mask_path=mock_cord_mask,
+        csf_mask_path=None,
+        out_png=str(out_png),
+    )
+
+    # Check PNG was created
+    assert out_png.exists()
+    assert out_png.stat().st_size > 0
+
+
+def test_save_provenance_json(tmp_path):
+    """Test provenance JSON creation."""
+    from spineprep.preproc.masking import save_provenance_json
+
+    out_json = tmp_path / "provenance.json"
+
+    save_provenance_json(
+        out_path=str(out_json),
+        cmd="sct_deepseg_sc -i input.nii.gz -c t2 -o output.nii.gz",
+        sct_version="6.0.0",
+        inputs={"t2w": "/path/to/t2w.nii.gz"},
+    )
+
+    assert out_json.exists()
+
+    with open(out_json) as f:
+        data = json.load(f)
+
+    assert "cmd" in data
+    assert data["cmd"] == "sct_deepseg_sc -i input.nii.gz -c t2 -o output.nii.gz"
+    assert "sct_version" in data
+    assert data["sct_version"] == "6.0.0"
+    assert "time_utc" in data
+    assert "inputs" in data
+    assert data["inputs"]["t2w"] == "/path/to/t2w.nii.gz"
+
+
+def test_run_masks_full_workflow(tmp_path, mock_t2w_image):
+    """Test full masking workflow."""
+    from spineprep.preproc.masking import run_masks
+
+    out_dir = tmp_path / "derivatives" / "spineprep" / "sub-01" / "anat"
+    out_dir.mkdir(parents=True, exist_ok=True)
+
+    qc_dir = tmp_path / "derivatives" / "spineprep" / "sub-01" / "qc"
+    qc_dir.mkdir(parents=True, exist_ok=True)
+
+    with patch("spineprep.preproc.masking.run_sct_deepseg") as mock_deepseg:
+        # Mock successful deepseg
+        def mock_deepseg_fn(t2w_path, out_mask, contrast):
+            # Create mock cord mask
+            mock_data = np.zeros((40, 40, 20), dtype=np.uint8)
+            mock_data[17:23, 17:23, 7:13] = 1
+            nib.save(nib.Nifti1Image(mock_data, np.eye(4)), out_mask)
+
+            return {
+                "success": True,
+                "return_code": 0,
+                "stdout": "Segmentation completed",
+                "stderr": "",
+                "sct_version": "6.0.0",
+                "cmd": f"sct_deepseg_sc -i {t2w_path} -c {contrast} -o {out_mask}",
+            }
+
+        mock_deepseg.side_effect = mock_deepseg_fn
+
+        outputs = run_masks(
+            t2w_path=mock_t2w_image,
+            subject="sub-01",
+            out_root=str(tmp_path / "derivatives" / "spineprep"),
+            make_csf=True,
+            qc=True,
+        )
+
+        # Check outputs
+        assert "cord_mask" in outputs
+        assert "csf_mask" in outputs
+        assert "cord_json" in outputs
+        assert "qc_png" in outputs
+
+        # Check files exist
+        assert Path(outputs["cord_mask"]).exists()
+        assert Path(outputs["csf_mask"]).exists()
+        assert Path(outputs["cord_json"]).exists()
+        assert Path(outputs["qc_png"]).exists()
+
+        # Check JSON content
+        with open(outputs["cord_json"]) as f:
+            prov = json.load(f)
+        assert "cmd" in prov
+        assert "sct_version" in prov
+        assert prov["sct_version"] == "6.0.0"
+
+
+def test_run_masks_no_csf(tmp_path, mock_t2w_image):
+    """Test masking workflow without CSF mask."""
+    from spineprep.preproc.masking import run_masks
+
+    with patch("spineprep.preproc.masking.run_sct_deepseg") as mock_deepseg:
+
+        def mock_deepseg_fn(t2w_path, out_mask, contrast):
+            mock_data = np.zeros((40, 40, 20), dtype=np.uint8)
+            mock_data[17:23, 17:23, 7:13] = 1
+            nib.save(nib.Nifti1Image(mock_data, np.eye(4)), out_mask)
+
+            return {
+                "success": True,
+                "return_code": 0,
+                "stdout": "Segmentation completed",
+                "stderr": "",
+                "sct_version": "6.0.0",
+                "cmd": f"sct_deepseg_sc -i {t2w_path} -c {contrast} -o {out_mask}",
+            }
+
+        mock_deepseg.side_effect = mock_deepseg_fn
+
+        outputs = run_masks(
+            t2w_path=mock_t2w_image,
+            subject="sub-01",
+            out_root=str(tmp_path / "derivatives" / "spineprep"),
+            make_csf=False,
+            qc=True,
+        )
+
+        # Check that CSF mask was not created
+        assert "csf_mask" not in outputs or outputs["csf_mask"] is None
+
+
+def test_run_masks_failure_cleanup(tmp_path, mock_t2w_image):
+    """Test that partial outputs are cleaned up on failure."""
+    from spineprep.preproc.masking import run_masks
+
+    with patch("spineprep.preproc.masking.run_sct_deepseg") as mock_deepseg:
+        # Mock failure
+        mock_deepseg.return_value = {
+            "success": False,
+            "return_code": 1,
+            "stdout": "",
+            "stderr": "Segmentation failed",
+            "sct_version": "6.0.0",
+            "cmd": "sct_deepseg_sc ...",
+        }
+
+        with pytest.raises(RuntimeError, match="SCT deepseg failed"):
+            run_masks(
+                t2w_path=mock_t2w_image,
+                subject="sub-01",
+                out_root=str(tmp_path / "derivatives" / "spineprep"),
+                make_csf=True,
+                qc=True,
+            )
+
+
+def test_run_masks_empty_mask_error(tmp_path, mock_t2w_image):
+    """Test that empty mask raises error."""
+    from spineprep.preproc.masking import run_masks
+
+    with patch("spineprep.preproc.masking.run_sct_deepseg") as mock_deepseg:
+
+        def mock_deepseg_fn(t2w_path, out_mask, contrast):
+            # Create EMPTY mask
+            mock_data = np.zeros((40, 40, 20), dtype=np.uint8)
+            nib.save(nib.Nifti1Image(mock_data, np.eye(4)), out_mask)
+
+            return {
+                "success": True,
+                "return_code": 0,
+                "stdout": "Segmentation completed",
+                "stderr": "",
+                "sct_version": "6.0.0",
+                "cmd": f"sct_deepseg_sc -i {t2w_path} -c {contrast} -o {out_mask}",
+            }
+
+        mock_deepseg.side_effect = mock_deepseg_fn
+
+        with pytest.raises(RuntimeError, match="empty"):
+            run_masks(
+                t2w_path=mock_t2w_image,
+                subject="sub-01",
+                out_root=str(tmp_path / "derivatives" / "spineprep"),
+                make_csf=True,
+                qc=True,
+            )
diff --git a/tests/test_qc_report.py b/tests/test_qc_report.py
new file mode 100644
index 00000000..89da9907
--- /dev/null
+++ b/tests/test_qc_report.py
@@ -0,0 +1,300 @@
+"""Tests for per-subject QC HTML report generation (A7)."""
+
+from __future__ import annotations
+
+import json
+import tempfile
+from pathlib import Path
+
+import pytest
+
+
+@pytest.fixture
+def mock_subject_dir():
+    """Create a mock subject directory with required files."""
+    with tempfile.TemporaryDirectory() as tmpdir:
+        subj_dir = Path(tmpdir) / "sub-01"
+        subj_dir.mkdir(parents=True)
+
+        # Create QC directory
+        qc_dir = subj_dir / "qc"
+        qc_dir.mkdir()
+
+        # Create mock confounds TSV
+        confounds_tsv = (
+            subj_dir / "sub-01_task-rest_run-01_desc-confounds_timeseries.tsv"
+        )
+        confounds_tsv.write_text(
+            "fd\tdvars\tacompcor01\tacompcor02\tacompcor03\tacompcor04\tacompcor05\tacompcor06\tcensor\n"
+            "0.1\t1.0\t0.5\t0.3\t0.2\t0.1\t0.05\t0.02\t0\n"
+            "0.2\t1.2\t0.4\t0.3\t0.2\t0.1\t0.05\t0.02\t0\n"
+            "0.8\t2.5\t0.6\t0.4\t0.3\t0.2\t0.1\t0.05\t1\n"
+        )
+
+        # Create mock doctor.json
+        doctor_json = Path(tmpdir) / "provenance" / "doctor-latest.json"
+        doctor_json.parent.mkdir(parents=True, exist_ok=True)
+        doctor_json.write_text(
+            json.dumps(
+                {
+                    "status": "pass",
+                    "platform": {"os": "Linux", "python": "3.11.0", "cpu_count": 8},
+                    "deps": {
+                        "sct": {"found": True, "version": "6.0"},
+                        "pam50": {"found": True, "path": "/opt/sct/data/PAM50"},
+                    },
+                }
+            )
+        )
+
+        # Create mock PNG assets
+        (qc_dir / "overlay_epi_t2w.png").write_bytes(b"fake_png")
+        (qc_dir / "overlay_t2w_template.png").write_bytes(b"fake_png")
+        (qc_dir / "motion_fd_dvars.png").write_bytes(b"fake_png")
+
+        yield {
+            "subj_dir": subj_dir,
+            "qc_dir": qc_dir,
+            "confounds_tsv": confounds_tsv,
+            "doctor_json": doctor_json,
+        }
+
+
+def test_write_qc_creates_index_html(mock_subject_dir):
+    """Test that write_qc creates index.html in subject QC directory."""
+    from spineprep.qc.report import write_qc
+
+    assets = {
+        "confounds_tsv": mock_subject_dir["confounds_tsv"],
+        "doctor_json": mock_subject_dir["doctor_json"],
+    }
+
+    html_path = write_qc(
+        subject_dir=mock_subject_dir["subj_dir"],
+        assets=assets,
+    )
+
+    assert html_path.exists()
+    assert html_path.name == "index.html"
+    assert html_path.parent == mock_subject_dir["qc_dir"]
+
+
+def test_qc_html_has_required_elements(mock_subject_dir):
+    """Test that QC HTML contains img, table, and Environment text."""
+    from spineprep.qc.report import write_qc
+
+    assets = {
+        "confounds_tsv": mock_subject_dir["confounds_tsv"],
+        "doctor_json": mock_subject_dir["doctor_json"],
+    }
+
+    html_path = write_qc(
+        subject_dir=mock_subject_dir["subj_dir"],
+        assets=assets,
+    )
+
+    html = html_path.read_text()
+
+    # Check for images
+    assert "<img" in html, "HTML must contain at least one <img> tag"
+
+    # Check for tables
+    assert "<table" in html, "HTML must contain at least one <table> tag"
+
+    # Check for Environment section
+    assert (
+        "Environment" in html or "environment" in html.lower()
+    ), "HTML must contain Environment section"
+
+
+def test_qc_html_contains_motion_metrics(mock_subject_dir):
+    """Test that motion metrics (FD, DVARS) are displayed."""
+    from spineprep.qc.report import write_qc
+
+    assets = {
+        "confounds_tsv": mock_subject_dir["confounds_tsv"],
+        "doctor_json": mock_subject_dir["doctor_json"],
+    }
+
+    html_path = write_qc(
+        subject_dir=mock_subject_dir["subj_dir"],
+        assets=assets,
+    )
+
+    html = html_path.read_text()
+
+    # Check for motion-related content
+    assert "FD" in html or "framewise" in html.lower()
+    assert "DVARS" in html or "dvars" in html.lower()
+    assert "motion" in html.lower() or "Motion" in html
+
+
+def test_qc_html_contains_acompcor_summary(mock_subject_dir):
+    """Test that aCompCor summary table is present."""
+    from spineprep.qc.report import write_qc
+
+    assets = {
+        "confounds_tsv": mock_subject_dir["confounds_tsv"],
+        "doctor_json": mock_subject_dir["doctor_json"],
+    }
+
+    html_path = write_qc(
+        subject_dir=mock_subject_dir["subj_dir"],
+        assets=assets,
+    )
+
+    html = html_path.read_text()
+
+    # Check for aCompCor content
+    assert (
+        "acompcor" in html.lower() or "CompCor" in html or "compcor" in html.lower()
+    )
+    # Should show component columns
+    assert "acompcor01" in html or "PC" in html or "Component" in html
+
+
+def test_qc_html_contains_environment_info(mock_subject_dir):
+    """Test that environment info from doctor.json is displayed."""
+    from spineprep.qc.report import write_qc
+
+    assets = {
+        "confounds_tsv": mock_subject_dir["confounds_tsv"],
+        "doctor_json": mock_subject_dir["doctor_json"],
+    }
+
+    html_path = write_qc(
+        subject_dir=mock_subject_dir["subj_dir"],
+        assets=assets,
+    )
+
+    html = html_path.read_text()
+
+    # Check for environment details
+    assert "Linux" in html or "Python" in html or "SCT" in html
+    assert "Environment" in html or "environment" in html.lower()
+
+
+def test_qc_html_is_self_contained(mock_subject_dir):
+    """Test that QC HTML has inline CSS and no external CDN dependencies."""
+    from spineprep.qc.report import write_qc
+
+    assets = {
+        "confounds_tsv": mock_subject_dir["confounds_tsv"],
+        "doctor_json": mock_subject_dir["doctor_json"],
+    }
+
+    html_path = write_qc(
+        subject_dir=mock_subject_dir["subj_dir"],
+        assets=assets,
+    )
+
+    html = html_path.read_text()
+
+    # Should have inline CSS
+    assert "<style>" in html and "</style>" in html
+
+    # Should not reference external CDNs
+    assert "cdn.jsdelivr.net" not in html
+    assert "googleapis.com" not in html
+    assert "cloudflare.com" not in html
+
+
+def test_qc_html_parses_with_beautifulsoup(mock_subject_dir):
+    """Test that generated HTML can be parsed with BeautifulSoup."""
+    from spineprep.qc.report import write_qc
+
+    try:
+        from bs4 import BeautifulSoup
+    except ImportError:
+        pytest.skip("BeautifulSoup not installed")
+
+    assets = {
+        "confounds_tsv": mock_subject_dir["confounds_tsv"],
+        "doctor_json": mock_subject_dir["doctor_json"],
+    }
+
+    html_path = write_qc(
+        subject_dir=mock_subject_dir["subj_dir"],
+        assets=assets,
+    )
+
+    html = html_path.read_text()
+    soup = BeautifulSoup(html, "html.parser")
+
+    # Validate structure
+    assert soup.find("img"), "Must have at least one img tag"
+    assert soup.find("table"), "Must have at least one table"
+    assert soup.find(
+        string=lambda t: t and "Environment" in t
+    ), "Must have Environment text"
+
+
+def test_qc_handles_missing_assets_gracefully(mock_subject_dir):
+    """Test that QC generation works even with missing optional assets."""
+    from spineprep.qc.report import write_qc
+
+    # Remove optional assets
+    (mock_subject_dir["qc_dir"] / "overlay_epi_t2w.png").unlink()
+
+    assets = {
+        "confounds_tsv": mock_subject_dir["confounds_tsv"],
+        "doctor_json": mock_subject_dir["doctor_json"],
+    }
+
+    # Should not raise
+    html_path = write_qc(
+        subject_dir=mock_subject_dir["subj_dir"],
+        assets=assets,
+    )
+
+    assert html_path.exists()
+    html = html_path.read_text()
+    assert "<html" in html or "<!DOCTYPE" in html
+
+
+def test_qc_tabs_for_overlays(mock_subject_dir):
+    """Test that overlay images are organized in tabs or sections."""
+    from spineprep.qc.report import write_qc
+
+    assets = {
+        "confounds_tsv": mock_subject_dir["confounds_tsv"],
+        "doctor_json": mock_subject_dir["doctor_json"],
+    }
+
+    html_path = write_qc(
+        subject_dir=mock_subject_dir["subj_dir"],
+        assets=assets,
+    )
+
+    html = html_path.read_text()
+
+    # Should have overlay-related content
+    assert (
+        "overlay" in html.lower()
+        or "registration" in html.lower()
+        or "EPI" in html
+    )
+
+
+def test_qc_censoring_summary(mock_subject_dir):
+    """Test that censoring summary is displayed."""
+    from spineprep.qc.report import write_qc
+
+    assets = {
+        "confounds_tsv": mock_subject_dir["confounds_tsv"],
+        "doctor_json": mock_subject_dir["doctor_json"],
+    }
+
+    html_path = write_qc(
+        subject_dir=mock_subject_dir["subj_dir"],
+        assets=assets,
+    )
+
+    html = html_path.read_text()
+
+    # Should mention censoring
+    assert (
+        "censor" in html.lower()
+        or "excluded" in html.lower()
+        or "flagged" in html.lower()
+    )
diff --git a/tests/test_version.py b/tests/test_version.py
new file mode 100644
index 00000000..06823d9e
--- /dev/null
+++ b/tests/test_version.py
@@ -0,0 +1,47 @@
+"""Tests for spineprep version command."""
+
+from __future__ import annotations
+
+import subprocess
+
+
+def test_cli_version_command():
+    """Test that 'spineprep version' prints version in semver format."""
+    result = subprocess.run(
+        ["spineprep", "version"],
+        capture_output=True,
+        text=True,
+        check=True,
+    )
+
+    assert result.returncode == 0
+    output = result.stdout.strip()
+
+    # Should print "spineprep X.Y.Z"
+    assert output.startswith("spineprep ")
+
+    # Extract version part
+    version_str = output.split()[-1]
+
+    # Validate semver format (X.Y.Z)
+    parts = version_str.split(".")
+    assert len(parts) == 3, f"Version should be X.Y.Z format, got {version_str}"
+
+    # Each part should be a number
+    for part in parts:
+        assert (
+            part.isdigit()
+        ), f"Version part '{part}' should be numeric in {version_str}"
+
+
+def test_version_import():
+    """Test that __version__ is importable from package."""
+    from spineprep import __version__
+
+    assert __version__ is not None
+    assert isinstance(__version__, str)
+    assert len(__version__) > 0
+
+    # Should be semver format
+    parts = __version__.split(".")
+    assert len(parts) == 3, f"Version should be X.Y.Z format, got {__version__}"
diff --git a/tests/unit/test_doctor.py b/tests/unit/test_doctor.py
index 613b3d3e..da542d8c 100644
--- a/tests/unit/test_doctor.py
+++ b/tests/unit/test_doctor.py
@@ -151,7 +151,7 @@ def test_generate_report_all_pass():
     report = generate_report(sct_info, pam50_info, python_deps, os_info)

     assert report["status"] == "pass"
-    assert report["spineprep"]["version"] == "0.1.0"
+    assert report["spineprep"]["version"] == "0.2.0"
     assert "timestamp" in report
     assert report["deps"]["sct"]["found"] is True
     assert report["deps"]["pam50"]["found"] is True
diff --git a/tests/unit/test_header_checks.py b/tests/unit/test_header_checks.py
new file mode 100644
index 00000000..dc91d4d6
--- /dev/null
+++ b/tests/unit/test_header_checks.py
@@ -0,0 +1,173 @@
+"""Unit tests for NIfTI header validation."""
+
+from __future__ import annotations
+
+import nibabel as nib
+import numpy as np
+import pytest
+
+
+def test_check_header_valid():
+    """Test header check passes for valid image."""
+    from spineprep.registration.header import check_header
+
+    # Create a simple valid NIfTI image
+    data = np.random.rand(64, 64, 16).astype(np.float32)
+    affine = np.eye(4)
+    affine[0, 0] = 2.0  # 2mm voxels in x
+    affine[1, 1] = 2.0  # 2mm voxels in y
+    affine[2, 2] = 2.0  # 2mm voxels in z
+    img = nib.Nifti1Image(data, affine)
+
+    result = check_header(img)
+    assert result["valid"] is True
+    assert "shape" in result
+    assert "zooms" in result
+    assert "sform_code" in result
+    assert "qform_code" in result
+
+
+def test_check_header_shape():
+    """Test header check captures image shape."""
+    from spineprep.registration.header import check_header
+
+    data = np.random.rand(32, 48, 24).astype(np.float32)
+    img = nib.Nifti1Image(data, np.eye(4))
+
+    result = check_header(img)
+    assert result["shape"] == [32, 48, 24]
+
+
+def test_check_header_zooms():
+    """Test header check captures voxel sizes."""
+    from spineprep.registration.header import check_header
+
+    data = np.random.rand(64, 64, 16).astype(np.float32)
+    affine = np.eye(4)
+    affine[0, 0] = 1.5
+    affine[1, 1] = 1.5
+    affine[2, 2] = 3.0
+    img = nib.Nifti1Image(data, affine)
+
+    result = check_header(img)
+    assert result["zooms"] == pytest.approx([1.5, 1.5, 3.0])
+
+
+def test_check_affines_match():
+    """Test affine matching check."""
+    from spineprep.registration.header import check_affines_match
+
+    affine1 = np.eye(4)
+    affine1[0, 0] = 2.0
+    affine2 = affine1.copy()
+
+    assert check_affines_match(affine1, affine2) is True
+
+
+def test_check_affines_mismatch():
+    """Test affine mismatch detection."""
+    from spineprep.registration.header import check_affines_match
+
+    affine1 = np.eye(4)
+    affine1[0, 0] = 2.0
+    affine2 = np.eye(4)
+    affine2[0, 0] = 1.5
+
+    assert check_affines_match(affine1, affine2) is False
+
+
+def test_check_affines_match_tolerance():
+    """Test affine matching with tolerance."""
+    from spineprep.registration.header import check_affines_match
+
+    affine1 = np.eye(4)
+    affine1[0, 0] = 2.0
+    affine2 = affine1.copy()
+    affine2[0, 0] = 2.0 + 1e-7  # Tiny difference
+
+    assert check_affines_match(affine1, affine2, atol=1e-6) is True
+    assert check_affines_match(affine1, affine2, atol=1e-8) is False
+
+
+def test_check_form_codes():
+    """Test form code validation."""
+    from spineprep.registration.header import check_form_codes
+
+    data = np.random.rand(64, 64, 16).astype(np.float32)
+    img = nib.Nifti1Image(data, np.eye(4))
+
+    # Set form codes to aligned (code 2)
+    img.header.set_sform(img.affine, code=2)
+    img.header.set_qform(img.affine, code=2)
+
+    result = check_form_codes(img)
+    assert result["sform_code"] == 2
+    assert result["qform_code"] == 2
+    assert result["forms_match"] is True
+
+
+def test_check_form_codes_mismatch():
+    """Test form code mismatch detection."""
+    from spineprep.registration.header import check_form_codes
+
+    data = np.random.rand(64, 64, 16).astype(np.float32)
+    img = nib.Nifti1Image(data, np.eye(4))
+
+    # Set different form codes
+    img.header.set_sform(img.affine, code=2)
+    img.header.set_qform(img.affine, code=1)
+
+    result = check_form_codes(img)
+    assert result["sform_code"] == 2
+    assert result["qform_code"] == 1
+    assert result["forms_match"] is False
+
+
+def test_check_form_codes_unknown():
+    """Test detection of unknown form codes."""
+    from spineprep.registration.header import check_form_codes
+
+    data = np.random.rand(64, 64, 16).astype(np.float32)
+    img = nib.Nifti1Image(data, np.eye(4))
+
+    # Set to unknown (code 0)
+    img.header.set_sform(img.affine, code=0)
+    img.header.set_qform(img.affine, code=0)
+
+    result = check_form_codes(img)
+    assert result["sform_code"] == 0
+    assert result["qform_code"] == 0
+    assert result["has_unknown_codes"] is True
+
+
+def test_validate_registration_output():
+    """Test complete registration output validation."""
+    from spineprep.registration.header import validate_registration_output
+
+    data = np.random.rand(64, 64, 16).astype(np.float32)
+    affine = np.eye(4)
+    affine[0, 0] = 0.5
+    affine[1, 1] = 0.5
+    affine[2, 2] = 0.5
+    img = nib.Nifti1Image(data, affine)
+    img.header.set_sform(affine, code=2)
+    img.header.set_qform(affine, code=2)
+
+    result = validate_registration_output(img, expected_zooms=(0.5, 0.5, 0.5))
+    assert result["valid"] is True
+    assert result["zooms_match"] is True
+
+
+def test_validate_registration_output_wrong_zooms():
+    """Test validation fails for incorrect voxel sizes."""
+    from spineprep.registration.header import validate_registration_output
+
+    data = np.random.rand(64, 64, 16).astype(np.float32)
+    affine = np.eye(4)
+    affine[0, 0] = 1.0
+    affine[1, 1] = 1.0
+    affine[2, 2] = 1.0
+    img = nib.Nifti1Image(data, affine)
+
+    result = validate_registration_output(img, expected_zooms=(0.5, 0.5, 0.5))
+    assert result["zooms_match"] is False
diff --git a/tests/unit/test_registration_metrics.py b/tests/unit/test_registration_metrics.py
new file mode 100644
index 00000000..1d6a5907
--- /dev/null
+++ b/tests/unit/test_registration_metrics.py
@@ -0,0 +1,132 @@
+"""Unit tests for registration metrics computation (SSIM/PSNR)."""
+
+from __future__ import annotations
+
+import numpy as np
+import pytest
+
+
+def test_ssim_identical_images():
+    """SSIM should be 1.0 for identical images."""
+    from spineprep.registration.metrics import compute_ssim
+
+    img = np.random.rand(64, 64).astype(np.float32)
+    ssim = compute_ssim(img, img.copy())
+    assert ssim == pytest.approx(1.0, abs=1e-6)
+
+
+def test_ssim_different_images():
+    """SSIM should be < 1.0 for different images."""
+    from spineprep.registration.metrics import compute_ssim
+
+    img1 = np.random.rand(64, 64).astype(np.float32)
+    img2 = np.random.rand(64, 64).astype(np.float32)
+    ssim = compute_ssim(img1, img2)
+    assert 0.0 <= ssim < 1.0
+
+
+def test_ssim_3d_images():
+    """SSIM should work on 3D volumes."""
+    from spineprep.registration.metrics import compute_ssim
+
+    img = np.random.rand(32, 32, 16).astype(np.float32)
+    ssim = compute_ssim(img, img.copy())
+    assert ssim == pytest.approx(1.0, abs=1e-6)
+
+
+def test_ssim_shape_mismatch():
+    """SSIM should raise ValueError for mismatched shapes."""
+    from spineprep.registration.metrics import compute_ssim
+
+    img1 = np.random.rand(64, 64).astype(np.float32)
+    img2 = np.random.rand(32, 32).astype(np.float32)
+    with pytest.raises(ValueError, match="Shape mismatch"):
+        compute_ssim(img1, img2)
+
+
+def test_psnr_identical_images():
+    """PSNR should be inf for identical images."""
+    from spineprep.registration.metrics import compute_psnr
+
+    img = np.random.rand(64, 64).astype(np.float32)
+    psnr = compute_psnr(img, img.copy())
+    assert np.isinf(psnr) or psnr > 100.0  # Very high PSNR for identical
+
+
+def test_psnr_different_images():
+    """PSNR should be finite for different images."""
+    from spineprep.registration.metrics import compute_psnr
+
+    img1 = np.random.rand(64, 64).astype(np.float32)
+    img2 = np.random.rand(64, 64).astype(np.float32)
+    psnr = compute_psnr(img1, img2)
+    assert psnr > 0.0 and not np.isinf(psnr)
+
+
+def test_psnr_shape_mismatch():
+    """PSNR should raise ValueError for mismatched shapes."""
+    from spineprep.registration.metrics import compute_psnr
+
+    img1 = np.random.rand(64, 64).astype(np.float32)
+    img2 = np.random.rand(32, 32).astype(np.float32)
+    with pytest.raises(ValueError, match="Shape mismatch"):
+        compute_psnr(img1, img2)
+
+
+def test_normalize_intensity_clip():
+    """Test intensity normalization with percentile clipping."""
+    from spineprep.registration.metrics import normalize_intensity
+
+    img = np.random.rand(64, 64).astype(np.float32) * 1000
+    normalized = normalize_intensity(img, clip_percentiles=(2, 98))
+    # Should be z-scored
+    assert normalized.mean() == pytest.approx(0.0, abs=0.1)
+    assert normalized.std() == pytest.approx(1.0, abs=0.1)
+
+
+def test_normalize_intensity_with_mask():
+    """Test intensity normalization with mask."""
+    from spineprep.registration.metrics import normalize_intensity
+
+    img = np.random.rand(64, 64).astype(np.float32)
+    mask = np.zeros((64, 64), dtype=bool)
+    mask[16:48, 16:48] = True  # Center region
+
+    normalized = normalize_intensity(img, mask=mask)
+    # Should normalize based on mask region
+    assert normalized.shape == img.shape
+    # Mean and std computed from masked region should be ~0 and ~1
+    masked_vals = normalized[mask]
+    assert masked_vals.mean() == pytest.approx(0.0, abs=0.1)
+    assert masked_vals.std() == pytest.approx(1.0, abs=0.1)
+
+
+def test_normalize_intensity_handles_nan():
+    """Test that normalization handles NaN gracefully."""
+    from spineprep.registration.metrics import normalize_intensity
+
+    img = np.random.rand(64, 64).astype(np.float32)
+    img[0, 0] = np.nan
+    normalized = normalize_intensity(img)
+    # Should not propagate NaN everywhere
+    assert np.isfinite(normalized[10, 10])
+
+
+def test_ssim_psnr_integration():
+    """Test SSIM and PSNR on a realistic registration scenario."""
+    from spineprep.registration.metrics import compute_psnr, compute_ssim
+
+    # Create a simple test pattern
+    img1 = np.zeros((64, 64), dtype=np.float32)
+    img1[20:44, 20:44] = 1.0  # Square
+
+    # Slightly shifted version
+    img2 = np.zeros((64, 64), dtype=np.float32)
+    img2[21:45, 21:45] = 1.0
+
+    ssim = compute_ssim(img1, img2)
+    psnr = compute_psnr(img1, img2)
+
+    # Should have decent but not perfect similarity
+    assert 0.7 < ssim < 1.0
+    assert 10.0 < psnr < 50.0
diff --git a/tests/unit/test_sct_wrapper.py b/tests/unit/test_sct_wrapper.py
new file mode 100644
index 00000000..c96731dd
--- /dev/null
+++ b/tests/unit/test_sct_wrapper.py
@@ -0,0 +1,215 @@
+"""Unit tests for SCT wrapper."""
+
+from __future__ import annotations
+
+from unittest.mock import MagicMock, patch
+
+import pytest
+
+
+def test_build_sct_command_basic():
+    """Test basic SCT command building."""
+    from spineprep.registration.sct import build_sct_register_cmd
+
+    cmd = build_sct_register_cmd(
+        src_image="epi.nii.gz",
+        dest_image="template.nii.gz",
+        out_warp="warp.nii.gz",
+    )
+
+    assert "sct_register_multimodal" in cmd
+    assert "-i" in cmd
+    assert "epi.nii.gz" in cmd
+    assert "-d" in cmd
+    assert "template.nii.gz" in cmd
+    assert "-owarp" in cmd
+    assert "warp.nii.gz" in cmd
+
+
+def test_build_sct_command_with_resampled():
+    """Test SCT command with resampled output."""
+    from spineprep.registration.sct import build_sct_register_cmd
+
+    cmd = build_sct_register_cmd(
+        src_image="epi.nii.gz",
+        dest_image="template.nii.gz",
+        out_warp="warp.nii.gz",
+        out_resampled="epi_reg.nii.gz",
+    )
+
+    assert "-o" in cmd
+    assert "epi_reg.nii.gz" in cmd
+
+
+def test_build_sct_command_with_params():
+    """Test SCT command with custom parameters."""
+    from spineprep.registration.sct import build_sct_register_cmd
+
+    cmd = build_sct_register_cmd(
+        src_image="epi.nii.gz",
+        dest_image="template.nii.gz",
+        out_warp="warp.nii.gz",
+        param="step=1,type=im,algo=rigid",
+    )
+
+    assert "-param" in cmd
+    assert "step=1,type=im,algo=rigid" in cmd
+
+
+@patch("subprocess.run")
+def test_run_sct_register_success(mock_run):
+    """Test successful SCT registration."""
+    from spineprep.registration.sct import run_sct_register
+
+    # Mock successful subprocess run
+    mock_result = MagicMock()
+    mock_result.returncode = 0
+    mock_result.stdout = "Registration completed successfully"
+    mock_result.stderr = ""
+    mock_run.return_value = mock_result
+
+    result = run_sct_register(
+        src_image="epi.nii.gz",
+        dest_image="template.nii.gz",
+        out_warp="warp.nii.gz",
+    )
+
+    assert result["success"] is True
+    assert result["return_code"] == 0
+    assert "Registration completed" in result["stdout"]
+    # Should be called twice: once for version, once for registration
+    assert mock_run.call_count == 2
+
+
+@patch("subprocess.run")
+def test_run_sct_register_failure(mock_run):
+    """Test failed SCT registration."""
+    from spineprep.registration.sct import run_sct_register
+
+    # Mock failed subprocess run
+    mock_result = MagicMock()
+    mock_result.returncode = 1
+    mock_result.stdout = ""
+    mock_result.stderr = "Error: Image not found"
+    mock_run.return_value = mock_result
+
+    result = run_sct_register(
+        src_image="missing.nii.gz",
+        dest_image="template.nii.gz",
+        out_warp="warp.nii.gz",
+    )
+
+    assert result["success"] is False
+    assert result["return_code"] == 1
+    assert "Error" in result["stderr"]
+
+
+@patch("subprocess.run")
+def test_run_sct_register_captures_version(mock_run):
+    """Test that SCT version is captured."""
+    from spineprep.registration.sct import run_sct_register
+
+    # Mock version check and registration
+    def side_effect(*args, **kwargs):
+        cmd = args[0]
+        if "sct_version" in cmd:
+            result = MagicMock()
+            result.returncode = 0
+            result.stdout = "5.8.0"
+            return result
+        else:
+            result = MagicMock()
+            result.returncode = 0
+            result.stdout = "Registration completed"
+            result.stderr = ""
+            return result
+
+    mock_run.side_effect = side_effect
+
+    result = run_sct_register(
+        src_image="epi.nii.gz",
+        dest_image="template.nii.gz",
+        out_warp="warp.nii.gz",
+    )
+
+    assert "sct_version" in result
+    assert result["sct_version"] == "5.8.0"
+
+
+def test_check_doctor_status_green(tmp_path):
+    """Test doctor status check passes when green."""
+    from spineprep.registration.sct import check_doctor_status
+
+    doctor_json = tmp_path / "doctor.json"
+    doctor_json.write_text(
+        """{
+        "checks": {
+            "sct_installed": {"status": "green"},
+            "pam50_available": {"status": "green"}
+        }
+    }"""
+    )
+
+    # Should not raise
+    check_doctor_status(str(doctor_json))
+
+
+def test_check_doctor_status_missing_file():
+    """Test doctor status check raises if file missing."""
+    from spineprep.registration.sct import check_doctor_status
+
+    with pytest.raises((FileNotFoundError, RuntimeError), match="doctor"):
+        check_doctor_status("/nonexistent/doctor.json")
+
+
+def test_check_doctor_status_red(tmp_path):
+    """Test doctor status check raises if checks are red."""
+    from spineprep.registration.sct import check_doctor_status
+
+    doctor_json = tmp_path / "doctor.json"
+    doctor_json.write_text(
+        """{
+        "checks": {
+            "sct_installed": {"status": "red", "message": "SCT not found"},
+            "pam50_available": {"status": "green"}
+        }
+    }"""
+    )
+
+    with pytest.raises(RuntimeError, match="SCT.*red"):
+        check_doctor_status(str(doctor_json))
+
+
+def test_map_sct_error_actionable():
+    """Test SCT error mapping provides actionable messages."""
+    from spineprep.registration.sct import map_sct_error
+
+    error_msg = map_sct_error(1, "sct_register_multimodal: command not found")
+    assert "install" in error_msg.lower() or "path" in error_msg.lower()
+
+    error_msg = map_sct_error(1, "cannot open image")
+    assert "file" in error_msg.lower() or "path" in error_msg.lower()
+
+
+def test_create_output_paths():
+    """Test creation of output paths following BIDS derivatives structure."""
+    from spineprep.registration.sct import create_output_paths
+
+    paths = create_output_paths(
+        subject="sub-01",
+        session=None,
+        task="rest",
+        out_dir="/data/derivatives",
+    )
+
+    assert "warp" in paths
+    assert "registered" in paths
+    assert "metrics" in paths
+    assert "mosaic_before" in paths
+    assert "mosaic_after" in paths
+
+    # Check paths follow BIDS structure
+    assert "sub-01" in paths["warp"]
+    assert "xfm" in paths["warp"]
+    assert "func" in paths["registered"]
+    assert "qc" in paths["metrics"]
diff --git a/workflow/rules/confounds.smk b/workflow/rules/confounds.smk
new file mode 100644
index 00000000..c757511b
--- /dev/null
+++ b/workflow/rules/confounds.smk
@@ -0,0 +1,91 @@
+"""
+Confounds computation rules for SpinePrep.
+
+This module defines Snakemake rules for computing confounds:
+- confounds_motion: compute FD/DVARS from motion params and BOLD
+"""
+
+from pathlib import Path as _P
+
+
+rule confounds_motion:
+    """
+    Compute motion-based confounds (FD, DVARS) and generate QC plots.
+
+    Inputs:
+        - motion-corrected BOLD NIfTI
+        - motion parameters TSV
+
+    Outputs:
+        - confounds TSV (motion params + FD + DVARS)
+        - confounds JSON sidecar (schema v1)
+        - motion QC PNG plot
+    """
+    input:
+        bold="{prefix}_desc-motion_bold.nii.gz",
+        motion_params="{prefix}_desc-motion_params.tsv",
+    output:
+        tsv="{prefix}_desc-confounds_timeseries.tsv",
+        json="{prefix}_desc-confounds_timeseries.json",
+        png="{prefix}_motion.png",
+    log:
+        "{prefix}_confounds_motion.log",
+    resources:
+        mem_mb=500,
+    run:
+        import sys
+        from pathlib import Path
+
+        # Add spineprep to path
+        sys.path.insert(0, str(_P(workflow.basedir).parent))
+
+        import nibabel as nib
+        import numpy as np
+
+        from spineprep.confounds import (
+            compute_dvars,
+            compute_fd_power,
+            plot_motion_png,
+            write_confounds_tsv_json,
+        )
+
+        # Load motion parameters from TSV
+        import pandas as pd
+
+        motion_df = pd.read_csv(input.motion_params, sep="\t")
+        motion_params = motion_df[
+            ["trans_x", "trans_y", "trans_z", "rot_x", "rot_y", "rot_z"]
+        ].values.astype(np.float32)
+
+        # Compute FD
+        fd = compute_fd_power(motion_params, radius_mm=50.0)
+
+        # Load BOLD and compute DVARS
+        img = nib.load(input.bold)
+        data_4d = img.get_fdata().astype(np.float32)
+        dvars = compute_dvars(data_4d)
+
+        # Write outputs
+        write_confounds_tsv_json(
+            tsv_path=output.tsv,
+            json_path=output.json,
+            motion_params=motion_params,
+            fd=fd,
+            dvars=dvars,
+        )
+
+        plot_motion_png(
+            png_path=output.png,
+            motion_params=motion_params,
+            fd=fd,
+            dvars=dvars,
+        )
+
+        # Log summary
+        with open(log[0], "w") as f:
+            f.write(f"Confounds computed for {input.bold}\n")
+            f.write(f"  Timepoints: {len(fd)}\n")
+            f.write(f"  Mean FD: {fd.mean():.3f} mm\n")
+            f.write(f"  Max FD: {fd.max():.3f} mm\n")
+            f.write(f"  Mean DVARS: {dvars.mean():.3f}\n")
+            f.write(f"  Max DVARS: {dvars.max():.3f}\n")
diff --git a/workflow/rules/registration.smk b/workflow/rules/registration.smk
new file mode 100644
index 00000000..714ee719
--- /dev/null
+++ b/workflow/rules/registration.smk
@@ -0,0 +1,233 @@
+"""
+Registration rule for EPI→PAM50 with metrics and QC.
+
+Implements SCT-based registration with:
+- Doctor precheck for SCT/PAM50 availability
+- Intensity-normalized SSIM/PSNR metrics
+- Header sanity checks
+- Before/after mosaics for QC
+"""
+
+import json
+import os
+import subprocess
+from pathlib import Path
+
+import matplotlib
+import matplotlib.pyplot as plt
+import nibabel as nib
+import numpy as np
+
+matplotlib.use("Agg")
+
+
+rule registration_pam50:
+    """
+    Register EPI reference to PAM50 template with full QC metrics.
+
+    Conservative SCT registration (rigid+affine) with SSIM/PSNR validation,
+    header checks, and mosaic generation for QC.
+    """
+    input:
+        epi_ref="{deriv}/sub-{sub}/func/sub-{sub}_task-{task}_run-{run}_desc-mean_bold.nii.gz",
+        doctor_json=lambda wildcards: f"{CFG['paths']['logs_dir']}/doctor.json",
+    output:
+        # Transformation
+        warp="{deriv}/sub-{sub}/xfm/sub-{sub}_task-{task}_run-{run}_from-epi_to-PAM50_desc-sct_xfm.nii.gz",
+        # Registered reference
+        registered="{deriv}/sub-{sub}/func/sub-{sub}_task-{task}_run-{run}_space-PAM50_desc-ref_bold.nii.gz",
+        # Metrics
+        metrics="{deriv}/sub-{sub}/qc/sub-{sub}_task-{task}_run-{run}_desc-registration_metrics.json",
+        # Mosaics
+        mosaic_before="{deriv}/sub-{sub}/qc/registration/sub-{sub}_task-{task}_run-{run}_desc-before_mosaic.png",
+        mosaic_after="{deriv}/sub-{sub}/qc/registration/sub-{sub}_task-{task}_run-{run}_desc-after_mosaic.png",
+    log:
+        "{deriv}/logs/registration_pam50_{sub}_{task}_{run}.log",
+    resources:
+        mem_mb=2000,
+    run:
+        from spineprep.registration.sct import (
+            check_doctor_status,
+            create_output_paths,
+            map_sct_error,
+            run_sct_register,
+        )
+        from spineprep.registration.metrics import (
+            compute_psnr,
+            compute_ssim,
+            normalize_intensity,
+        )
+        from spineprep.registration.header import validate_registration_output
+
+        # Step 1: Check doctor status for SCT/PAM50
+        try:
+            check_doctor_status(input.doctor_json)
+        except (FileNotFoundError, RuntimeError) as e:
+            msg = f"Doctor precheck failed: {e}\nRun 'spineprep doctor' before registration."
+            with open(log[0], "w") as f:
+                f.write(msg)
+            raise RuntimeError(msg)
+
+            # Step 2: Get PAM50 template path
+        pam50_dir = os.environ.get("SCT_DIR", "/opt/sct") + "/data/PAM50"
+        fixed_image_key = CFG.get("registration", {}).get("fixed_image", "PAM50_t2")
+        pam50_template = f"{pam50_dir}/{fixed_image_key}.nii.gz"
+
+        if not Path(pam50_template).exists():
+            msg = f"PAM50 template not found: {pam50_template}"
+            with open(log[0], "w") as f:
+                f.write(msg)
+            raise FileNotFoundError(msg)
+
+            # Step 3: Run SCT registration
+        result = run_sct_register(
+            src_image=input.epi_ref,
+            dest_image=pam50_template,
+            out_warp=output.warp,
+            out_resampled=output.registered,
+        )
+
+        # Write registration log
+        with open(log[0], "w") as f:
+            f.write(f"Command: {result['command']}\n")
+            f.write(f"SCT version: {result['sct_version']}\n")
+            f.write(f"Return code: {result['return_code']}\n")
+            f.write(f"\nSTDOUT:\n{result['stdout']}\n")
+            f.write(f"\nSTDERR:\n{result['stderr']}\n")
+
+            # Check registration success
+        if not result["success"]:
+            error_msg = map_sct_error(result["return_code"], result["stderr"])
+            raise RuntimeError(f"Registration failed: {error_msg}")
+
+            # Step 4: Load images for metrics
+        epi_img = nib.load(input.epi_ref)
+        registered_img = nib.load(output.registered)
+        template_img = nib.load(pam50_template)
+
+        # Step 5: Validate header
+        # Get expected zooms from template
+        expected_zooms = template_img.header.get_zooms()[:3]
+        header_validation = validate_registration_output(
+            registered_img,
+            expected_zooms=expected_zooms,
+        )
+
+        # Step 6: Compute metrics (SSIM/PSNR)
+        # Normalize images for fair comparison
+        template_data = template_img.get_fdata()
+        registered_data = registered_img.get_fdata()
+
+        # Clip and normalize
+        template_norm = normalize_intensity(template_data, clip_percentiles=(2, 98))
+        registered_norm = normalize_intensity(registered_data, clip_percentiles=(2, 98))
+
+        # Read thresholds from config
+        qc_cfg = CFG.get("qc", {})
+        reg_cfg = qc_cfg.get("registration", {})
+        ssim_min = reg_cfg.get("ssim_min", 0.60)
+        psnr_min = reg_cfg.get("psnr_min", 15.0)
+
+        # Compute metrics
+        ssim_val = compute_ssim(registered_norm, template_norm)
+        psnr_val = compute_psnr(registered_norm, template_norm)
+
+        # Step 7: Write metrics JSON
+        metrics_data = {
+            "ssim": float(ssim_val),
+            "psnr": float(psnr_val),
+            "thresholds": {
+                "ssim_min": ssim_min,
+                "psnr_min": psnr_min,
+            },
+            "header": header_validation,
+            "sct_version": result["sct_version"],
+            "template": pam50_template,
+            "status": ("pass" if (ssim_val >= ssim_min and psnr_val >= psnr_min) else "warn"),
+        }
+
+        Path(output.metrics).parent.mkdir(parents=True, exist_ok=True)
+        with open(output.metrics, "w") as f:
+            json.dump(metrics_data, f, indent=2)
+
+            # Step 8: Create mosaics (before/after)
+
+
+        def create_mosaic(img_data, title, out_path):
+            """Create simple orthogonal mosaic."""
+            # Get mid-slices
+            mid_x = img_data.shape[0] // 2
+            mid_y = img_data.shape[1] // 2
+            mid_z = img_data.shape[2] // 2
+
+            fig, axes = plt.subplots(1, 3, figsize=(12, 4))
+
+            # Sagittal
+            axes[0].imshow(img_data[mid_x, :, :].T, cmap="gray", origin="lower")
+            axes[0].set_title("Sagittal")
+            axes[0].axis("off")
+
+            # Coronal
+            axes[1].imshow(img_data[:, mid_y, :].T, cmap="gray", origin="lower")
+            axes[1].set_title("Coronal")
+            axes[1].axis("off")
+
+            # Axial
+            axes[2].imshow(img_data[:, :, mid_z].T, cmap="gray", origin="lower")
+            axes[2].set_title("Axial")
+            axes[2].axis("off")
+
+            fig.suptitle(title)
+            fig.tight_layout()
+            Path(out_path).parent.mkdir(parents=True, exist_ok=True)
+            fig.savefig(out_path, dpi=100, bbox_inches="tight")
+            plt.close(fig)
+
+            # Before: EPI reference (original)
+
+
+        epi_data = epi_img.get_fdata()
+        create_mosaic(
+            epi_data,
+            f"Before Registration: {wildcards.sub} {wildcards.task}",
+            output.mosaic_before,
+        )
+
+        # After: Registered EPI in PAM50 space
+        create_mosaic(
+            registered_data,
+            f"After Registration: {wildcards.sub} {wildcards.task} (PAM50 space)",
+            output.mosaic_after,
+        )
+
+        # Step 9: Write provenance
+        prov_path = Path(output.warp).with_suffix(".prov.json")
+        with open(prov_path, "w") as f:
+            json.dump(
+                {
+                    "step": "registration_pam50",
+                    "inputs": {
+                        "epi_ref": input.epi_ref,
+                        "template": pam50_template,
+                        "doctor_json": input.doctor_json,
+                    },
+                    "outputs": {
+                        "warp": output.warp,
+                        "registered": output.registered,
+                        "metrics": output.metrics,
+                    },
+                    "sct_version": result["sct_version"],
+                    "command": result["command"],
+                    "metrics": metrics_data,
+                    "timestamp": subprocess.check_output(["date", "-Iseconds"], text=True).strip(),
+                },
+                f,
+                indent=2,
+            )
+
+            # Log completion
+        with open(log[0], "a") as f:
+            f.write(f"\nRegistration completed successfully.\n")
+            f.write(f"SSIM: {ssim_val:.4f} (threshold: {ssim_min})\n")
+            f.write(f"PSNR: {psnr_val:.2f} dB (threshold: {psnr_min})\n")
+            f.write(f"Status: {metrics_data['status']}\n")
